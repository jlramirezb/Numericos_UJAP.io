<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="José Luis Ramírez" />


<title>Álgebra Lineal Numérica</title>

<script src="site_libs/header-attrs-2.30/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="css/estilos.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Métodos Numéricos</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/jlramirezb">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Álgebra Lineal Numérica</h1>
<h4 class="author">José Luis Ramírez</h4>
<h4 class="date">Octubre 2025</h4>

</div>


<div id="motivación" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Motivación</h1>
<ul>
<li>En el planteamiento matemático de muchos problemas realistas, los
sistemas de ecuaciones algebraicas, y de una manera especial los
lineales, aparecen de manera natural.</li>
<li>La búsqueda de métodos de resolución de sistemas de ecuaciones
lineales es un tema de gran importancia en la ciencia.</li>
<li>El objetivo de este tema es desarrollar estrategias numéricas que
permitan resolver sistemas de ecuaciones relativamente grandes de una
manera eficiente.</li>
<li>El estudio de los autovalores de sistemas surge por doquier en
muchas áreas de la ciencia, ingeniería, economía …
<ul>
<li>Análisis de estructuras</li>
<li>Diseño de sistemas electrónicos</li>
<li>Análisis de sistemas eléctricos</li>
<li>Mercados financieros.</li>
</ul></li>
<li>Es también muy importante para analizar el comportamiento de métodos
numéricos.</li>
</ul>
</div>
<div id="solución-de-sistemas-de-ecuaciones-lineales"
class="section level1" number="2">
<h1><span class="header-section-number">2</span> Solución de Sistemas de
Ecuaciones Lineales</h1>
<ul>
<li>El objetivo de este tema es desarrollar estrategias numéricas que
permitan resolver sistemas de ecuaciones relativamente grandes de una
manera eficiente.</li>
<li>Además, se analizarán con detalle algunos métodos directos.</li>
<li>Si bien existen métodos exactos como el método de Cramer, estos son
muy costosos de aplicar en situaciones donde los sistemas a resolver
tienen muchas ecuaciones.</li>
<li>El número total de operaciones para resolver un sistema de dimensión
<span class="math inline">\(n\)</span> con este método es</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
T_C = (n+1)^2n!-1
\]</span> <span class="math display">\[
\begin{array}{|c|c|}\hline
n &amp; T_C \\\hline
5 &amp; 4319\\\hline
10 &amp; 4\times10^{8}\\\hline
100 &amp; 10\times10^{158}\\\hline
\end{array}
\]</span></p>
</div>
<div id="introducción" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Introducción</h2>
<ul>
<li>Un sistema de <span class="math inline">\(n\)</span>-ecuaciones (con
coeficientes reales) en las <span
class="math inline">\(n\)</span>-incógnitas <span
class="math inline">\(x_1, x_2, \ldots , x_n\)</span> es un conjunto de
<span class="math inline">\(n\)</span> ecuaciones de la forma:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\left\{
\begin{array}{rclclc}
a_{11}x_{1} &amp; + &amp; a_{12}x_{2} &amp; + \cdots + &amp; a_{1n}x_{n}
&amp; = b_{1} \\
a_{21}x_{1} &amp; + &amp; a_{22}x_{2} &amp; + \cdots + &amp; a_{2n}x_{n}
&amp; = b_{2} \\
\cdots &amp; &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots
\\
a_{n1}x_{1} &amp; + &amp; a_{n2}x_{2} &amp; + \cdots + &amp; a_{nn}x_{n}
&amp; = b_{n}
\end{array}
\right.
\]</span></p>
</div>
<ul>
<li>A los números <span class="math inline">\(a_{ij}\)</span> se les
denomina coeficientes del sistema y a los <span
class="math inline">\(b_i\)</span> términos independientes.</li>
</ul>
<p>Si se introducen las matrices</p>
<div class="recuadro-gris">
<p><span class="math display">\[
A = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}
\end{pmatrix}, \quad
x = \begin{pmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{pmatrix}, \quad
b = \begin{pmatrix}
b_{1} \\
b_{2} \\
\vdots \\
b_{n}
\end{pmatrix}
\]</span></p>
</div>
<ul>
<li>El sistema de ecuaciones puede escribirse de forma matricial
como</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
Ax = b
\]</span></p>
</div>
<div id="clasificación" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Clasificación</h3>
<p>Podemos clasificar los sistemas de ecuaciones lineales atendiendo
a:</p>
<ul>
<li>Su tamaño:
<ul>
<li>Pequeños: <span class="math inline">\(n \leq 300\)</span> donde
<span class="math inline">\(n\)</span> representa el número de
ecuaciones.</li>
<li>Grandes: <span class="math inline">\(n &gt; 300\)</span></li>
</ul></li>
<li>Su estructura
<ul>
<li>Si la matriz posee pocos elementos nulos diremos que se trata de un
sistema lleno.</li>
<li>Si, por el contrario, la matriz contiene muchos elementos nulos,
diremos que la matriz, y por lo tanto, el sistema lineal es disperso o
sparce.</li>
</ul></li>
</ul>
</div>
<div id="solución" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Solución</h3>
<p>La primera opción que se plantea es</p>
<div class="recuadro-gris">
<p><span class="math display">\[
x = A^{-1}b
\]</span></p>
</div>
<ul>
<li>No es eficiente (demasiadas operaciones).</li>
<li>Si el determinante de <span class="math inline">\(A\)</span> es
próximo a cero, el error de redondeo puede ser muy grande, y esto es
dificil de estimar numéricamente</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\det(\gamma A) = \gamma^n \det(A)
\]</span></p>
</div>
<ul>
<li>Por lo tanto, se hace necesario desarrollar métodos numéricos que
permitan resolver sistemas de ecuaciones lineales de una manera
eficiente y con un control del error cometido.</li>
</ul>
</div>
</div>
<div id="métodos-directos" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Métodos directos</h2>
<div id="sistemas-triangulares" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Sistemas
triangulares</h3>
<ul>
<li>Un sistema de ecuaciones lineales se dice que es triangular superior
si su matriz de coeficientes <span class="math inline">\(A\)</span> es
una matriz triangular superior, es decir, si todos los elementos
situados por debajo de la diagonal principal son nulos.</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\left\{\begin{array}{r}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1i}x_i + \cdots + a_{1n}x_n = b_1 \\
a_{22}x_2 + \cdots + a_{2i}x_i + \cdots + a_{2n}x_n = b_2 \\
\vdots \\
a_{ii}x_i + \cdots + a_{in}x_n = b_i \\
\vdots \\
a_{nn}x_n = b_n\\
\end{array}\right.
\]</span></p>
</div>
<ul>
<li>Como <span class="math inline">\(a_{n,n} \ne 0\)</span>, se puede
despejar <span class="math inline">\(x_n\)</span> de la última ecuación,
y se obtiene:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
x_n = \dfrac{b_n}{a_{nn}}
\]</span></p>
</div>
<ul>
<li>Sustituyendo este valor en la ecuación <span
class="math inline">\(n-1\)</span> y despejando <span
class="math inline">\(x_{n-1}\)</span> se tiene:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[x_{n-1} = \dfrac{b_{n-1} -
a_{n-1,n}x_n}{a_{n-1,n-1}}\]</span></p>
</div>
<ul>
<li>Continuando este proceso, se obtiene la siguiente fórmula de
recurrencia para <span class="math inline">\(i = n-1, n-2, \ldots,
1\)</span>:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[x_i = \dfrac{b_i -
\displaystyle\sum_{j=i+1}^{n} a_{ij}x_j}{a_{ii}}\]</span></p>
</div>
<ul>
<li>Este proceso de resolución de sistemas triangulares superiores se
denomina sustitución regresiva.</li>
</ul>
<div class="cuadro-alg">
<p><strong>FUNCION Sustitucion_Regresiva(A, b):</strong></p>
<pre><code>n = dimension de A (número de filas)

# 1. Resolver la última incógnita (x_n)
x[n] = b[n] / A[n, n]

# 2. Iteración regresiva (desde n-1 hasta 1)
PARA i = n-1 HASTA 1:
    suma = 0
    PARA j = i+1 HASTA n:
        suma = suma + A[i, j] * x[j]
    FIN PARA
    x[i] = (b[i] - suma) / A[i, i]
FIN PARA

RETORNAR x</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
<pre class="python"><code>import numpy as np
def sustitucion_regresiva(A, b):
    n = len(b)
    x = np.zeros(n)
    x[n-1] = b[n-1] / A[n-1, n-1]
    for i in range(n-2, -1, -1):
        suma = np.dot(A[i, i+1:], x[i+1:])
        x[i] = (b[i] - suma) / A[i, i]
    return x</code></pre>
<ul>
<li>En el caso de sistemas triangulares inferiores, el proceso es
análogo, pero se realiza una sustitución progresiva.</li>
<li>La fórmula de recurrencia para <span class="math inline">\(i = 1, 2,
\ldots, n-1, n\)</span> es:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[x_i = \dfrac{b_i -
\displaystyle\sum_{j=1}^{i-1} a_{ij}x_j}{a_{ii}}\]</span></p>
</div>
<ul>
<li>El proceso de resolución de sistemas triangulares inferiores se
denomina sustitución progresiva.</li>
</ul>
<div class="cuadro-alg">
<p><strong>FUNCION Sustitucion_Progresiva(A, b):</strong></p>
<pre><code>n = dimension de A (número de filas)

# 1. Resolver la primera incógnita (x_1)
x[1] = b[1] / A[1, 1]

# 2. Iteración progresiva (desde 2 hasta n)
PARA i = 2 HASTA n:
    suma = 0
    PARA j = 1 HASTA i-1:
        suma = suma + A[i, j] * x[j]
    FIN PARA
    x[i] = (b[i] - suma) / A[i, i]
FIN PARA

RETORNAR x</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
</div>
<div id="eliminación-gaussiana" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Eliminación
Gaussiana</h3>
<ul>
<li>Suponiendo que <span class="math inline">\(A\)</span> no posee una
estructura triangular, y es tal que no requiere intercambio de filas
para transformar el sistema original en otro equivalente que sí la posea
forma triangular.</li>
<li>Una forma de resolver el sistema es transformar la matriz <span
class="math inline">\(A\)</span> en una matriz triangular superior <span
class="math inline">\(U\)</span> mediante operaciones elementales sobre
las filas de la matriz.</li>
<li>Digamos que el sistema original es:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
Ax = b \equiv \left\{
\begin{array}{l}
E_1: a_{11}x_{1}  +  a_{12}x_{2}  + \cdots +  a_{1n}x_{n}  = b_{1} \\
E_2: a_{21}x_{1}  +  a_{22}x_{2}  + \cdots +  a_{2n}x_{n}  = b_{2} \\
\cdots  \cdots  \cdots  \cdots  \cdots \cdots \cdots\\
E_n: a_{n1}x_{1}  +  a_{n2}x_{2}  + \cdots +  a_{nn}x_{n}  = b_{n}
\end{array}
\right.
\]</span></p>
</div>
<ul>
<li>Para eliminar la incógnita <span class="math inline">\(x_1\)</span>
de las ecuaciones <span class="math inline">\(E_2, E_3, \ldots,
E_n\)</span>, se pueden realizar las siguientes operaciones elementales
sobre las filas:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
E_i \leftarrow E_i - \dfrac{a_{i1}}{a_{11}} E_1 \quad \text{para } i =
2, 3, \ldots, n
\]</span></p>
</div>
<ul>
<li>Los números <span
class="math inline">\(\dfrac{a_{i1}}{a_{11}}\)</span> se denominan
multiplicadores de eliminación.</li>
<li>Después de aplicar estas operaciones, se obtiene un nuevo sistema
equivalente:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(1)}x = b^{(1)} \equiv \left\{
\begin{array}{l}
E_1: a_{11}x_{1}  +  a_{12}x_{2}  + \cdots +  a_{1n}x_{n}  = b_{1} \\
E_2: 0  +  a_{22}^{(1)}x_{2}  + \cdots +  a_{2n}^{(1)}x_{n}  =
b_{2}^{(1)} \\
\cdots  \cdots  \cdots  \cdots  \cdots \cdots \cdots\\
E_n: 0  +  a_{n2}^{(1)}x_{2}  + \cdots +  a_{nn}^{(1)}x_{n}  =
b_{n}^{(1)}
\end{array}
\right.
\]</span></p>
</div>
<ul>
<li>Repitiendo este proceso para las incógnitas <span
class="math inline">\(x_2, x_3, \ldots, x_{n-1}\)</span>, se obtiene
finalmente un sistema triangular superior:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
Ux = y \equiv \left\{
\begin{array}{l}
E_1: u_{11}x_{1}  +  u_{12}x_{2}  + \cdots +  u_{1n}x_{n}  = y_{1} \\
E_2: 0  +  u_{22}x_{2}  + \cdots +  u_{2n}x_{n}  = y_{2} \\
\cdots  \cdots  \cdots  \cdots  \cdots \cdots \cdots\\
E_n: 0  +  0  + \cdots +  u_{nn}x_{n}  = y_{n}
\end{array}
\right.
\]</span></p>
</div>
<ul>
<li>Donde <span class="math inline">\(U\)</span> es una matriz
triangular superior y <span class="math inline">\(y\)</span> es el nuevo
vector de términos independientes.</li>
<li>La solución del sistema original <span class="math inline">\(Ax =
b\)</span> se obtiene resolviendo primero el sistema triangular superior
<span class="math inline">\(Ux = y\)</span> mediante sustitución
regresiva.</li>
</ul>
<div class="cuadro-alg">
<p><strong>FUNCION Eliminacion_Gauss(A, b):</strong></p>
<pre><code>n = dimension de A (número de filas)

# 1. Eliminación hacia adelante
PARA k = 1 HASTA n-1:
    PARA i = k+1 HASTA n:
        multiplicador = A[i, k] / A[k, k]
        A[i, k] = 0  # Elemento eliminado
        PARA j = k+1 HASTA n:
            A[i, j] = A[i, j] - multiplicador * A[k, j]
        FIN PARA
        b[i] = b[i] - multiplicador * b[k]
    FIN PARA
FIN PARA
RETORNAR A, b</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
<div id="ejemplo-1" class="section level4 caja-ejemplo"
number="2.2.2.1">
<h4><span class="header-section-number">2.2.2.1</span> Ejemplo 1:</h4>
<p>Resolver el sistema de ecuaciones lineales utilizando el método de
eliminación de Gauss: <span class="math display">\[
\left\{
\begin{array}{rcl}
2x_1 + x_2 + x_3 &amp; = &amp; -3 \\
x_1 -2x_2 + 3x_3 &amp; = &amp; 6 \\
x_1  - x_2 - x_3 &amp; = &amp; 6
\end{array}
\right.
\]</span></p>
<pre class="python"><code>import numpy as np
A = np.array([[2, 1, 1],
              [1, -2, 3],
              [1, -1, -1]], dtype=float)
b = np.array([-3, 6, 6], dtype=float)
def eliminacion_gauss(A, b):
    n = len(b)
    for k in range(n-1):
        for i in range(k+1, n):
            multiplicador = A[i, k] / A[k, k]
            A[i, k] = 0  # Elemento eliminado
            for j in range(k+1, n):
                A[i, j] = A[i, j] - multiplicador * A[k, j]
            b[i] = b[i] - multiplicador * b[k]
    return A, b
A_triangular, b_modificado = eliminacion_gauss(A, b)
print(&quot;Matriz triangular superior U:&quot;)</code></pre>
<pre><code>## Matriz triangular superior U:</code></pre>
<pre class="python"><code>print(A_triangular)</code></pre>
<pre><code>## [[ 2.   1.   1. ]
##  [ 0.  -2.5  2.5]
##  [ 0.   0.  -3. ]]</code></pre>
<pre class="python"><code>print(&quot;Vector modificado y:&quot;)</code></pre>
<pre><code>## Vector modificado y:</code></pre>
<pre class="python"><code>print(b_modificado)</code></pre>
<pre><code>## [-3.   7.5  3. ]</code></pre>
<pre class="python"><code>def sustitucion_regresiva(A, b):
    n = len(b)
    x = np.zeros(n)
    x[n-1] = b[n-1] / A[n-1, n-1]
    for i in range(n-2, -1, -1):
        suma = np.dot(A[i, i+1:], x[i+1:])
        x[i] = (b[i] - suma) / A[i, i]
    return x
solucion = sustitucion_regresiva(A_triangular, b_modificado)
print(&quot;Solución del sistema:&quot;)</code></pre>
<pre><code>## Solución del sistema:</code></pre>
<pre class="python"><code>print(solucion)</code></pre>
<pre><code>## [ 1. -4. -1.]</code></pre>
</div>
</div>
<div id="estrategias-de-pivoteo" class="section level3" number="2.2.3">
<h3><span class="header-section-number">2.2.3</span> Estrategias de
Pivoteo</h3>
<ul>
<li><strong>Pivoteo Parcial</strong></li>
<li>En la eliminación gaussiana, es posible que durante el proceso de
eliminación se encuentren elementos pivote que sean cero o muy pequeños,
lo que puede llevar a errores numéricos significativos.</li>
</ul>
<div id="ejemplo-2" class="section level4 caja-ejemplo"
number="2.2.3.1">
<h4><span class="header-section-number">2.2.3.1</span> Ejemplo 2:</h4>
<p>Considere el siguiente sistema de ecuaciones lineales en una
aritmética de punto flotante a 4 dígitos con redondeo correcto: <span
class="math display">\[
\left\{
\begin{array}{rcl}
0.003000x_1 + 59.14x_2 &amp; = &amp; 59.17 \\
5.291x_1 - 6.130x_2 &amp; = &amp; 46.78
\end{array}
\right.
\]</span></p>
<p>cuya solución exacta es <span class="math inline">\(x_1 = 10\)</span>
y <span class="math inline">\(x_2 = 1\)</span>.</p>
<ul>
<li>Aplicando la eliminación gaussiana sin pivoteo, se obtiene el
siguiente resultado <span
class="math inline">\(\tilde{x}_1=-10.00\)</span> y <span
class="math inline">\(\tilde{x}_2=1.001\)</span>.</li>
<li>El error relativo en la solución es:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{array}{l}
\text{Error relativo en } x_1 = \dfrac{|\tilde{x}_1 - x_1|}{|x_1|} =
\dfrac{|-10.00 - 10|}{|10|} = 2.0 \\
\text{Error relativo en } x_2 = \dfrac{|\tilde{x}_2 - x_2|}{|x_2|} =
\dfrac{|1.001 - 1|}{|1|} = 0.001
\end{array}
\]</span></p>
</div>
<ul>
<li>El error tan grande de la solución numérica de <span
class="math inline">\(x_1\)</span>, resulta del error pequeño de <span
class="math inline">\(0.001\)</span> al resolver para <span
class="math inline">\(x_2\)</span>.</li>
</ul>
</div>
<ul>
<li>Ahora, si se elige como pivote el máximo entre <span
class="math inline">\(a_{1,1}\)</span> y <span
class="math inline">\(a_{2,1}\)</span>.</li>
<li>Pivote =max(|0.003|; |5.291|) = 5.291, por tanto se realiza un
intercambio de filas quedando el sistema de la siguiente manera:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\left\{
\begin{array}{rcl}
5.291x_1 − 6.130x_2 &amp;=&amp; 46.78\\
0.003000x_1 + 59.14x_2 &amp;=&amp; 59.17
\end{array}\right.
\]</span></p>
</div>
<ul>
<li><p>cuya solución aproximada es <span
class="math inline">\(\tilde{x}_2 = 1 = x_2\)</span> y <span
class="math inline">\(\tilde{x}_1 = 10 = x_1\)</span>.</p></li>
<li><p>Por tanto para cada paso de eliminación Gaussiana tenemos
que:</p>
<ul>
<li>Seleccionar como pivote el elemento de mayor valor absoluto en la
columna correspondiente, desde la fila actual hasta la última fila.</li>
<li>Intercambiar la fila actual con la fila que contiene el pivote
seleccionado.</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\text{Pivoteo parcial: } \quad
E_i \leftrightarrow E_p \quad \text{donde } p = \underset{i \leq k \leq
n}{\mathrm{argmax}} |a_{k,i}|
\]</span></p>
</div></li>
<li><p><strong>Pivoteo Parcial Escalado</strong></p></li>
<li><p>Modificando ligeramente el <strong>pivotaje parcial</strong> de
la siguiente forma.</p></li>
<li><p>Concretamente, en primer lugar, se calcula el valor máximo en
valor absoluto de cada fila <span class="math inline">\(i\)</span>-ésima
de la matriz del sistema a resolver <span
class="math inline">\(A\)</span>, <span class="math inline">\(n\times
n\)</span>:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
s_i = \max_{1 \leq j \leq n} |a_{ij}| \quad \text{para } i = 1, 2,
\ldots, n
\]</span></p>
</div>
<p>*Dichos valores <span class="math inline">\(s_i\)</span> sólo se
calculan una vez al principio del algoritmo y no aumentan demasiado el
coste computacional del mismo.</p>
<ul>
<li>A continuación, imaginemos que estamos en el <span
class="math inline">\(k\)</span>-ésimo pase del <strong>algoritmo de
Gauss</strong>. Entonces, en lugar de hallar el máximo de <span
class="math inline">\(|a^{(k)}_{ji}|\)</span> desde <span
class="math inline">\(j=k\)</span> hasta <span
class="math inline">\(j=n\)</span>, se halla el máximo de <span
class="math inline">\(\left|\frac{a^{(k)}_{ji}}{s_j}\right|\)</span>
desde <span class="math inline">\(j=i\)</span> hasta <span
class="math inline">\(j=n\)</span>:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\max_{k \leq j \leq n} \left|\dfrac{a^{(k)}_{ji}}{s_j}\right|
\]</span></p>
</div>
<ul>
<li>Es decir, “escalamos” cada fila por el valor <span
class="math inline">\(s_j\)</span> y hallamos el máximo de los valores
escalados.</li>
<li>Llamemos <span class="math inline">\(j_{\max}\)</span> a dicho
valor.</li>
<li>Igual que en el pivoto parcial, realizamos un cambio entre las filas
<span class="math inline">\(k\)</span> y <span
class="math inline">\(j_{\max}\)</span>: <span class="math inline">\(E_k
\leftrightarrow E_{j_{\max}}\)</span>.</li>
<li>El <strong>pivoteo parcial escalado</strong> es especialmente útil
cuando los elementos de la matriz del sistema varían mucho en
magnitud.</li>
</ul>
<div id="ejemplo-3" class="section level4 caja-ejemplo"
number="2.2.3.2">
<h4><span class="header-section-number">2.2.3.2</span> Ejemplo 3:</h4>
<p>Resolvamos el siguiente sistema usando pivotaje parcial escalado:</p>
<div class="recuadro-gris">
<p><span class="math display">\[
\left\{
\begin{array}{rrrrrrcr}
E_1:  &amp; x_1 &amp; + x_2 &amp; - 3x_3  &amp; -x_4 &amp; -2x_5 &amp; =
&amp; 2 \\
E_2:  &amp; x_1 &amp; + 2x_2 &amp; + 2x_3  &amp;  &amp; + 3x_5 &amp; =
&amp; 2 \\
E_3:  &amp; x_1 &amp; -x_2 &amp; +3x_3  &amp; +2x_4 &amp; - x_5 &amp; =
&amp; 2 \\
E_4:  &amp;  &amp; x_2 &amp;   &amp; + 4x_4 &amp; -x_5 &amp; = &amp; 3
\\
E_5:  &amp; x_1 &amp; + 3x_2 &amp; + x_3  &amp;  &amp; + 5x_5 &amp; =
&amp; 1 \\
\end{array}
\right.
\]</span></p>
</div>
<p>Los valores <span class="math inline">\(s_i\)</span> son en este
caso: <span class="math display">\[
s_1 = 3, \quad s_2 = 3, \quad s_3 = 3, \quad s_4 = 4, \quad s_5 = 5
\]</span> * Primer paso (no se ha realizado ningún cambio de filas ya
que el máximo valor entre los valores <span class="math display">\[
\left|\dfrac{1}{3}\right|, \left|\dfrac{1}{3}\right|,
\left|\dfrac{1}{3}\right|, 0, \left|\dfrac{1}{5}\right|
\]</span></p>
<p>es <span class="math inline">\(\dfrac{1}{3}\)</span>. Aplicamos la
eliminación gaussiana para obtener:</p>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(1)} = \begin{pmatrix}
1 &amp; 1 &amp; -3 &amp; -1 &amp; -2 \\
0 &amp; 1 &amp; 5 &amp; 1 &amp; 5 \\
0 &amp; -2 &amp; 6 &amp; 3 &amp; 1 \\
0 &amp; 1 &amp; 0 &amp; 4 &amp; -1 \\
0 &amp; 2 &amp; 4 &amp; 1 &amp; 7
\end{pmatrix}, \quad
b^{(1)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1
\end{pmatrix}
\]</span></p>
</div>
<ul>
<li>Segundo paso (cambiando las filas 2 y 3 ya que el máximo valor entre
los valores <span class="math display">\[
\left|\dfrac{1}{5}\right|, \left|\dfrac{-2}{6}\right|,
\left|\dfrac{1}{4}\right|, \left|\dfrac{2}{7}\right|
\]</span> es <span
class="math inline">\(\dfrac{2}{6}=\dfrac{1}{3}\)</span>. Aplicamos la
eliminación gaussiana para obtener:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(2)} = \begin{pmatrix}
1 &amp; 1 &amp; -3 &amp; -1 &amp; -2 \\
0 &amp; -2 &amp; 6 &amp; 3 &amp; 1 \\
0 &amp; 0 &amp; 8 &amp; 2.5 &amp; 5.5  \\
0 &amp; 0 &amp; 3 &amp; 5.5 &amp; -0.5 \\
0 &amp; 0 &amp; 10 &amp; 4 &amp; 8
\end{pmatrix}, \quad
b^{(2)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1
\end{pmatrix}
\]</span></p>
</div>
<ul>
<li>Tercer paso (no hay cambio de filas ya que el máximo valor entre los
valores <span class="math display">\[
\left|\dfrac{8}{8}\right|, \left|\dfrac{3}{5.5}\right|,
\left|\dfrac{10}{10}\right|
\]</span> es <span class="math inline">\(\dfrac{8}{8}=1\)</span>.
Aplicamos la eliminación gaussiana para obtener:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(3)} = \begin{pmatrix}
1 &amp; 1 &amp; -3 &amp; -1 &amp; -2 \\
0 &amp; -2 &amp; 6 &amp; 3 &amp; 1 \\
0 &amp; 0 &amp; 8 &amp; 2.5 &amp; 5.5  \\
0 &amp; 0 &amp; 0 &amp; 4.5625 &amp; -2.5625\\
0 &amp; 0 &amp; 0 &amp; 0.8750 &amp; 1.1250
\end{pmatrix}, \quad
b^{(3)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1
\end{pmatrix}
\]</span></p>
</div>
<ul>
<li>Cuarto paso (no hay cambio de filas ya que el máximo valor entre los
valores <span class="math display">\[
\left|\dfrac{4.5625}{4.5625}\right|, \left|\dfrac{0.875}{1.5}\right|
\]</span> es <span
class="math inline">\(\dfrac{4.5625}{4.5625}=1\)</span>. Aplicamos la
eliminación gaussiana para obtener:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
U = A^{(4)} = \begin{pmatrix}
1 &amp; 1 &amp; -3 &amp; -1 &amp; -2 \\
0 &amp; -2 &amp; 6 &amp; 3 &amp; 1 \\
0 &amp; 0 &amp; 8 &amp; 2.5 &amp; 5.5  \\
0 &amp; 0 &amp; 0 &amp; 4.5625 &amp; -2.5625\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1.6164
\end{pmatrix}, \quad
y = b^{(4)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1.5753
\end{pmatrix}
\]</span></p>
</div>
<ul>
<li>Finalmente, resolvemos el sistema triangular superior <span
class="math inline">\(Ux = y\)</span> mediante sustitución regresiva
para obtener la solución del sistema original:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{align*}
x_5 &amp; = \dfrac{-1.5753}{1.6164} \approx -0.97457 \\
x_4 &amp; = \dfrac{3 - (-2.5625)(-0.97457)}{4.5625} \approx 0.11018 \\
x_3 &amp; = \dfrac{0 - 2.5(0.11018) - 5.5(-0.97457)}{8} \approx 0.63559
\\
x_2 &amp; = \dfrac{0 - 6(0.63559) - 3(0.11018)-1(-0.97457)}{-2} \approx
1.5848 \\
x_1 &amp; = 2 - 1(1.5848) - (-3)(0.63559) - (-1)(0.11018) -
(-2)(-0.97457) \approx 0.48310
\end{align*}
\]</span></p>
</div>
</div>
<ul>
<li><strong>Pivoteo Completo o Total</strong></li>
<li>Una manera de optimizar el error de redondeo cometido cuando
aplicamos el algoritmo de Gauss es realizar un pivotaje global: -es
decir, modificar el algoritmo de Gauss en el <span
class="math inline">\(k\)</span>-ésimo paso eligiendo el máximo valor en
valor absoluto de la submatriz de <span
class="math inline">\(A^(k)\)</span> formada por las filas <span
class="math inline">\(k,k+1,\ldots,n\)</span> y por las columnas <span
class="math inline">\(k,k+1,\ldots,n\)</span>.
<ul>
<li>Concretamente hallamos <span
class="math inline">\(i_{\max}\)</span>,<span
class="math inline">\(j_{\max}\)</span> tal que:</li>
</ul></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\left|a^{(k)}_{i_{\max},j_{\max}}\right| =
\max_{i,j=k,\ldots,n}\left|a^{(k)_{i,j}}\right|
\]</span></p>
</div>
<ul>
<li>A continuación, realizamos un cambio entre las filas <span
class="math inline">\(k\)</span> y <span
class="math inline">\(i_{\max}\)</span> y un cambio entre las columnas
<span class="math inline">\(k\)</span> y <span
class="math inline">\(j_{\max}\)</span>.</li>
<li><strong>Permutar filas</strong> no afecta a las soluciones del
sistema de ecuaciones al obtener un sistema de ecuaciones equivalente.
Sin embargo, <strong>permutar columnas</strong> equivale a permutar los
valores de las variables correspondientes.</li>
<li>Concretamente, cuando cambiamos las columnas <span
class="math inline">\(k\)</span> y <span
class="math inline">\(j_{\max}\)</span>, intercambiamos el “papel” de
las variables <span class="math inline">\(x_k\)</span> y <span
class="math inline">\(x_{j_{\max}}\)</span>.</li>
<li>Por tanto, necesitamos recordar todos los cambios de columnas para
realizar el cambio inverso en la solución final.</li>
</ul>
<div id="ejemplo-4" class="section level4 caja-ejemplo"
number="2.2.3.3">
<h4><span class="header-section-number">2.2.3.3</span> Ejemplo 4</h4>
<p>Aplicando esta tecnica de pivoteo al ejemplo anterior</p>
<div class="recuadro-gris">
<p><span class="math display">\[
\left\{
\begin{array}{rrrrrrcr}
E_1:  &amp; x_1 &amp; + x_2 &amp; - 3x_3  &amp; -x_4 &amp; -2x_5 &amp; =
&amp; 2 \\
E_2:  &amp; x_1 &amp; + 2x_2 &amp; + 2x_3  &amp;  &amp; + 3x_5 &amp; =
&amp; 2 \\
E_3:  &amp; x_1 &amp; -x_2 &amp; +3x_3  &amp; +2x_4 &amp; - x_5 &amp; =
&amp; 2 \\
E_4:  &amp;  &amp; x_2 &amp;   &amp; + 4x_4 &amp; -x_5 &amp; = &amp; 3
\\
E_5:  &amp; x_1 &amp; + 3x_2 &amp; + x_3  &amp;  &amp; + 5x_5 &amp; =
&amp; 1 \\
\end{array}
\right. \equiv
\left(\begin{array}{rrrrr|r}
1 &amp; 1 &amp; -3 &amp; -1 &amp; -2 &amp; 2\\
1 &amp; 2 &amp; 2 &amp; 0 &amp; 3 &amp; 2\\
1 &amp; -1 &amp; 3 &amp; 2&amp; -1 &amp; 2\\
0 &amp; 1 &amp; 0 &amp; 4 &amp; -1 &amp; 3\\
1 &amp; 3 &amp; 1 &amp; 0 &amp; 5 &amp; 1\\
\end{array}\right)
\]</span></p>
</div>
<ul>
<li>Paso 1. El máximo valor de la submatriz del sistema formada por las
5 primeras filas y columnas vale 5 que corresponde a la fila 5 y a la
columna 5. Entonces permutamos la fila 1 con la 5 y a continuación la
columna 1 con la 5 y aplicamos el algoritmo de Gauss obteniendo la
matriz siguiente:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(1)}=\left(\begin{array}{rrrrr|r}
  5 &amp; 3 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\
  0 &amp; 0.2 &amp; 1.4 &amp; 0 &amp; 0.4 &amp; 1.4\\
  0 &amp; -0.4 &amp; 3.2 &amp; 2 &amp; 1.2 &amp; 2.2\\
  0 &amp; 1.6 &amp; 0.2 &amp; 4 &amp; 0.2 &amp; 3.2\\
  0 &amp; 2.2 &amp; -2.6 &amp; -1 &amp; 1.4 &amp; 2.4
  \end{array}\right)
\]</span></p>
</div>
<ul>
<li><p>Las variables del sistema anterior quedan en el siguiente orden
<span class="math inline">\(x_5\)</span>, <span
class="math inline">\(x_2\)</span>, <span
class="math inline">\(x_3\)</span>, <span
class="math inline">\(x_4\)</span> y <span
class="math inline">\(x_1\)</span>.</p></li>
<li><p>Paso 2. El máximo valor de la submatriz del sistema formada por
las 4 últimas filas y columnas vale 4 que corresponde a la fila 4 y a la
columna 4. Entonces permutamos la fila 2 con la 4 y a continuación la
columna 2 con la 4 y aplicamos el algoritmo de Gauss obteniendo la
matriz siguiente:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(2)}=\left(\begin{array}{rrrrr|r}
   5 &amp; 0 &amp; 1 &amp; 3 &amp; 1 &amp; 1\\
   0 &amp; 4 &amp; 0.2 &amp; 1.6 &amp; 0.2 &amp; 3.2\\
   0 &amp; 0 &amp; 3.1 &amp; -1.2 &amp; 1.1 &amp; 0.6\\
   0 &amp; 0 &amp; 1.4 &amp; 0.2 &amp; 0.4 &amp; 1.4\\
   0 &amp; 0 &amp; -2.55 &amp; 2.6 &amp; 1.45 &amp; 3.2
\end{array}
\right)
\]</span></p>
</div>
<ul>
<li><p>Las variables del sistema anterior son las siguientes <span
class="math inline">\(x_5\)</span>, <span
class="math inline">\(x_4\)</span>, <span
class="math inline">\(x_3\)</span>, <span
class="math inline">\(x_2\)</span> y <span
class="math inline">\(x_1\)</span>.</p></li>
<li><p>Paso 3. El máximo valor de la submatriz del sistema formada por
las 3 últimas filas y columnas vale <span
class="math inline">\(3.1\)</span> que corresponde a la fila 3 y a la
columna 3. Entonces no realizamos ninguna permutación y aplicamos el
algoritmo de Gauss obteniendo la matriz siguiente:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(3)}\left(
\begin{array}{rrrrr|r}
  5 &amp; 0 &amp; 1 &amp; 3 &amp; 1 &amp; 1\\
  0 &amp; 4 &amp; 0.2 &amp; 1.6 &amp; 0.2 &amp; 3.2\\
  0 &amp; 0 &amp; 3.1 &amp; -1.2 &amp; 1.1 &amp; 0.6\\
  0 &amp; 0 &amp; 0 &amp; 0.74194 &amp; -0.096774 &amp; 1.1290\\
  0 &amp; 0 &amp; 0 &amp; 1.6129 &amp; 2.3548 &amp; 3.6935\\
\end{array}
\right)
\]</span></p>
</div>
<ul>
<li>Paso 4. El máximo valor de la submatriz del sistema formada por las
2 últimas filas y columnas vale <span
class="math inline">\(2.3548\)</span> que corresponde a la fila 5 y a la
columna 5. Entonces permutamos la fila 4 con la 5 y a continuación la
columna 4 con la 5 y aplicamos el algoritmo de Gauss obteniendo la
matriz siguiente:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A^{(4)}\left(\begin{array}{rrrrr|r}
   5 &amp; 0 &amp; 1 &amp; 1 &amp; 3 &amp; 1\\
   0 &amp; 4 &amp; 0.2 &amp; 0.2 &amp; 1.6 &amp; 3.2\\
   0 &amp; 0 &amp; 3.1 &amp; 1.1 &amp; -1.2 &amp; 0.6\\
   0 &amp; 0 &amp; 0 &amp; 2.3548 &amp; 1.6129 &amp; 3.6935\\
   0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.80822 &amp; 1.2808
\end{array}\right)
\]</span></p>
</div>
<ul>
<li><p>Las variables del sistema anterior son las siguientes <span
class="math inline">\(x_5\)</span>, <span
class="math inline">\(x_4\)</span>, <span
class="math inline">\(x_3\)</span>, <span
class="math inline">\(x_1\)</span> y <span
class="math inline">\(x_2\)</span>.</p></li>
<li><p>La solución del sistema será usando el método de sustitución
hacia atrás:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{align*}
x_2 &amp; = \dfrac{1.2808}{0.80822} \approx 1.5847 \\
x_1 &amp; = \dfrac{3.6935 - 1.6129(1.5847)}{2.3548} \approx 0.48306 \\
x_3 &amp; = \dfrac{0.6 - (1.1)(0.48306) +1.2 (1.5847)}{3.1} \approx
0.63555 \\
x_4 &amp; = \dfrac{3.2 - 0.2(0.63555) - 0.2(0.48306) - 1.6(1.5847)}{4.0}
\approx 0.11020 \\
x_5 &amp; = \dfrac{1 - 0(0.11020) - 1(0.63555) - 1(0.48306) -
3(1.5847)}{0.5} \approx -0.97456
\end{align*}
\]</span></p>
</div>
</div>
</div>
<div id="conteo-de-operaciones" class="section level3" number="2.2.4">
<h3><span class="header-section-number">2.2.4</span> Conteo de
operaciones</h3>
<ul>
<li>El número total de operaciones (sumas, restas, multiplicaciones y
divisiones) necesarias para resolver un sistema de <span
class="math inline">\(n\)</span> ecuaciones con <span
class="math inline">\(n\)</span> incógnitas mediante el método de
eliminación gaussiana es aproximadamente:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math inline">\((+/-)\)</span> <span
class="math display">\[
\sum_{k=1}^{n-1} ((n-k)^2 + (n-k)) = \dfrac{1}{3}n^3 - \dfrac{1}{3}n
\approx \mathcal{O}(n^3)
\]</span> <span class="math inline">\((×/÷)\)</span> <span
class="math display">\[
\sum_{k=1}^{n-1} (2(n-k) + (n-k)^2) = \dfrac{2n^3+3n^2-5n}{6} \approx
\mathcal{O}(n^3)
\]</span></p>
</div>
</div>
<div id="número-de-condición" class="section level3" number="2.2.5">
<h3><span class="header-section-number">2.2.5</span> Número de
condición</h3>
<ul>
<li>¿Cómo se evalua el buen o mal condicionamiento de la matriz de
coeficientes?</li>
<li>El número de condioción de una matriz permite cuantificar su nivel
de condicionamiento.</li>
<li>El número de condición de una matriz <span
class="math inline">\(A\)</span> se define como:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[\kappa(A) = ||A|| \cdot ||A^{-1}||
\]</span></p>
</div>
<ul>
<li>Donde <span class="math inline">\(||\cdot||\)</span> denota una
norma matricial.</li>
<li>Si <span class="math inline">\(\kappa(A)\)</span> es cercano a 1, la
matriz está bien condicionada.</li>
<li>Si <span class="math inline">\(\kappa(A)\)</span> es grande, la
matriz está mal condicionada.</li>
</ul>
<div id="ejemplo-5" class="section level4 caja-ejemplo"
number="2.2.5.1">
<h4><span class="header-section-number">2.2.5.1</span> Ejemplo 5:</h4>
<p>Calcular el número de condición de la siguiente matriz utilizando la
norma 2: <span class="math display">\[A = \begin{pmatrix}
2 &amp; 3.01 \\
4 &amp; 6
\end{pmatrix}\]</span></p>
<pre class="python"><code>import numpy as np
A = np.array([[2, 3.01],
              [4, 6]], dtype=float)
def numero_de_condicion(A):
    norma_A = np.linalg.norm(A, 2)
    norma_A_inv = np.linalg.norm(np.linalg.inv(A), 2)
    kappa = norma_A * norma_A_inv
    return kappa
kappa_A = numero_de_condicion(A)
print(&quot;Número de condición de A:&quot;, kappa_A)</code></pre>
<pre><code>## Número de condición de A: 1626.501885183655</code></pre>
<pre class="python"><code># Alternativamente, usando la función cond de NumPy
print(np.linalg.cond(A, 2))</code></pre>
<pre><code>## 1626.5018851838065</code></pre>
</div>
<ul>
<li>Una cota válida para el nuúmero de condición es:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[\kappa(A) \geq 1 \]</span></p>
</div>
<ul>
<li>Si <span class="math inline">\(\kappa(A)\)</span> es muy grande,
pequeños errores en los datos de entrada pueden provocar grandes errores
en la solución del sistema.</li>
</ul>
</div>
</div>
<div id="descomposición-de-matrices" class="section level2"
number="2.3">
<h2><span class="header-section-number">2.3</span> Descomposición de
Matrices</h2>
<ul>
<li>Al aplicar el método de Gauss al sistema <span
class="math inline">\(Ax = b\)</span> se realizan transformaciones
elementales para triangularizar la matriz del sistema.</li>
<li>Si este proceso puede realizarse sin intercambios de filas, la
matriz triangular superior <span class="math inline">\(U\)</span>
obtenida viene determinada por el producto de un número finito de
transformaciones filas <span class="math inline">\(N_{n−1}N_{n−2}\cdots
N_2N_1\)</span> aplicadas a la matriz <span
class="math inline">\(A\)</span>. Llamando <span
class="math inline">\(L^{−1} = N_{n−1}N_{n−2}\cdots N_2N_1\)</span> se
tiene que</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
L^{−1}A = U \Rightarrow A = LU.
\]</span></p>
</div>
<ul>
<li>Además <span class="math inline">\(L\)</span> es una matriz
triangular inferior con unos en la diagonal.</li>
<li>Entonces para resolver el sistema <span class="math inline">\(Ax =
b\)</span>, sabiendo que <span class="math inline">\(A\)</span> posee
descomposición <span class="math inline">\(LU\)</span>, debemos hacer lo
siguiente:
<ul>
<li><span class="math inline">\(Ax = b \Rightarrow LUx = b\)</span>,
llamando <span class="math inline">\(z\)</span> al producto <span
class="math inline">\(Ux\)</span> tenemos que:</li>
<li><span class="math inline">\(Lz = b \Rightarrow\)</span> sustitución
progresiva.</li>
<li><span class="math inline">\(Ux = z \Rightarrow\)</span> sustitución
regresiva.</li>
</ul></li>
</ul>
<p>Si <span class="math inline">\(A\)</span> es estrictamente diagonal
dominante entonces <span class="math inline">\(A\)</span> admite una
descomposición <span class="math inline">\(LU\)</span> que se obtiene
mediante el proceso de eliminación Gaussiana.</p>
<div class="recuadro-gris">
<p>Se dice que una matriz <span class="math inline">\(A\)</span> es
diagonalmente dominante si <span class="math display">\[
|a_{ii}| &gt; \sum_{\substack{j=1 \\ j \ne i}}^{n} |a_{ij}| \quad
\text{para } i = 1, 2, \ldots, n
\]</span></p>
</div>
<div class="cuadro-alg">
<p><strong>FUNCION Factorizacion_LU_Doolittle(A):</strong> # Asume que A
es una matriz cuadrada (n x n) n = dimension de A (número de filas) L =
Matriz Identidad n x n U = Matriz n x n (copia de A, que será
modificada)</p>
<pre><code># El algoritmo se ejecuta en A y sus elementos modificados forman L y U.
# Por simplicidad, calculamos U in-place en A y L en una matriz separada.

PARA k = 1 HASTA n:

    # 1. Cálculo de U_{k,k} (Elemento Pivote de la fila k)
    #    u_{k,k} = a_{k,k} - SUM_{p=1}^{k-1} l_{k,p} * u_{p,k}
    
    # El elemento pivote u_kk ya está en A[k,k] si no hay pivoteo parcial.

    # 2. CÁLCULO DE LA FILA k DE U: (j = k+1 hasta n)
    PARA j = k+1 HASTA n:
        # u_{k,j} = a_{k,j} - SUM_{p=1}^{k-1} l_{k,p} * u_{p,j}
        suma_U = 0
        PARA p = 1 HASTA k-1:
            suma_U = suma_U + L[k, p] * U[p, j] 
        FIN PARA
        U[k, j] = A[k, j] - suma_U  # Guardamos el resultado en U
    FIN PARA

    # 3. CÁLCULO DE LA COLUMNA k DE L: (i = k+1 hasta n)
    # L_{i,k} = (1 / U_{k,k}) * [ a_{i,k} - SUM_{p=1}^{k-1} l_{i,p} * u_{p,k} ]
    SI U[k, k] es CASI CERO ENTONCES
        RETORNAR &quot;Falla: Pivoteo necesario o matriz singular.&quot;
    FIN SI

    factor = 1.0 / U[k, k]

    PARA i = k+1 HASTA n:
        # l_{i,k}
        suma_L = 0
        PARA p = 1 HASTA k-1:
            suma_L = suma_L + L[i, p] * U[p, k]
        FIN PARA
        
        L[i, k] = factor * (A[i, k] - suma_L) 
    FIN PARA

FIN PARA

RETORNAR L, U</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
<div id="ejemplo-6" class="section level4 caja-ejemplo"
number="2.3.0.1">
<h4><span class="header-section-number">2.3.0.1</span> Ejemplo 6:</h4>
<p>Realizar la descomposición LU de la siguiente matriz: <span
class="math display">\[A = \begin{pmatrix}
1 &amp; 2 &amp; 0 \\
-1 &amp; 1 &amp; 3 \\
2 &amp; 1 &amp; -1
\end{pmatrix}\]</span></p>
<pre class="python"><code>import numpy as np
A = np.array([[1, 2, 0],
              [-1, 1, 3],
              [2, 1, -1]], dtype=float)
def factorizacion_LU_doolittle(A):
    n = A.shape[0]
    L = np.eye(n)
    U = np.zeros((n, n))
    for k in range(n):
        suma_U = sum(L[k, p] * U[p, k] for p in range(k))
        U[k, k] = A[k, k] - suma_U
        for j in range(k+1, n):
            suma_U = sum(L[k, p] * U[p, j] for p in range(k))
            U[k, j] = A[k, j] - suma_U
        factor = 1.0 / U[k, k]
        for i in range(k+1, n):
            suma_L = sum(L[i, p] * U[p, k] for p in range(k))
            L[i, k] = factor * (A[i, k] - suma_L)
    return L, U
L, U = factorizacion_LU_doolittle(A)
print(&quot;Matriz L:&quot;)</code></pre>
<pre><code>## Matriz L:</code></pre>
<pre class="python"><code>print(L)</code></pre>
<pre><code>## [[ 1.  0.  0.]
##  [-1.  1.  0.]
##  [ 2. -1.  1.]]</code></pre>
<pre class="python"><code>print(&quot;Matriz U:&quot;)</code></pre>
<pre><code>## Matriz U:</code></pre>
<pre class="python"><code>print(U)</code></pre>
<pre><code>## [[1. 2. 0.]
##  [0. 3. 3.]
##  [0. 0. 2.]]</code></pre>
<pre class="python"><code>print(&quot;Verificación LU:&quot;)</code></pre>
<pre><code>## Verificación LU:</code></pre>
<pre class="python"><code>print(np.dot(L, U))</code></pre>
<pre><code>## [[ 1.  2.  0.]
##  [-1.  1.  3.]
##  [ 2.  1. -1.]]</code></pre>
</div>
<div id="factorización-lu-con-pivoteo" class="section level3"
number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Factorización LU
con Pivoteo</h3>
<ul>
<li>En la práctica, para garantizar la estabilidad numérica del proceso
de eliminación Gaussiana, es necesario incorporar el pivoteo parcial en
la descomposición LU.</li>
<li>Esto implica que durante el proceso de eliminación, se selecciona el
pivote como el elemento de mayor valor absoluto en la columna actual, y
se intercambian las filas correspondientes.</li>
</ul>
<p>Definición de matriz de permutación. Diremos que una matriz <span
class="math inline">\(P_{ij}=(p_{kl})\, k,l=1,\ldots,n\)</span>, <span
class="math inline">\(n\times n\)</span>, es una <strong>matriz de
permutación</strong> si</p>
<div class="recuadro-gris">
<p><span class="math display">\[
p_{kl} = \begin{cases}
1, &amp; \text{si }k=l\neq i,j,\\
1, &amp; \text{si }k=i,l= j,\\
1 &amp; \text{si }k=j, l=i,\\
0 &amp; \text{en caso contrario}
\end{cases}
\]</span></p>
</div>
<ul>
<li>Al utilizar eliminación Gaussiana con pivoteo en cada paso se
introduce una matriz de permutación (que puede ser la identidad) de modo
que:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
N_{n-1}P_{n-1}N_{n-2}P_{n-2}\cdots N_2P_2N_1P_1A = U
\]</span></p>
</div>
<ul>
<li>Donde <span class="math inline">\(P_i\)</span> es la matriz de
permutación que intercambia las filas correspondientes en el paso <span
class="math inline">\(i\)</span>.</li>
<li>Llamando <span class="math inline">\(P = P_1P_2\cdots
P_{n-1}\)</span> y <span class="math inline">\(L^{-1} =
N_{n-1}P_{n-1}N_{n-2}P_{n-2}\cdots N_2P_2N_1P_1\)</span> se tiene
que:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[  
L^{-1}A = U \Rightarrow PA = PP^{-1}LU \Rightarrow PA = LU
\]</span></p>
</div>
<ul>
<li>Por lo tanto para resolver el sistema <span class="math inline">\(Ax
= b\)</span> premultiplicando por <span
class="math inline">\(P\)</span>, queda <span class="math inline">\(PAx
= Pb\)</span>, y dado que <span class="math inline">\(PA = LU\)</span>,
entonces:
<ul>
<li><span class="math inline">\(LUx = Pb\)</span>, llamando <span
class="math inline">\(z\)</span> al producto <span
class="math inline">\(Ux\)</span> tenemos que:</li>
<li><span class="math inline">\(Lz = Pb \Rightarrow\)</span> sustitución
progresiva.</li>
<li><span class="math inline">\(Ux = z \Rightarrow\)</span> sustitución
regresiva.</li>
</ul></li>
</ul>
<div id="ejemplo-7" class="section level4 caja-ejemplo"
number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> Ejemplo 7:</h4>
<p>Realizar la descomposición <span class="math inline">\(PA=LU\)</span>
con pivoteo de la siguiente matriz: <span class="math display">\[
A = \begin{pmatrix}
0 &amp; 2 &amp; 0 \\
-1 &amp; 1 &amp; 3 \\
2 &amp; 1 &amp; -1
\end{pmatrix}
\]</span></p>
<div class="recuadro-gris">
<p><span class="math display">\[
P_1 = \begin{pmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0
\end{pmatrix} \qquad P_1A = \begin{pmatrix}
2 &amp; 1 &amp; -1 \\
-1 &amp; 1 &amp; 3 \\
0 &amp; 2 &amp; 0
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
N_1= \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0.5 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix} \qquad N_1P_1A = \begin{pmatrix}
2 &amp; 1 &amp; -1 \\
0 &amp; 1.5 &amp; 2.5 \\
0 &amp; 2 &amp; 0
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
P_2 = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{pmatrix} \qquad P_2N_1P_1A = \begin{pmatrix}
2 &amp; 1 &amp; -1 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 1.5 &amp; 2.5
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
N_2 = \begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -0.75 &amp; 1
\end{pmatrix} \qquad N_2P_2N_1P_1A = \begin{pmatrix}
2 &amp; 1 &amp; -1 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 2.5
\end{pmatrix} = U
\]</span></p>
<p><span class="math display">\[
L^{-1} = N_2P_2N_1P_1 = \begin{pmatrix}
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 \\
-0.75 &amp; 1 &amp; 0.5
\end{pmatrix} \Rightarrow L = \begin{pmatrix}
0 &amp; 1 &amp; 0 \\
-0.5 &amp; 0.75 &amp; 1 \\
1 &amp; 0 &amp; 0
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
PA = LU \quad \text{con} \quad P = P_2P_1 = \begin{pmatrix}
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0
\end{pmatrix} \qquad
\]</span></p>
<p><span class="math display">\[
LU = \begin{pmatrix}
2 &amp; 1 &amp; -1 \\
0 &amp; 2 &amp; 0 \\
-1 &amp; 1 &amp; 3
\end{pmatrix} = PA
\]</span></p>
</div>
</div>
</div>
<div id="factorización-de-cholesky" class="section level3"
number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Factorización de
Cholesky</h3>
<ul>
<li>La factorización de Cholesky es un método de descomposición de
matrices que permite expresar una <strong>matriz simétrica</strong> y
<strong>definida positiva</strong> como el producto de una matriz
triangular inferior y su transpuesta.</li>
<li>Este método es especialmente útil en la resolución de sistemas de
ecuaciones lineales, optimización y simulaciones en estadística y
finanzas, facilitando cálculos en diversos campos como la estadística y
la ingeniería.</li>
</ul>
<p><strong>Proposición (Criterio de Sylvester de matriz definida
positiva)</strong></p>
<p>Sea <span class="math inline">\(A\)</span> una matriz real simétrica
<span class="math inline">\(n \times n\)</span>. Dicha matriz es
definida positiva si y sólo si todos sus menores principales son
positivos, es decir, para todo <span
class="math inline">\(k=1,\ldots,n\)</span>, <span
class="math inline">\(\det(A_k)&gt;0\)</span>, donde <span
class="math inline">\(A_k\)</span> recordemos que es la submatriz
formada por las <span class="math inline">\(k\)</span> primeras filas y
columnas de la matriz <span class="math inline">\(A\)</span>.</p>
<p>Partiendo de que <span class="math inline">\(A = LU\)</span> y <span
class="math inline">\(A\)</span> es simétrica tenemos que:</p>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{align*}
LU = A = A^T = (LU)^T = U^TL^T &amp; \Rightarrow LU = U^TL^T\\
LU(L^T)^{−1} = U^TL^T(L^T)^{−1} &amp; \Rightarrow LU(L^T)^{−1} = U^T\\
L^{−1}LU(L^T)^{−1} = L^{−1}U^T &amp; \Rightarrow U(L^T)^{−1} =
L^{−1}U^T\\
U(L^T)^{−1} = D \Rightarrow U(L^T)^{−1}L^T = DL^T &amp; \Rightarrow U =
DL^T \Rightarrow A = LDL^T
\end{align*}
\]</span></p>
</div>
<ul>
<li>Donde <span class="math inline">\(D\)</span> es una matriz
diagonal.</li>
<li>Si <span class="math inline">\(A\)</span> es definida positiva, los
elementos diagonales de <span class="math inline">\(D\)</span> son
positivos, por lo que podemos escribir <span class="math inline">\(D =
D^{1/2}D^{1/2}\)</span>.</li>
<li>Entonces:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
A = LDL^T = (LD^{1/2})(D^{1/2}L^T) = LL^T
\]</span></p>
</div>
<ul>
<li>Donde <span class="math inline">\(L = LD^{1/2}\)</span> es una
matriz triangular inferior con elementos positivos en la diagonal.</li>
<li>Por tanto, la factorización de Cholesky de una matriz simétrica
definida positiva <span class="math inline">\(A\)</span> es <span
class="math inline">\(A = LL^T\)</span>.</li>
<li>La factorización de Cholesky se puede calcular mediante el siguiente
algoritmo:</li>
</ul>
<div class="cuadro-alg">
<p><strong>FUNCION Factorizacion_Cholesky(A):</strong> # Asume que A es
una matriz simétrica definida positiva (n x n) n = dimension de A
(número de filas) L = Matriz n x n (inicializada a ceros)</p>
<pre><code>PARA i = 1 HASTA n:
    # Cálculo de los elementos de la fila i de L
    PARA j = 1 HASTA i:
        suma = 0
        PARA k = 1 HASTA j-1:
            suma = suma + L[i, k] * L[j, k]
        FIN PARA
        
        SI i == j ENTONCES
            L[i, j] = sqrt(A[i, i] - suma)  # Elemento diagonal
        SINO
            L[i, j] = (1 / L[j, j]) * (A[i, j] - suma)  # Elemento fuera de la diagonal
        FIN SI
    FIN PARA
FIN PARA

RETORNAR L</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
<div id="ejemplo-8" class="section level4 caja-ejemplo"
number="2.3.2.1">
<h4><span class="header-section-number">2.3.2.1</span> Ejemplo 8</h4>
<p>Determine si la siguiente matriz es simétrica y postiva definida
<span class="math display">\[
A=\begin{pmatrix}
5 &amp; 2 &amp; -2 &amp; -1 &amp; -1\\
2 &amp; 7 &amp; 1 &amp; 1 &amp; 6\\
-2 &amp; 1 &amp; 9 &amp; 2 &amp; 1\\
-1 &amp; 1 &amp; 2 &amp; 11 &amp; -1\\
-1 &amp; 6 &amp; 1 &amp; -1 &amp; 13
\end{pmatrix}
\]</span></p>
<pre class="python"><code>import numpy as np

def is_square(A):
    A = np.asarray(A)
    return A.ndim == 2 and A.shape[0] == A.shape[1]

def is_symmetric_by_transpose(A, tol=1e-8):
    &quot;&quot;&quot;
    Comprueba simetría comparando A y A.T con tolerancia absoluta.
    &quot;&quot;&quot;
    A = np.asarray(A, dtype=float)
    if not is_square(A):
        return False
    return np.allclose(A, A.T, atol=tol, rtol=0)

def is_positive_definite_by_principal_minors(A, tol=1e-12):
    &quot;&quot;&quot;
    Comprueba que todos los menores principales (determinantes de A[:k,:k], k=1..n)
    son positivos. Esto es una condición necesaria y suficiente para simetría
    y positividad definida en matrices reales simétricas.
    &quot;&quot;&quot;
    A = np.asarray(A, dtype=float)
    if not is_symmetric_by_transpose(A):
        return False
    n = A.shape[0]
    for k in range(1, n+1):
        minor = A[:k, :k]
        det = np.linalg.det(minor)
        if det &lt;= tol:
            return False
    return True

def check_matrix_simple(A, tol_sym=1e-8, tol_minor=1e-12):
    A = np.asarray(A, dtype=float)
    square = is_square(A)
    symmetric = square and is_symmetric_by_transpose(A, tol=tol_sym)
    pd = False
    if symmetric:
        pd = is_positive_definite_by_principal_minors(A, tol=tol_minor)
    return {
        &quot;square&quot;: square,
        &quot;symmetric&quot;: symmetric,
        &quot;positive_definite&quot;: pd
    }

# Ejemplo: usar la matriz A del enunciado (5x5) y calcular la factorización de Cholesky
if __name__ == &quot;__main__&quot;:
    A = np.array([[5, 2, -2, -1, -1],
                  [2, 7, 1, 1, 6],
                  [-2, 1, 9, 2, 1],
                  [-1, 1, 2, 11, -1],
                  [-1, 6, 1, -1, 13]], dtype=float)

    print(&quot;Matriz A:&quot;)
    print(A)

    res = check_matrix_simple(A)
    print(&quot;square:&quot;, res[&quot;square&quot;]) 
    print(&quot;symmetric:&quot;, res[&quot;symmetric&quot;]) 
    print(&quot;positive_definite (por menores principales):&quot;, res[&quot;positive_definite&quot;]) 

    # Mostrar determinantes de los menores principales para diagnóstico
    if res[&quot;symmetric&quot;]:
        print(&quot;Determinantes de los menores principales:&quot;)
        for k in range(1, A.shape[0] + 1):
            det = np.linalg.det(A[:k, :k])
            print(f&quot;det(A[:{k},:{k}]) = {det:.8f}&quot;)

    # Implementación explícita del algoritmo de Cholesky (tal como aparece en el Rmd)
    def factorizacion_cholesky(A, tol=1e-12):
        A = np.asarray(A, dtype=float)
        if A.ndim != 2 or A.shape[0] != A.shape[1]:
            raise ValueError(&quot;La matriz debe ser cuadrada&quot;)
        n = A.shape[0]
        L = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(i+1):
                suma = 0.0
                for k in range(j):
                    suma += L[i, k] * L[j, k]
                if i == j:
                    val = A[i, i] - suma
                    if val &lt;= tol:
                        # No es definida positiva (o es numéricamente no PD)
                        raise np.linalg.LinAlgError(f&quot;No es definida positiva: diagonal negativa en i={i}, valor={val}&quot;)
                    L[i, j] = np.sqrt(val)
                else:
                    L[i, j] = (A[i, j] - suma) / L[j, j]
        return L

    # Intentar la factorización y mostrar resultados
    try:
        L = factorizacion_cholesky(A)
        print(&quot;\nFactor L (Cholesky):&quot;)
        print(L)
        recon = L.dot(L.T)
        print(&quot;\nReconstrucción L @ L.T:&quot;)
        print(recon)
        err = np.max(np.abs(A - recon))
        print(f&quot;\nMáximo error |A - LL^T| = {err:.3e}&quot;)
    except Exception as e:
        print(&quot;Error al calcular Cholesky:&quot;, e)</code></pre>
<pre><code>## Matriz A:
## [[ 5.  2. -2. -1. -1.]
##  [ 2.  7.  1.  1.  6.]
##  [-2.  1.  9.  2.  1.]
##  [-1.  1.  2. 11. -1.]
##  [-1.  6.  1. -1. 13.]]
## square: True
## symmetric: True
## positive_definite (por menores principales): True
## Determinantes de los menores principales:
## det(A[:1,:1]) = 5.00000000
## det(A[:2,:2]) = 31.00000000
## det(A[:3,:3]) = 238.00000000
## det(A[:4,:4]) = 2451.00000000
## det(A[:5,:5]) = 13247.00000000
## 
## Factor L (Cholesky):
## [[ 2.23606798  0.          0.          0.          0.        ]
##  [ 0.89442719  2.48997992  0.          0.          0.        ]
##  [-0.89442719  0.7228974   2.77081565  0.          0.        ]
##  [-0.4472136   0.56225353  0.43075705  3.20909946  0.        ]
##  [-0.4472136   2.57030185 -0.45404122 -0.76332313  2.32480811]]
## 
## Reconstrucción L @ L.T:
## [[ 5.  2. -2. -1. -1.]
##  [ 2.  7.  1.  1.  6.]
##  [-2.  1.  9.  2.  1.]
##  [-1.  1.  2. 11. -1.]
##  [-1.  6.  1. -1. 13.]]
## 
## Máximo error |A - LL^T| = 8.882e-16</code></pre>
</div>
<ul>
<li>Al ser la factorización de Cholesky una factorización <span
class="math inline">\(LU\)</span>, para resolver el sistema lineal <span
class="math inline">\(Ax=b\)</span> y al ser <span
class="math inline">\(A=LL^T\)</span>, se sigue el siguiente
procedimiento:
<ul>
<li><span class="math inline">\(Ax=b \Rightarrow LL^Tx=b \Rightarrow
L(L^Tx)=b\)</span>, llamando <span
class="math inline">\(z=L^Tx\)</span>, se resuelve <span
class="math inline">\(Lz=b\)</span> aplicando sustitución
progresiva.</li>
<li>Hallado los valores de <span class="math inline">\(z\)</span>, se
devuelve el cambio planteado resolviendo <span
class="math inline">\(L^Tx=z\)</span> aplicando sustitución
regresiva.</li>
</ul></li>
</ul>
</div>
</div>
<div id="métodos-iterativos" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Métodos
Iterativos</h2>
<ul>
<li><p>Dado un sistema lineal <span class="math inline">\(Ax=b\)</span>,
un <strong>método iterativo</strong> para resolver dicho sistema
consiste en hallar una sucesión de <strong>soluciones
aproximadas</strong> <span
class="math inline">\(\{x^{(k)}\}_{k=0}^{\infty}\)</span> tal que <span
class="math inline">\(\displaystyle\lim_{k\to\infty}x^{(k)}=x\)</span>
donde <span class="math inline">\(x\)</span> es la solución del
sistema.</p></li>
<li><p>Cuando se escribe que <span
class="math inline">\(\displaystyle\lim_{k\to\infty}x^{(k)}=x\)</span>,
se quiere decir que dada una norma matricial <span
class="math inline">\(\|\cdot\|\)</span>, <span
class="math inline">\(\displaystyle\lim_{k\to\infty}\|x^{(k)}-x\|=0\)</span>.</p></li>
</ul>
<p>Los algoritmos numéricos basado en <strong>métodos
iterativos</strong> tienen las propiedades siguientes:</p>
<ul>
<li>Se obtiene una <strong>solución aproximada</strong>, al contrario de
lo que pasaba con los métodos directos que hallan una solución exacta
del sistema lineal. Es decir, suponiendo que no hay errores en los datos
ni en las operaciones aritméticas, la solución obtenida por un método
iterativo es aproximada.</li>
<li>El número de pasos que hay que realizar para hallar la solución de
un sistema lineal no está predeterminado de entrada.</li>
</ul>
<p>Al ser un método iterativo hay que establecer un criterio de parada
en términos de un cierto <strong>umbral de error</strong> <span
class="math inline">\(\epsilon\)</span>.</p>
<ul>
<li>Se pueden establecer 3 criterios de parada:
<ul>
<li>hallar el entero <span class="math inline">\(k\)</span> tal que
<span class="math inline">\(\||x^{(k)}-x^{(k-1)}\|&lt;\epsilon\)</span>,
(criterio del error absoluto)</li>
<li>hallar el entero <span class="math inline">\(k\)</span> tal que
<span
class="math inline">\(\dfrac{\|x^{(k)}-x^{(k-1)}\|}{\|x^{(k)}\|}&lt;\epsilon\)</span>,
(criterio del error relativo)</li>
<li>hallar el entero <span class="math inline">\(k\)</span> tal que
<span class="math inline">\(\|Ax^{(k)}-b\|&lt;\epsilon\)</span>,
(criterio del residual)</li>
</ul></li>
</ul>
<div id="construcción-de-la-sucesión." class="section level3"
number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Construcción de la
Sucesión.</h3>
<p>Supongase un el Sistema Lineal <span class="math inline">\(Ax =
b\)</span>, se busca una matriz <span class="math inline">\(T \in
\mathcal{M}_n\)</span> y un vector <span class="math inline">\(c \in
\mathbb{R}^n\)</span>, de forma que la matriz <span
class="math inline">\(I - T\)</span> sea inversible y que la única
solución del sistema lineal</p>
<div class="recuadro-gris">
<p><span class="math display">\[
\underbrace{x = Tx + c}_{(I−T)x=c}
\]</span></p>
</div>
<p>es la solución de <span class="math inline">\(Ax = b\)</span>.</p>
<p>Considerando <span class="math inline">\(x^{(0)} \in
\mathbb{R}^n\)</span> un vector arbitrario, se construye una sucesión de
vectores <span class="math inline">\(\{x\}_{k=0}^{\infty}\)</span> dada
por</p>
<div class="recuadro-gris">
<p><span class="math display">\[
x^{(k+1)} = Tx^{(k)} + c;\quad k \in \mathbb{N} \cup \{0\}
\]</span></p>
</div>
<p>y se pretende que la sucesión <span
class="math inline">\(\{x\}_k\)</span> converja a la solución del
sistema lineal.</p>
<p>Donde <span class="math inline">\(T\)</span> es la llamada matriz de
iteración y c el vector independiente del método iterativo en
cuestión.</p>
</div>
<div id="método-de-jacobi" class="section level3" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Método de
Jacobi</h3>
<ul>
<li><p>Suponse que los coeficientes <span
class="math inline">\(a_{ii}\)</span> de la matriz <span
class="math inline">\(A=(a_{ij})_{i,j=1,\ldots,n}\)</span> del sistema
<span class="math inline">\(Ax=b\)</span> son distintos de
cero.</p></li>
<li><p>Siempre existe una permutación de filas para que dicha condición
se cumpla ya que, en caso contrario, la matriz del sistema sería
singular y el sistema no tendría solución única.</p></li>
<li><p>Entonces para transformar el sistema <span
class="math inline">\(Ax=b\)</span> en un sistema equivalente de la
forma <span class="math inline">\(x=Tx+c\)</span>, se puede despejar las
incógnitas <span class="math inline">\(x_i\)</span> de la <span
class="math inline">\(i\)</span>-ésima ecuación de la forma
siguiente:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{align*}
E_i: &amp; a_{i1}x_1 + a_{i2}x_2 + \cdots + a_{ii}x_i + \cdots +
a_{in}x_n = b_i, \Rightarrow\\
&amp; x_i = \dfrac{1}{a_{ii}}\left(b_i
-(a_{i1}x_1+\cdots+a_{i,i-1}x_{i-1}+a_{i,i+1}x_{i+1}+\cdots+a_{in}x_n)\right)
\end{align*}
\]</span></p>
</div>
<ul>
<li>Entonces, la matriz de iteración <strong><span
class="math inline">\(T_J\)</span></strong> y el vector independiente
<strong><span class="math inline">\(c_J\)</span></strong> serán los
siguientes:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
T_J = \begin{pmatrix}
0 &amp; -\dfrac{a_{12}}{a_{11}} &amp; \cdots &amp;
-\dfrac{a_{1n}}{a_{11}}\\
-\dfrac{a_{21}}{a_{22}} &amp; 0 &amp; \cdots &amp;
-\dfrac{a_{2n}}{a_{22}}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
-\dfrac{a_{n1}}{a_{nn}} &amp; -\dfrac{a_{n2}}{a_{nn}} &amp; \cdots &amp;
0  
\end{pmatrix}, \quad c_J = \begin{pmatrix}
\dfrac{b_1}{a_{11}}\\ \dfrac{b_2}{a_{22}}\\ \vdots\\
\dfrac{b_n}{a_{nn}}\\
\end{pmatrix}
\]</span></p>
</div>
<p>La sucesión <span class="math inline">\(x^{(k)}\)</span> se generará
de la forma siguiente:</p>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{pmatrix}
x_1^{(k)}\\
x_2^{(k)}\\
\vdots\\
x_n^{(k)}\end{pmatrix} =  \begin{pmatrix}
0 &amp; -\frac{a_{12}}{a_{11}} &amp; \cdots &amp;
-\frac{a_{1n}}{a_{11}}\\
-\frac{a_{21}}{a_{22}} &amp; 0 &amp; \cdots &amp;
-\frac{a_{2n}}{a_{22}}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
-\frac{a_{n1}}{a_{nn}} &amp; -\frac{a_{n2}}{a_{nn}} &amp; \cdots &amp;
0  
\end{pmatrix}\begin{pmatrix}
x_1^{(k-1)}\\
x_2^{(k-1)}\\
\vdots\\
x_n^{(k-1)}\end{pmatrix} + \begin{pmatrix}
\frac{b_1}{a_{11}}\\ \frac{b_2}{a_{22}}\\ \vdots\\ \frac{b_n}{a_{nn}}\\
\end{pmatrix}
\]</span></p>
</div>
<ul>
<li><p>Otra manera de escribir la <strong>matriz de iteración del método
de Jacobi</strong> es la siguiente.</p></li>
<li><p>Descomponemos la matriz del sistema <span
class="math inline">\(A=(a_{ij})_{i,j=1,\ldots,n\)</span> de la forma
siguiente: <span class="math display">\[
A=D+L+U,
\]</span> donde las matrices <span class="math inline">\(D\)</span>,
<span class="math inline">\(L\)</span> y <span
class="math inline">\(U\)</span> contienen los <strong>elementos
diagonales</strong>, los elementos por <strong>debajo de la
diagonal</strong> y por <strong>encima de la diagonal</strong>
respectivamente:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
D = \begin{pmatrix}
a_{11} &amp; 0 &amp; \cdots &amp; 0\\
0 &amp; a_{22} &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; a_{nn}
\end{pmatrix}, \quad L = \begin{pmatrix}
0 &amp; 0 &amp; \cdots &amp; 0\\
a_{21} &amp; 0 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
a_{n1} &amp; a_{n2} &amp; \cdots &amp; 0
\end{pmatrix}, \quad U = \begin{pmatrix}
0 &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
0 &amp; 0 &amp; \cdots &amp; a_{2n}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}
\]</span></p>
</div>
<ul>
<li>El sistema <span class="math inline">\(Ax=b\)</span> puede
escribirse como:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
(D+L+U)x=b.
\]</span></p>
</div>
<ul>
<li><p>El método de Jacobi a partir de las matrices anteriores, se
exribe de la siguiente manera.</p></li>
<li><p>Dejando los elementos diagonales <span
class="math inline">\(D\)</span> a la izquierda y los elementos <span
class="math inline">\(L\)</span> y <span
class="math inline">\(U\)</span> a la derecha:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
Ax=b \Leftrightarrow (D+(L+U))x = b \Leftrightarrow Dx = -(L+U)x+b
\]</span></p>
</div>
<ul>
<li>Despejamos el vector <span class="math inline">\(x\)</span></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
x=-D^{-1}(L+U)x+D^{-1}b.
\]</span></p>
</div>
<ul>
<li>Así, se calcula la solución del sistema, como el límite de la
sucesión <span class="math inline">\(\{x^{(k)}\}_{k\in
\mathbb{N}}\)</span> donde se define el término <span
class="math inline">\((k+1)\)</span>-ésimo como:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
x^{(k+1)} =
\underbrace{-D^{-1}(L+U)}_{T_J}x^{(k)}+\underbrace{D^{-1}b}_{c_J}
\]</span></p>
</div>
<ul>
<li>La fórmula escalar para el método de Jacobi es:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
x_i^{(k+1)} = \dfrac{1}{a_{ii}} \left( b_i - \sum_{\substack{j=1 \\ j
\neq i}}^{n} a_{ij}x_j^{(k)} \right), \quad \text{para } i = 1, 2,
\ldots, n
\]</span></p>
</div>
<div class="cuadro-alg">
<p><strong>FUNCION Metodo_Jacobi(A, b, x0, tol, max_iter):</strong></p>
<pre><code>n = dimension de A (número de filas)
x = x0  # Vector de solución inicial

PARA k = 1 HASTA max_iter:
    x_anterior = x
    x = nuevo vector de ceros de tamaño n

    # Calcular cada componente de la nueva solución
    PARA i = 1 HASTA n:
        suma = 0
        PARA j = 1 HASTA n:
            SI i != j ENTONCES
                suma = suma + A[i, j] * x_anterior[j]
            FIN SI
        FIN PARA
        x[i] = (b[i] - suma) / A[i, i]
    FIN PARA

    # Criterio de parada (error absoluto)
    SI norma(x - x_anterior) &lt; tol ENTONCES
        RETORNAR x  # Solución encontrada
    FIN SI
FIN PARA

RETORNAR &quot;El método no convergió en max_iter iteraciones&quot;</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
<div id="ejemplo-9" class="section level4 caja-ejemplo"
number="2.4.2.1">
<h4><span class="header-section-number">2.4.2.1</span> Ejemplo 9</h4>
<p>Considerese el siguiente sistema de 4 ecuaciones con 4 incógnitas:
<span class="math display">\[
\begin{align*}
E_1: &amp; 22x_1 + 5x_1 + 5x_3 + 6x_4 = 5\\
E_2: &amp; 5x_1 + 19x_2 + 3x_3 + 6x_4 = 7\\
E_3: &amp; 5x_1 + 5x_2 + 24x_3 + 5x_4 = 8\\
E_4: &amp; 7x_1 + 7x_2 + 4x_3 + 25x_4 = 5
\end{align*}
\]</span></p>
<p>Para aplicar el método de Jacobi al sistema anterior, despejamos las
incógnitas <span class="math inline">\(x_i\)</span> de sus respectivas
ecuaciones: <span class="math display">\[
\begin{align*}
E_1: &amp; x_1 =
\dfrac{5}{22}-\left(\dfrac{5}{22}x_2+\dfrac{5}{22}x_3+\dfrac{6}{22}x_4\right)\\
E_2: &amp; x_2 =
\dfrac{7}{19}-\left(\dfrac{5}{19}x_1+\dfrac{3}{19}x_3+\dfrac{6}{19}x_4\right)\\
E_3: &amp; x_3 =
\dfrac{8}{24}-\left(\dfrac{5}{24}x_1+\dfrac{5}{24}x_2+\dfrac{5}{24}x_4\right)\\
E_4: &amp; x_4 =
\dfrac{5}{25}-\left(\dfrac{7}{25}x_1+\dfrac{7}{25}x_2+\dfrac{4}{25}x_3\right)\\
\end{align*}
\]</span></p>
<pre class="python"><code>import numpy as np

def stopCriterion(x, x0, A, b, criterion = &quot;ABS&quot;):
  &quot;&quot;&quot;
  Esta función devuelve el valor según el criterio de parada

  Args:
    x: Array unidimensional que representa el punto actual de iteración
    x0: Array unidimensional que representa el punto anterior de iteración
    A: Matriz del sistema
    b: Vector de términos independientes
    criterion: String (Criterio de parada: ABS, REL, RES)

  Returns:
    Float (error absoluto, relativo o residual, en función de criterion)
  &quot;&quot;&quot;

  if criterion.upper() == &quot;ABS&quot;:
    return np.linalg.norm(x - x0)
  elif criterion.upper() == &quot;REL&quot;:
    return np.linalg.norm(x - x0) / np.linalg.norm(x)
  elif criterion.upper() == &quot;RES&quot;:
    return np.linalg.norm(np.dot(A, x) - b)
  else:
    # Por defecto, devuelve el error absoluto si el criterio no es reconocido
    return np.linalg.norm(x - x0)

def jacobi(A, b, x0, TOL=1e-07, nmax=100, stop=&quot;ABS&quot;, verbose=False):
  &quot;&quot;&quot;
  Esta función resuelve un sistema lineal Ax = b mediante el método
  de Jacobi

  Args:
    A: Array bidimensional de numpy (Matriz de coeficientes)
    b: Array unidimensional (Vector de términos independientes)
    x0: Array unidimensional (Valor inicial)
    TOL: Float (Tolerancia)
    nmax: Float (Número máximo de iteraciones)
    stop: String (Criterio de parada: ABS, REL o RES)
    verbose: Booleano para mostrar o no los resultados relevantes
  
  Returns:
    x: Array unidimensional (Solución del sistema)
  &quot;&quot;&quot;
  n = A.shape[0]
  x = np.empty(n)
  
  if stop.upper() not in [&quot;ABS&quot;, &quot;REL&quot;, &quot;RES&quot;]:
    print(&quot;El criterio de parada introducido no es válido&quot;)
    return
  
  k = 1 # Contador de iteraciones
  while k &lt;= nmax:
    for i in range(n):
      suma = 0
      for j in range(n):
        if j != i:
          suma += A[i, j] * x0[j]
      x[i] = (b[i] - suma) / A[i, i]
      
    if verbose:
      print(&quot;x^({}) = {}&quot;.format(k, x))
      
    if stopCriterion(x, x0, A, b, criterion=stop) &lt; TOL:
      print(&quot;Número total de iteraciones:&quot;, k)
      print(&quot;x =&quot;, x)
      return x
      
    k += 1
    x0 = x.copy()
    
  print(&quot;Número máximo de iteraciones alcanzado.&quot;)
  print(&quot;El método no converge.&quot;)
  return

A = np.array([[22., 5., 5., 6.],
             [5., 19., 3., 6.],
             [5., 5., 24., 5.],
             [7., 7., 4., 25.]])

b = np.array([5., 7., 8., 5.])
x0 = np.zeros(A.shape[0])

print(&quot;--- Solución con Criterio de Parada Absoluto (ABS) ---&quot;)</code></pre>
<pre><code>## --- Solución con Criterio de Parada Absoluto (ABS) ---</code></pre>
<pre class="python"><code>jacobi(A, b, x0.copy(), stop=&quot;ABS&quot;, verbose=False)</code></pre>
<pre><code>## Número total de iteraciones: 46
## x = [0.09157774 0.28873163 0.24271059 0.05467965]
## array([0.09157774, 0.28873163, 0.24271059, 0.05467965])</code></pre>
<pre class="python"><code>print(&quot;\n&quot; + &quot;=&quot;*50 + &quot;\n&quot;)</code></pre>
<pre><code>## 
## ==================================================</code></pre>
<pre class="python"><code>print(&quot;--- Solución con Criterio de Parada Relativo (REL) ---&quot;)</code></pre>
<pre><code>## --- Solución con Criterio de Parada Relativo (REL) ---</code></pre>
<pre class="python"><code>jacobi(A, b, x0.copy(), stop=&quot;REL&quot;, verbose=False)</code></pre>
<pre><code>## Número total de iteraciones: 49
## x = [0.09157777 0.28873166 0.24271061 0.05467967]
## array([0.09157777, 0.28873166, 0.24271061, 0.05467967])</code></pre>
<pre class="python"><code>print(&quot;\n&quot; + &quot;=&quot;*50 + &quot;\n&quot;)</code></pre>
<pre><code>## 
## ==================================================</code></pre>
<pre class="python"><code>print(&quot;--- Solución con Criterio de Parada Residual (RES) ---&quot;)</code></pre>
<pre><code>## --- Solución con Criterio de Parada Residual (RES) ---</code></pre>
<pre class="python"><code>jacobi(A, b, x0.copy(), stop=&quot;RES&quot;, verbose=True)</code></pre>
<pre><code>## x^(1) = [0.22727273 0.36842105 0.33333333 0.2       ]
## x^(2) = [ 0.01323764  0.19282297  0.1675638  -0.02012759]
## x^(3) = [0.15085599 0.3448361  0.29459729 0.11549282]
## x^(4) = [0.05044891 0.24573533 0.20600314 0.01407065]
## x^(5) = [0.12076744 0.31817485 0.2686969  0.08410791]
## x^(6) = [0.07095426 0.26765393 0.22436454 0.03410465]
## x^(7) = [0.10614908 0.30355301 0.25568482 0.06929138]
## x^(8) = [0.08127557 0.27823431 0.23354303 0.04437384]
## x^(9) = [0.09885774 0.29614474 0.24919089 0.06197035]
## x^(10) = [0.08643181 0.28349034 0.23813066 0.04952876]
## x^(11) = [0.09521465 0.2924356  0.24594773 0.05832089]
## x^(12) = [0.08900718 0.28611359 0.24042268 0.05210629]
## x^(13) = [0.09339459 0.29058202 0.24432769 0.05649856]
## x^(14) = [0.09029364 0.28742382 0.24156768 0.05339412]
## x^(15) = [0.09248535 0.289656   0.24351842 0.05558828]
## x^(16) = [0.09093628 0.28807833 0.24213966 0.05403747]
## x^(17) = [0.09203115 0.28919341 0.24311415 0.05513356]
## x^(18) = [0.09125731 0.28840529 0.24242539 0.05435886]
## x^(19) = [0.09180425 0.28896232 0.2429122  0.05490641]
## x^(20) = [0.09141768 0.28856862 0.24256813 0.05451941]
## x^(21) = [0.0916909  0.28884688 0.24281131 0.05479294]
## x^(22) = [0.09149779 0.28865021 0.24263943 0.05459961]
## x^(23) = [0.09163428 0.28878921 0.24276091 0.05473625]
## x^(24) = [0.09153781 0.28869097 0.24267505 0.05463968]
## x^(25) = [0.09160599 0.28876041 0.24273574 0.05470793]
## x^(26) = [0.0915578  0.28871133 0.24269285 0.05465969]
## x^(27) = [0.09159186 0.28874602 0.24272316 0.05469379]
## x^(28) = [0.09156779 0.2887215  0.24270174 0.05466969]
## x^(29) = [0.0915848  0.28873883 0.24271688 0.05468672]
## x^(30) = [0.09157278 0.28872658 0.24270618 0.05467468]
## x^(31) = [0.09158128 0.28873524 0.24271374 0.05468319]
## x^(32) = [0.09157527 0.28872912 0.24270839 0.05467718]
## x^(33) = [0.09157952 0.28873344 0.24271217 0.05468143]
## x^(34) = [0.09157652 0.28873039 0.2427095  0.05467842]
## x^(35) = [0.09157864 0.28873255 0.24271139 0.05468055]
## x^(36) = [0.09157714 0.28873102 0.24271006 0.05467905]
## x^(37) = [0.0915782  0.2887321  0.242711   0.05468011]
## x^(38) = [0.09157745 0.28873134 0.24271033 0.05467936]
## x^(39) = [0.09157798 0.28873187 0.2427108  0.05467989]
## x^(40) = [0.0915776  0.28873149 0.24271047 0.05467951]
## x^(41) = [0.09157787 0.28873176 0.24271071 0.05467978]
## x^(42) = [0.09157768 0.28873157 0.24271054 0.05467959]
## x^(43) = [0.09157781 0.28873171 0.24271066 0.05467972]
## x^(44) = [0.09157772 0.28873161 0.24271057 0.05467963]
## x^(45) = [0.09157779 0.28873168 0.24271063 0.0546797 ]
## x^(46) = [0.09157774 0.28873163 0.24271059 0.05467965]
## x^(47) = [0.09157777 0.28873167 0.24271062 0.05467968]
## x^(48) = [0.09157775 0.28873164 0.2427106  0.05467966]
## x^(49) = [0.09157777 0.28873166 0.24271061 0.05467967]
## x^(50) = [0.09157775 0.28873165 0.2427106  0.05467966]
## x^(51) = [0.09157776 0.28873165 0.24271061 0.05467967]
## x^(52) = [0.09157776 0.28873165 0.24271061 0.05467967]
## x^(53) = [0.09157776 0.28873165 0.24271061 0.05467967]
## x^(54) = [0.09157776 0.28873165 0.24271061 0.05467967]
## Número total de iteraciones: 54
## x = [0.09157776 0.28873165 0.24271061 0.05467967]
## array([0.09157776, 0.28873165, 0.24271061, 0.05467967])</code></pre>
</div>
</div>
<div id="método-de-gauss-seidel" class="section level3" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Método de
Gauss-Seidel</h3>
<ul>
<li><p>El método de Gauss-Seidel consiste en aplicar el método de Jacobi
con una modificación: las componentes del vector <span
class="math inline">\(x^{(k)}\)</span> de la iteración actual, <span
class="math inline">\(x^{(k)}_i\)</span>, que se van calculando, se usan
para el cálculo de las componentes <span
class="math inline">\(x^{(k)}_{i+1},\ldots,x^{(k)}_n\)</span> de dicho
vector.</p></li>
<li><p>Es decir, la sucesión de aproximaciones se calcula de la forma
siguiente:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[\begin{align*}
x_1^{(k)} = &amp;
\dfrac{1}{a_{11}}\left(b_1-\left(a_{12}x_2^{(k-1)}+\cdots+a_{1n}x_n^{(k-1)}\right)\right)\\
x_2^{(k)} = &amp;
\dfrac{1}{a_{22}}\left(b_2-\left(a_{21}x_1^{(k)}+a_{23}x_3^{(k-1)}+\cdots+a_{2n}x_n^{(k-1)}\right)\right)\\
\vdots &amp;\\
x_i^{(k)} = &amp;
\dfrac{1}{a_{ii}}\left(b_i-\left(a_{i1}x_1^{(k)}+\cdots+a_{i,i-1}x_{i-1}^{(k)}+a_{i,i+1}x_{i+1}^{(k-1)}\cdots+a_{in}x_n^{(k-1)}\right)\right)\\
\vdots &amp; \\
x_n^{(k)} = &amp;
\dfrac{1}{a_{nn}}\left(b_n-\left(a_{n1}x_1^{(k)}+\cdots+a_{n,n-1}x_{n-1}^{(k)}\right)\right)\\
\end{align*}\]</span></p>
</div>
<ul>
<li>Matricialmente, la sucesión de aproximaciones se calcularía de la
forma siguiente:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[{
\begin{pmatrix}
x_1^{(k)} \\ x_2^{(k)}\\ \vdots\\ x_n^{(k)} \\
\end{pmatrix} = \begin{pmatrix}
0 &amp; 0 &amp; \cdots &amp; 0\\
-\frac{a_{21}}{a_{22}} &amp; 0 &amp; \cdots &amp; 0\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
-\frac{a_{n1}}{a_{nn}} &amp; -\frac{a_{n2}}{a_{nn}} &amp; \cdots &amp; 0
\end{pmatrix}\begin{pmatrix}
x_1^{(k)} \\ x_2^{(k)}\\ \vdots\\ x_n^{(k)} \\
\end{pmatrix} + \begin{pmatrix}
0 &amp; -\frac{a_{12}}{a_{11}} &amp; \cdots &amp;
-\frac{a_{1n}}{a_{11}}\\
0 &amp; 0 &amp; \cdots &amp; -\frac{a_{2n}}{a_{22}}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0 &amp; 0 &amp; \cdots &amp; 0
\end{pmatrix}\begin{pmatrix}
x_1^{(k-1)} \\ x_2^{(k-1)}\\ \vdots\\ x_n^{(k-1)} \\
\end{pmatrix}+\begin{pmatrix}
\frac{b_1}{a_{11}} \\ \frac{b_2}{a_{22}}\\ \vdots\\ \frac{b_n}{a_{nn}}
\\
\end{pmatrix}}
\]</span></p>
</div>
<ul>
<li><p>La matriz de iteración y el vector independiente del método de
Gauss-Seidel, se pueden deducir a partir de la descomposición de la
matriz del sistema <span class="math inline">\(A\)</span> usada
anteriormente: <span class="math display">\[
A=D+L+U
\]</span> El sistema a resolver <span
class="math inline">\(Ax=b\)</span> o, si se quiere: <span
class="math inline">\((D+L+U)x=b\)</span>:</p></li>
<li><p>Dejando los elementos diagonales y pasando los demás elementos al
término de la derecha:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
Dx = -(L+U)x+b
\]</span></p>
</div>
<ul>
<li>Separando las componentes calculadas de las no calculadas del vector
<span class="math inline">\(x\)</span>:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
Dx = -Lx -Ux + b
\]</span></p>
</div>
<ul>
<li>Escribiendo la expresión anterior en función de la sucesión de
aproximaciones <span class="math inline">\(x^{(k)}\)</span> se
obtiene:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
Dx^{(k)} = -Lx^{(k)} -Ux^{(k-1)} + b
\]</span></p>
</div>
<ul>
<li>Reagrupando los términos que tienen a <span
class="math inline">\(x^{(k)}\)</span> como factor en el lado
izquierdo:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
(D+L)x^{(k)} = -Ux^{(k-1)} + b
\]</span></p>
</div>
<ul>
<li>Despejando <span class="math inline">\(x^{(k)}\)</span>:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
x^{(k)} = -(D+L)^{-1}Ux^{(k-1)} + (D+L)^{-1}b
\]</span></p>
</div>
<p>Entonces, la matriz de iteración y el vector independiente del método
de Gauss-Seidel son:</p>
<div class="recuadro-gris">
<p><span class="math display">\[
T_{GS} = -(D+L)^{-1}U, \quad c_{GS} = (D+L)^{-1}b
\]</span></p>
</div>
<div class="cuadro-alg">
<p><strong>FUNCION Metodo_Gauss-Seidel(A, b, x0, tol,
max_iter):</strong></p>
<pre><code>n = dimensión de A (número de filas)
x = x0  # Vector de solución inicial

PARA k = 1 HASTA max_iter:
    x_anterior = x

    # Calcular cada componente de la nueva solución
    PARA i = 1 HASTA n:
        suma_nuevos = 0
        # Suma de los componentes ya actualizados en esta iteración (j &lt; i)
        PARA j = 1 HASTA i - 1:
            suma_nuevos = suma_nuevos + A[i, j] * x[j]
        FIN PARA

        suma_viejos = 0
        # Suma de los componentes de la iteración anterior (j &gt; i)
        PARA j = i + 1 HASTA n:
            suma_viejos = suma_viejos + A[i, j] * x_anterior[j]
        FIN PARA

        # Calcular y actualizar el componente x[i] inmediatamente
        x[i] = (b[i] - suma_nuevos - suma_viejos) / A[i, i]
    FIN PARA

    # Criterio de parada (usando el error absoluto como ejemplo)
    SI norma(x - x_anterior) &lt; tol ENTONCES
        RETORNAR x  # Solución encontrada
    FIN SI
FIN PARA

RETORNAR &quot;El método no convergió en max_iter iteraciones&quot;</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
<div id="ejemplo-10" class="section level4 caja-ejemplo"
number="2.4.3.1">
<h4><span class="header-section-number">2.4.3.1</span> Ejemplo 10</h4>
<p>Consideremos el mismo sistema de 4 ecuaciones del <strong>Ejemplo
9</strong> para aplicar el método de Gauss-Seidel: <span
class="math display">\[
\begin{align*}
E_1: &amp; 22x_1 + 5x_1 + 5x_3 + 6x_4 = 5\\
E_2: &amp; 5x_1 + 19x_2 + 3x_3 + 6x_4 = 7\\
E_3: &amp; 5x_1 + 5x_2 + 24x_3 + 5x_4 = 8\\
E_4: &amp; 7x_1 + 7x_2 + 4x_3 + 25x_4 = 5
\end{align*}
\]</span></p>
<p>Las ecuaciones de recurrencia para Gauss-Seidel, utilizando los
valores más recientes, son: <span class="math display">\[
\begin{align*}
x_1^{(k)} = &amp; \dfrac{1}{22}\left(5 - 5x_2^{(k-1)} - 5x_3^{(k-1)} -
6x_4^{(k-1)}\right) \\
x_2^{(k)} = &amp; \dfrac{1}{19}\left(7 - 5x_1^{(k)} - 3x_3^{(k-1)} -
6x_4^{(k-1)}\right) \\
x_3^{(k)} = &amp; \dfrac{1}{24}\left(8 - 5x_1^{(k)} - 5x_2^{(k)} -
5x_4^{(k-1)}\right) \\
x_4^{(k)} = &amp; \dfrac{1}{25}\left(5 - 7x_1^{(k)} - 7x_2^{(k)} -
4x_3^{(k)}\right)
\end{align*}
\]</span></p>
<pre><code>## --- Solución con Gauss-Seidel (Criterio Absoluto) ---</code></pre>
<pre><code>## Número total de iteraciones: 11
## x = [0.09157775 0.28873165 0.24271061 0.05467967]
## array([0.09157775, 0.28873165, 0.24271061, 0.05467967])</code></pre>
<pre><code>## 
## ==================================================</code></pre>
<pre><code>## --- Solución con Criterio de Parada Relativo (REL) ---</code></pre>
<pre><code>## Número total de iteraciones: 11
## x = [0.09157775 0.28873165 0.24271061 0.05467967]
## array([0.09157775, 0.28873165, 0.24271061, 0.05467967])</code></pre>
<pre><code>## 
## ==================================================</code></pre>
<pre><code>## --- Solución con Criterio de Parada Residual (RES) ---</code></pre>
<pre><code>## x^(1) = [0.22727273 0.30861244 0.22169059 0.01448166]
## x^(2) = [0.10279977 0.30179155 0.24602646 0.0473502 ]
## x^(3) = [0.08985494 0.29097604 0.24412892 0.0543067 ]
## x^(4) = [0.09084705 0.28881778 0.2429226  0.05482623]
## x^(5) = [0.09147003 0.28868024 0.24271323 0.05472381]
## x^(6) = [0.09157681 0.28871755 0.24270455 0.05468485]
## x^(7) = [0.09158093 0.28873014 0.24270918 0.05467943]
## x^(8) = [0.09157849 0.28873176 0.24271048 0.05467945]
## x^(9) = [0.09157782 0.28873172 0.24271063 0.05467963]
## x^(10) = [0.09157775 0.28873166 0.24271062 0.05467967]
## x^(11) = [0.09157775 0.28873165 0.24271061 0.05467967]
## Número total de iteraciones: 11
## x = [0.09157775 0.28873165 0.24271061 0.05467967]
## array([0.09157775, 0.28873165, 0.24271061, 0.05467967])</code></pre>
</div>
<ul>
<li><p>Se observa que en este caso el método de Gauss-Seidel requiere
menos iteraciones para converger que el método de Jacobi usando el mismo
criterio de parada.</p></li>
<li><p>Sin embargo, hay casos en que ocurre el efecto contrario: es el
método de Jacobi que converge más rápidamente.</p></li>
<li><p>Como se vera más adelante, depende de las matrices de iteración
de los dos métodos.</p></li>
</ul>
</div>
<div id="estudio-de-la-convergencia" class="section level3"
number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Estudio de la
convergencia</h3>
<p>Para hallar la recurrencia que gobierna la sucesión de aproximaciones
<span class="math inline">\(x^{(k)}\)</span>, se tranforma el sistema
lineal <span class="math inline">\(Ax=b\)</span> en el sistema lineal
equivalente <span class="math inline">\(x=Tx+c\)</span> y, a partir de
la última expresión, definimos la recurrencia de las aproximaciones
<span class="math inline">\(x^{(k)}\)</span>.</p>
<p>Entonces, si la sucesión de aproximaciones <span
class="math inline">\(x^{(k)}\)</span> converge a la solución <span
class="math inline">\(x\)</span>, ésta verificará:</p>
<div class="recuadro-gris">
<p><span class="math display">\[
x=Tx+c,\quad \Rightarrow (I-T)x=c,\quad \Rightarrow x=(I-T)^{-1}c.
\]</span></p>
</div>
<p>Escribiendo la sucesión de aproximaciones <span
class="math inline">\(x^{(k)}\)</span> en función de la aproximación
inicial <span class="math inline">\(x^{(0)}\)</span>, se tendría lo
siguiente:</p>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{align*}
&amp; x^{(1)} =Tx^{(0)}+c\\
&amp; x^{(2)} =Tx^{(1)}+c = T(Tx^{(0)}+c)+c = T^2x^{(0)}+Tc+c\\
&amp; x^{(3)} = Tx^{(2)}+c = T(T^2x^{(0)}+Tc+c) = T^3x^{(0)}+T^2c+Tc+c\\
&amp; \vdots\\
&amp; x^{(k)} = T^kx^{(0)}+T^{k-1}c+\cdots+Tc+c = T^kx^{(0)}+
\left(\sum_{i=0}^{k-1}T^i\right)c
\end{align*}
\]</span></p>
</div>
<p>Teniendo en cuenta que el límite <span
class="math inline">\(x\)</span> de la sucesión de aproximaciones, en
caso de existir, verifica que <span
class="math inline">\(x=(I-T)^{-1}c\)</span>, usando la expresión
anterior,</p>
<div class="recuadro-gris">
<p><span class="math display">\[
x^{(k)}=T^kx^{(0)}+\left(\sum_{i=0}^{k-1}T^i\right)c
\]</span></p>
</div>
<p>parece que para que la sucesión de aproximaciones sea convergente, la
sucesión de matrices <span class="math inline">\(T^k\)</span> debe
tender a la matriz <span class="math inline">\(0\)</span> y que la suma
de matrices <span
class="math inline">\(\displaystyle\left(\sum_{i=0}^{k-1}T^i\right)\)</span>
debe tender a <span class="math inline">\((I-T)^{-1}\)</span>.</p>
<div class="caja-lema">
<p><strong>Lema. Convergencia de una serie de matrices.</strong></p>
<p>Sea <span class="math inline">\(T\)</span> una matriz cuadrada <span
class="math inline">\(n\times n\)</span> cuyo radio espectral es menor
que 1, <span class="math inline">\(\rho(T)&lt;1\)</span>. Entonces,</p>
<div class="recuadro-gris">
<p><span class="math display">\[
(I-T)^{-1}=I+T+T^2+\cdots=\sum_{j=0}^{\infty}T^j.
\]</span></p>
</div>
<p>Es decir, la serie o la suma infinita de matrices <span
class="math inline">\(\displaystyle\sum_{j=0}^{\infty}T^j\)</span> es
convergente si el radio espectral de la matriz <span
class="math inline">\(T\)</span> de la que hallamos las potencias es
menor que 1.</p>
</div>
<ul>
<li>Es más sencillo calcular la norma de una matriz que el cálculo de
sus autovalores</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\left.\begin{align*}
Tx = &amp; \lambda x\\
\|Tx\| = &amp; |\lambda|\|x\|\\
\|Tx\| \leq &amp; \|T\|\|x\|
\end{align*}\right\} \Rightarrow |\lambda|\|x\| \leq \|T\|\|x\|
\Rightarrow |\lambda|\leq \|T\|
\]</span></p>
</div>
<ul>
<li>Por lo que una condición suficiente para la convergencia es: <span
class="math inline">\(\mathbf{\|T\| &lt; 1}\)</span></li>
</ul>
<div id="ejemplo-11" class="section level4 caja-ejemplo"
number="2.4.4.1">
<h4><span class="header-section-number">2.4.4.1</span> Ejemplo 11</h4>
<p>De el sistemas de ecuaciones trabajado en los <strong>Ejemplo
9</strong> y <strong>Ejemplo 10</strong>, se tiene que las matrices
iteración de Jacobi y Gauss-Seidel son: <span class="math display">\[
\begin{pmatrix}
22 &amp; 5 &amp; 5 &amp; 6\\
5 &amp; 19 &amp; 3 &amp; 6\\
5 &amp; 5 &amp; 24 &amp; 5\\
7 &amp; 7 &amp; 4 &amp; 25
\end{pmatrix}
\]</span> Y las matrices de Iteración para cada método son <span
class="math display">\[
T_J = -D^{-1}(L+U),\qquad T_{GS} = -(D+L)^{-1}U
\]</span> obteniendo <span class="math display">\[ T_J \approx
\begin{pmatrix}
0 &amp; -0.2273 &amp; -0.2273 &amp; -0.2727 \\
-0.2632 &amp; 0 &amp; -0.1579 &amp; -0.3158 \\
-0.2083 &amp; -0.2083 &amp; 0 &amp; -0.2083 \\
-0.2800 &amp; -0.2800 &amp; -0.1600 &amp; 0
\end{pmatrix} \qquad T_{GS} \approx
\begin{pmatrix}
0 &amp; -0.2273 &amp; -0.2273 &amp; -0.2727 \\
0 &amp; 0.0598 &amp; -0.0981 &amp; -0.2440 \\
0 &amp; 0.0349 &amp; 0.0678 &amp; -0.1007 \\
0 &amp; 0.0413 &amp; 0.0803 &amp; 0.1608
\end{pmatrix}
\]</span> Radio Espetral y norma de las matrices <span
class="math display">\[
\begin{array}{l}
\rho(T_J) \approx 0.7068 \qquad \|T_J\|_{\infty}\approx 0.7368\\
\rho(T_{GS}) \approx 0.1771 \qquad \|T_{GS}\|_{\infty}\approx 0.7273
\end{array}
\]</span></p>
</div>
</div>
<div id="métodos-de-sobrerelajación-sor" class="section level3"
number="2.4.5">
<h3><span class="header-section-number">2.4.5</span> Métodos de
Sobrerelajación (SOR)</h3>
<ul>
<li><p>Se ha visto que la clave para ver si la sucesión de
aproximaciones <span class="math inline">\(x^{(k)}\)</span> es
convergente es el radio espectral de la matriz de iteración <span
class="math inline">\(T\)</span>, y que la sucesión de aproximaciones se
define como: <span class="math display">\[
x^{(k)}=Tx^{(k-1)}+c.
\]</span></p></li>
<li><p>Concretamente, si el radio espectral de la matriz de iteración es
menor que 1, <span class="math inline">\(\tho(T)&lt;1\)</span>, la
sucesión de aproximaciones <span class="math inline">\(x^{(k)}\)</span>
convergerá a la solución exacta del sistema lineal x.</p></li>
<li><p>La idea es diseñar un método dependiendo de un parámetro y
estudiar para qué valores del parámetro el radio espectral de la matriz
de iteración correspondiente tiene un valor lo más pequeño
posible.</p></li>
<li><p>Para ello, vamos a hallar un método iterativo dependiendo de un
parámetro <span class="math inline">\(\omega\)</span> hallando un
sistema equivalente al sistema a resolver <span
class="math inline">\(Ax=b\)</span>.</p></li>
<li><p>Recordemos que podemos descomponer la matriz del sistema <span
class="math inline">\(A\)</span> en la parte diagonal <span
class="math inline">\(D\)</span>, la parte triangular inferior <span
class="math inline">\(L\)</span> y la parte triangular superior <span
class="math inline">\(U\)</span>: <span
class="math inline">\(A=D+L+U\)</span>.</p></li>
<li><p>Escribamos el sistema <span class="math inline">\(Ax=b\)</span>
en un sistema equivalente introduciendo un parámetro <span
class="math inline">\(\omega\)</span>:</p></li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
\begin{align*}
Ax &amp; = b \Rightarrow (D+L+U)x=b\\
\omega(D+L+U)x &amp; = \omega b\\
(\omega D+\omega L+\omega U)x &amp; = \omega b\\
(D+(\omega-1)D + \omega L+\omega U)x &amp; = \omega b\\
(D+\omega L)x &amp; = ((1-\omega)D-\omega U)x+\omega b.
\end{align*}
\]</span></p>
</div>
<ul>
<li>La sucesión de aproximaciones <span
class="math inline">\(x^{(k)}_{\omega}\)</span> definida por el
parámetro viene dada por:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
(D+\omega L)x^{(k)}_{\omega}=((1-\omega)D-\omega
U)x^{(k-1)}_{\omega}+\omega b
\]</span></p>
</div>
<ul>
<li>Entonces, la matriz de iteración y el vector independiente del
método de SOR son:</li>
</ul>
<div class="recuadro-gris">
<p><span class="math display">\[
T_{\omega} = (D+\omega L)^{-1}((1-\omega )D -  \omega U), \quad
c_{\omega} = \omega(D+\omega L)^{-1}b
\]</span></p>
</div>
<div id="ejemplo-12" class="section level4 caja-ejemplo"
number="2.4.5.1">
<h4><span class="header-section-number">2.4.5.1</span> Ejemplo 12</h4>
<p>De el sistemas de ecuaciones trabajado en los <strong>Ejemplo
9</strong> y <strong>Ejemplo 10</strong>, se tiene que la matriz de
iteración de SOR es: <span class="math display">\[ A= \begin{pmatrix}
22 &amp; 5 &amp; 5 &amp; 6\\
5 &amp; 19 &amp; 3 &amp; 6\\
5 &amp; 5 &amp; 24 &amp; 5\\
7 &amp; 7 &amp; 4 &amp; 25
\end{pmatrix}
\]</span> <span class="math display">\[
T_{\omega} = (D+\omega L)^{-1}((1-\omega)D-\omega U)
\]</span></p>
<p>Veamos como varia el radio espectral a medida que <span
class="math inline">\(\omega\)</span> varia</p>
<pre><code>## El valor óptimo de omega es: 1.0091</code></pre>
<pre><code>## El radio espectral mínimo es: 0.1757</code></pre>
<p><img src="unidad02_files/figure-html/unnamed-chunk-8-1.png" width="1152" /></p>
<p>Veamos como varia la norma de <span
class="math inline">\(T_{\omega}\)</span> a medida que <span
class="math inline">\(\omega\)</span> varia</p>
<pre><code>## El valor óptimo de omega según la norma infinita es: 0.9909</code></pre>
<pre><code>## La norma infinita mínima es: 0.7298</code></pre>
<p><img src="unidad02_files/figure-html/unnamed-chunk-9-3.png" width="1152" /></p>
</div>
<ul>
<li>Observaciones
<ul>
<li>Si se hace <span class="math inline">\(\omega=1\)</span>, el método
de SOR es el conocido método de Gauss-Seidel. Por tanto, este método
pueden considerarse una generalización del método de Gauss-Seidel.</li>
<li>Este método se denomina método de relajación. Si <span
class="math inline">\(0&lt;\omega &lt;1\)</span>, el método se denomina
método de subrelajación y si <span
class="math inline">\(\omega&gt;1\)</span>, método de sobrerelajación.
Dichos métodos se abrevian con métodos SOR. (successive
over-relaxation).</li>
<li>Para que el método iterativo de SOR converja, el parámetro <span
class="math inline">\(\omega\)</span> tiene que estar necesariamente
entre <span class="math inline">\(0 y 2\)</span>.</li>
</ul></li>
</ul>
<p><strong>Métodos iterativos vs. métodos directos</strong></p>
<ul>
<li><p>Cuando la dimensión de la matriz del sistema lineal a resolver
<span class="math inline">\(Ax=b\)</span> no es demasiado grande, es
mejor usar los métodos directos al ser éstos más eficientes en este caso
y proporcionar soluciones exactas.</p></li>
<li><p>En cambio, si la dimensión de la matriz del sistema lineal es muy
grande y es una matriz sparse que significa que tiene muchos ceros sin
ninguna estructura predefinida, es mejor usar métodos iterativos ya que
ahorran en almacenamiento y tienen un coste computacional más bajo que
los métodos directos. -Los sistemas de este segundo tipo aparecen en
problemas de frontera en ecuaciones en derivadas parciales así como en
problemas de redes neuronales en el campo de la inteligencia
artificial.</p></li>
</ul>
<div class="cuadro-alg">
<p><strong>FUNCION Metodo_SOR(A, b, x0, omega, tol,
max_iter):</strong></p>
<pre><code>n = dimensión de A (número de filas)
x = x0  # Vector de solución inicial

PARA k = 1 HASTA max_iter:
    x_anterior = COPIA(x)

    # Calcular cada componente de la nueva solución
    PARA i = 1 HASTA n:
        # Suma de los componentes ya actualizados en esta iteración (j &lt; i)
        suma_nuevos = 0
        PARA j = 1 HASTA i - 1:
            suma_nuevos = suma_nuevos + A[i, j] * x[j]
        FIN PARA

        # Suma de los componentes de la iteración anterior (j &gt; i)
        suma_viejos = 0
        PARA j = i + 1 HASTA n:
            suma_viejos = suma_viejos + A[i, j] * x_anterior[j]
        FIN PARA

        # Calcular y actualizar el componente x[i] usando la fórmula de SOR
        termino_gs = (b[i] - suma_nuevos - suma_viejos) / A[i, i]
        x[i] = (1 - omega) * x_anterior[i] + omega * termino_gs
    FIN PARA

    # Criterio de parada (usando el error absoluto como ejemplo)
    SI norma(x - x_anterior) &lt; tol ENTONCES
        RETORNAR x  # Solución encontrada
    FIN SI
FIN PARA

RETORNAR &quot;El método no convergió en max_iter iteraciones&quot;</code></pre>
<p><strong>FIN FUNCION</strong></p>
</div>
<div id="ejemplo-13-solución-con-sor"
class="section level4 caja-ejemplo" number="2.4.5.2">
<h4><span class="header-section-number">2.4.5.2</span> Ejemplo 13:
Solución con SOR</h4>
<p>Resolvamos el sistema de ecuaciones del <strong>Ejemplo 9</strong>
utilizando el método de Sobrerelajación Sucesiva (SOR).</p>
<p><span class="math display">\[
\begin{align*}
E_1: &amp; 22x_1 + 5x_2 + 5x_3 + 6x_4 = 5\\
E_2: &amp; 5x_1 + 19x_2 + 3x_3 + 6x_4 = 7\\
E_3: &amp; 5x_1 + 5x_2 + 24x_3 + 5x_4 = 8\\
E_4: &amp; 7x_1 + 7x_2 + 4x_3 + 25x_4 = 5
\end{align*}
\]</span></p>
<p>La fórmula de iteración para SOR es: <span class="math display">\[
x_i^{(k)} = (1-\omega)x_i^{(k-1)} + \frac{\omega}{a_{ii}} \left( b_i -
\sum_{j&lt;i} a_{ij}x_j^{(k)} - \sum_{j&gt;i} a_{ij}x_j^{(k-1)} \right)
\]</span></p>
<p>Del <strong>Ejemplo 12</strong>, sabemos que el parámetro de
relajación óptimo para este sistema es <span
class="math inline">\(\omega \approx 1.01\)</span>. Usaremos este valor
para acelerar la convergencia. Las ecuaciones de recurrencia son:</p>
<p><span class="math display">\[
\begin{align*}
x_1^{(k)} &amp;= (1-\omega)x_1^{(k-1)} + \frac{\omega}{22}\left(5 -
5x_2^{(k-1)} - 5x_3^{(k-1)} - 6x_4^{(k-1)}\right) \\
x_2^{(k)} &amp;= (1-\omega)x_2^{(k-1)} + \frac{\omega}{19}\left(7 -
5x_1^{(k)} - 3x_3^{(k-1)} - 6x_4^{(k-1)}\right) \\
x_3^{(k)} &amp;= (1-\omega)x_3^{(k-1)} + \frac{\omega}{24}\left(8 -
5x_1^{(k)} - 5x_2^{(k)} - 5x_4^{(k-1)}\right) \\
x_4^{(k)} &amp;= (1-\omega)x_4^{(k-1)} + \frac{\omega}{25}\left(5 -
7x_1^{(k)} - 7x_2^{(k)} - 4x_3^{(k)}\right)
\end{align*}
\]</span></p>
<p>A continuación, ejecutando el código Python para resolverlo.</p>
<pre><code>## --- Solución con SOR (omega = 1.0100) ---
## x^(1) = [0.22954545 0.3110945  0.22290701 0.01308525]
## x^(2) = [0.10106799 0.3024102  0.24678571 0.04788494]
## x^(3) = [0.08927922 0.29072308 0.2441642  0.05459956]
## x^(4) = [0.09083202 0.28870369 0.24287573 0.05487259]
## x^(5) = [0.09150059 0.28866458 0.24269871 0.05472045]
## x^(6) = [0.09158542 0.28871917 0.24270316 0.05468183]
## x^(7) = [0.09158166 0.28873124 0.24270949 0.05467884]
## x^(8) = [0.0915783  0.28873195 0.24271062 0.05467944]
## x^(9) = [0.09157775 0.28873172 0.24271064 0.05467965]
## x^(10) = [0.09157774 0.28873166 0.24271062 0.05467967]
## Número total de iteraciones: 10
## Solución x = [0.09157774 0.28873166 0.24271062 0.05467967]</code></pre>
</div>
</div>
</div>
<div id="guía-de-ejercicios" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Guía de
ejercicios</h2>
<p>En el siguiente enlace <code>Ejercicios_Unidad2</code> encontrará una
lista de ejercicios relacionado con los distintos tópicos cubiertos en
esta segunda unidad.</p>
<p>Descargue el PDF de esta guía de ejercicios aqui:</p>
<p><a href="Ejercicios_Unidad2.pdf">Descargar Guía PDF</a></p>
<hr />
<p><small>Última revisión: 08 de noviembre, 2025</small></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
