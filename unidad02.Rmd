---
title: "Álgebra Lineal Numérica"
author: "José Luis Ramírez"
date: "Octubre 2025"
output:
  html_document:
    self_contained: true
    mathjax: default
    pandoc_args: "--mathjax"
    toc: true
    toc_depth: 4
    toc_float: true
    theme: readable
    css: css/estilos.css
    number_sections: true
---

# Motivación

* En el planteamiento matemático de muchos problemas realistas, los sistemas de ecuaciones algebraicas, y de una manera especial los lineales, aparecen de manera natural.
* La búsqueda de métodos de resolución de sistemas de ecuaciones lineales es un tema de gran importancia en la ciencia.
* El objetivo de este tema es desarrollar estrategias numéricas que permitan resolver sistemas de ecuaciones relativamente grandes de una manera eficiente.
* El estudio de los autovalores de sistemas surge por doquier en muchas áreas de la ciencia, ingeniería, economía ...
  - Análisis de estructuras
  - Diseño de sistemas electrónicos
  - Análisis de sistemas eléctricos
  - Mercados financieros.
* Es también muy importante para analizar el comportamiento de métodos numéricos.

# Solución de Sistemas de Ecuaciones Lineales
* El objetivo de este tema es desarrollar estrategias numéricas que permitan resolver sistemas de ecuaciones relativamente grandes de una manera eficiente.
* Además, se analizarán con detalle algunos métodos directos.
* Si bien existen métodos exactos como el método de Cramer, estos son muy costosos de aplicar en situaciones donde los sistemas a resolver tienen muchas ecuaciones.
* El número total de operaciones para resolver un sistema de dimensión $n$ con este método es

::: recuadro-gris
$$
T_C = (n+1)^2n!-1
$$
$$
\begin{array}{|c|c|}\hline
n & T_C \\\hline
5 & 4319\\\hline
10 & 4\times10^{8}\\\hline
100 & 10\times10^{158}\\\hline
\end{array}
$$
:::

## Introducción
* Un sistema de $n$-ecuaciones (con coeficientes reales) en las $n$-incógnitas $x_1, x_2, \ldots , x_n$ es un conjunto de $n$ ecuaciones de la forma:

::: recuadro-gris
$$
\left\{
\begin{array}{rclclc}
a_{11}x_{1} & + & a_{12}x_{2} & + \cdots + & a_{1n}x_{n} & = b_{1} \\
a_{21}x_{1} & + & a_{22}x_{2} & + \cdots + & a_{2n}x_{n} & = b_{2} \\
\cdots & & \cdots & \cdots & \cdots & \cdots
\\ 
a_{n1}x_{1} & + & a_{n2}x_{2} & + \cdots + & a_{nn}x_{n} & = b_{n}
\end{array}
\right.
$$

:::
* A los números $a_{ij}$ se les denomina coeficientes del sistema y a los $b_i$ términos independientes.

Si se introducen las matrices

::: recuadro-gris
$$
A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}, \quad
x = \begin{pmatrix}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{pmatrix}, \quad
b = \begin{pmatrix}
b_{1} \\
b_{2} \\
\vdots \\
b_{n}
\end{pmatrix}
$$
:::

* El sistema de ecuaciones puede escribirse de forma matricial como

::: recuadro-gris
$$
Ax = b
$$
:::

### Clasificación
Podemos clasificar los sistemas de ecuaciones lineales atendiendo a:

* Su tamaño:
  - Pequeños: $n \leq 300$ donde $n$ representa el número de ecuaciones.
  - Grandes: $n > 300$
* Su estructura
  - Si la matriz posee pocos elementos nulos diremos que se trata de un sistema lleno.
  - Si, por el contrario, la matriz contiene muchos elementos nulos, diremos que la matriz, y por lo tanto, el sistema lineal es disperso o sparce.
  
### Solución
La primera opción que se plantea es

::: recuadro-gris
$$
x = A^{-1}b
$$
:::

* No es eficiente (demasiadas operaciones).
* Si el determinante de $A$ es próximo a cero, el error de redondeo puede ser muy grande, y esto es dificil de estimar numéricamente

::: recuadro-gris
$$
\det(\gamma A) = \gamma^n \det(A)
$$
:::

* Por lo tanto, se hace necesario desarrollar métodos numéricos que permitan resolver sistemas de ecuaciones lineales de una manera eficiente y con un control del error cometido.

## Métodos directos

### Sistemas triangulares
* Un sistema de ecuaciones lineales se dice que es triangular superior si su matriz de coeficientes $A$ es una matriz triangular superior, es decir, si todos los elementos situados por debajo de la diagonal principal son nulos.

::: recuadro-gris
$$
\left\{\begin{array}{r}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1i}x_i + \cdots + a_{1n}x_n = b_1 \\
a_{22}x_2 + \cdots + a_{2i}x_i + \cdots + a_{2n}x_n = b_2 \\
\vdots \\
a_{ii}x_i + \cdots + a_{in}x_n = b_i \\
\vdots \\
a_{nn}x_n = b_n\\
\end{array}\right.
$$
:::

* Como $a_{n,n} \ne 0$, se puede despejar $x_n$ de la última ecuación, y se obtiene:

::: recuadro-gris
$$
x_n = \dfrac{b_n}{a_{nn}}
$$
:::

* Sustituyendo este valor en la ecuación $n-1$ y despejando $x_{n-1}$ se tiene:

::: recuadro-gris
$$x_{n-1} = \dfrac{b_{n-1} - a_{n-1,n}x_n}{a_{n-1,n-1}}$$
:::

* Continuando este proceso, se obtiene la siguiente fórmula de recurrencia para $i = n-1, n-2, \ldots, 1$:

::: recuadro-gris
$$x_i = \dfrac{b_i - \displaystyle\sum_{j=i+1}^{n} a_{ij}x_j}{a_{ii}}$$
::: 

* Este proceso de resolución de sistemas triangulares superiores se denomina sustitución regresiva.

::: cuadro-alg
**FUNCION Sustitucion_Regresiva(A, b):**

    n = dimension de A (número de filas)

    # 1. Resolver la última incógnita (x_n)
    x[n] = b[n] / A[n, n]

    # 2. Iteración regresiva (desde n-1 hasta 1)
    PARA i = n-1 HASTA 1:
        suma = 0
        PARA j = i+1 HASTA n:
            suma = suma + A[i, j] * x[j]
        FIN PARA
        x[i] = (b[i] - suma) / A[i, i]
    FIN PARA

    RETORNAR x
**FIN FUNCION**
:::

```{python}
import numpy as np
def sustitucion_regresiva(A, b):
    n = len(b)
    x = np.zeros(n)
    x[n-1] = b[n-1] / A[n-1, n-1]
    for i in range(n-2, -1, -1):
        suma = np.dot(A[i, i+1:], x[i+1:])
        x[i] = (b[i] - suma) / A[i, i]
    return x
```

* En el caso de sistemas triangulares inferiores, el proceso es análogo, pero se realiza una sustitución progresiva.
* La fórmula de recurrencia para $i = 1, 2, \ldots, n-1, n$ es:

::: recuadro-gris
$$x_i = \dfrac{b_i - \displaystyle\sum_{j=1}^{i-1} a_{ij}x_j}{a_{ii}}$$
::: 
* El proceso de resolución de sistemas triangulares inferiores se denomina sustitución progresiva.

::: cuadro-alg
**FUNCION Sustitucion_Progresiva(A, b):**
    
    n = dimension de A (número de filas)

    # 1. Resolver la primera incógnita (x_1)
    x[1] = b[1] / A[1, 1]

    # 2. Iteración progresiva (desde 2 hasta n)
    PARA i = 2 HASTA n:
        suma = 0
        PARA j = 1 HASTA i-1:
            suma = suma + A[i, j] * x[j]
        FIN PARA
        x[i] = (b[i] - suma) / A[i, i]
    FIN PARA

    RETORNAR x
**FIN FUNCION**
:::

### Eliminación Gaussiana

* Suponiendo que $A$ no posee una estructura triangular, y es tal que no requiere intercambio de filas para transformar el sistema original en otro equivalente que sí la posea forma triangular.
* Una forma de resolver el sistema es transformar la matriz $A$ en una matriz triangular superior $U$ mediante operaciones elementales sobre las filas de la matriz.
* Digamos que el sistema original es:

::: recuadro-gris
$$
Ax = b \equiv \left\{
\begin{array}{l}
E_1: a_{11}x_{1}  +  a_{12}x_{2}  + \cdots +  a_{1n}x_{n}  = b_{1} \\
E_2: a_{21}x_{1}  +  a_{22}x_{2}  + \cdots +  a_{2n}x_{n}  = b_{2} \\
\cdots  \cdots  \cdots  \cdots  \cdots \cdots \cdots\\ 
E_n: a_{n1}x_{1}  +  a_{n2}x_{2}  + \cdots +  a_{nn}x_{n}  = b_{n}
\end{array}
\right.
$$  
:::

* Para eliminar la incógnita $x_1$ de las ecuaciones $E_2, E_3, \ldots, E_n$, se pueden realizar las siguientes operaciones elementales sobre las filas:

::: recuadro-gris
$$
E_i \leftarrow E_i - \dfrac{a_{i1}}{a_{11}} E_1 \quad \text{para } i = 2, 3, \ldots, n
$$
:::

* Los números $\dfrac{a_{i1}}{a_{11}}$ se denominan multiplicadores de eliminación.
* Después de aplicar estas operaciones, se obtiene un nuevo sistema equivalente:  

::: recuadro-gris
$$
A^{(1)}x = b^{(1)} \equiv \left\{
\begin{array}{l}
E_1: a_{11}x_{1}  +  a_{12}x_{2}  + \cdots +  a_{1n}x_{n}  = b_{1} \\
E_2: 0  +  a_{22}^{(1)}x_{2}  + \cdots +  a_{2n}^{(1)}x_{n}  = b_{2}^{(1)} \\
\cdots  \cdots  \cdots  \cdots  \cdots \cdots \cdots\\ 
E_n: 0  +  a_{n2}^{(1)}x_{2}  + \cdots +  a_{nn}^{(1)}x_{n}  = b_{n}^{(1)}
\end{array}
\right.
$$  
:::

* Repitiendo este proceso para las incógnitas $x_2, x_3, \ldots, x_{n-1}$, se obtiene finalmente un sistema triangular superior:

::: recuadro-gris
$$
Ux = y \equiv \left\{
\begin{array}{l}
E_1: u_{11}x_{1}  +  u_{12}x_{2}  + \cdots +  u_{1n}x_{n}  = y_{1} \\
E_2: 0  +  u_{22}x_{2}  + \cdots +  u_{2n}x_{n}  = y_{2} \\
\cdots  \cdots  \cdots  \cdots  \cdots \cdots \cdots\\ 
E_n: 0  +  0  + \cdots +  u_{nn}x_{n}  = y_{n}
\end{array}
\right.
$$  
:::

* Donde $U$ es una matriz triangular superior y $y$ es el nuevo vector de términos independientes.
* La solución del sistema original $Ax = b$ se obtiene resolviendo primero el sistema triangular superior $Ux = y$ mediante sustitución regresiva.

::: cuadro-alg
**FUNCION Eliminacion_Gauss(A, b):**
    
    n = dimension de A (número de filas)

    # 1. Eliminación hacia adelante
    PARA k = 1 HASTA n-1:
        PARA i = k+1 HASTA n:
            multiplicador = A[i, k] / A[k, k]
            A[i, k] = 0  # Elemento eliminado
            PARA j = k+1 HASTA n:
                A[i, j] = A[i, j] - multiplicador * A[k, j]
            FIN PARA
            b[i] = b[i] - multiplicador * b[k]
        FIN PARA
    FIN PARA
    RETORNAR A, b
**FIN FUNCION**
:::

::: caja-ejemplo
#### Ejemplo 1:
Resolver el sistema de ecuaciones lineales utilizando el método de eliminación de Gauss:
$$
\left\{
\begin{array}{rcl}
2x_1 + x_2 + x_3 & = & -3 \\
x_1 -2x_2 + 3x_3 & = & 6 \\
x_1  - x_2 - x_3 & = & 6
\end{array}
\right.
$$
```  {python}
import numpy as np
A = np.array([[2, 1, 1],
              [1, -2, 3],
              [1, -1, -1]], dtype=float)
b = np.array([-3, 6, 6], dtype=float)
def eliminacion_gauss(A, b):
    n = len(b)
    for k in range(n-1):
        for i in range(k+1, n):
            multiplicador = A[i, k] / A[k, k]
            A[i, k] = 0  # Elemento eliminado
            for j in range(k+1, n):
                A[i, j] = A[i, j] - multiplicador * A[k, j]
            b[i] = b[i] - multiplicador * b[k]
    return A, b
A_triangular, b_modificado = eliminacion_gauss(A, b)
print("Matriz triangular superior U:")
print(A_triangular)
print("Vector modificado y:")
print(b_modificado)
def sustitucion_regresiva(A, b):
    n = len(b)
    x = np.zeros(n)
    x[n-1] = b[n-1] / A[n-1, n-1]
    for i in range(n-2, -1, -1):
        suma = np.dot(A[i, i+1:], x[i+1:])
        x[i] = (b[i] - suma) / A[i, i]
    return x
solucion = sustitucion_regresiva(A_triangular, b_modificado)
print("Solución del sistema:")
print(solucion)
```
::: 

### Estrategias de Pivoteo
* **Pivoteo Parcial**
* En la eliminación gaussiana, es posible que durante el proceso de eliminación se encuentren elementos pivote que sean cero o muy pequeños, lo que puede llevar a errores numéricos significativos.

::: caja-ejemplo
#### Ejemplo 2:
Considere el siguiente sistema de ecuaciones lineales en una aritmética de punto flotante a 4 dígitos con redondeo correcto:
$$
\left\{
\begin{array}{rcl}
0.003000x_1 + 59.14x_2 & = & 59.17 \\
5.291x_1 - 6.130x_2 & = & 46.78
\end{array}
\right.
$$

cuya solución exacta es $x_1 = 10$ y $x_2 = 1$.

* Aplicando la eliminación gaussiana sin pivoteo, se obtiene el siguiente resultado $\tilde{x}_1=-10.00$ y $\tilde{x}_2=1.001$.
* El error relativo en la solución es:

::: recuadro-gris
$$
\begin{array}{l}
\text{Error relativo en } x_1 = \dfrac{|\tilde{x}_1 - x_1|}{|x_1|} = \dfrac{|-10.00 - 10|}{|10|} = 2.0 \\
\text{Error relativo en } x_2 = \dfrac{|\tilde{x}_2 - x_2|}{|x_2|} = \dfrac{|1.001 - 1|}{|1|} = 0.001
\end{array}
$$
:::

* El error tan grande de la solución numérica de $x_1$, resulta del error pequeño de $0.001$ al resolver para $x_2$.
:::

* Ahora, si se elige como pivote el máximo entre $a_{1,1}$ y $a_{2,1}$.
* Pivote =max(|0.003|; |5.291|) = 5.291, por tanto se realiza un intercambio de filas quedando el sistema de la siguiente manera:

::: recuadro-gris
$$
\left\{
\begin{array}{rcl}
5.291x_1 − 6.130x_2 &=& 46.78\\
0.003000x_1 + 59.14x_2 &=& 59.17
\end{array}\right.
$$
:::

* cuya solución aproximada es $\tilde{x}_2 = 1 = x_2$ y $\tilde{x}_1 = 10 = x_1$.
* Por tanto para cada paso de eliminación Gaussiana tenemos
que:
  - Seleccionar como pivote el elemento de mayor valor absoluto en la columna correspondiente, desde la fila actual hasta la última fila.
  - Intercambiar la fila actual con la fila que contiene el pivote seleccionado.
  
  ::: recuadro-gris
$$
\text{Pivoteo parcial: } \quad
E_i \leftrightarrow E_p \quad \text{donde } p = \underset{i \leq k \leq n}{\mathrm{argmax}} |a_{k,i}|
$$
:::

* **Pivoteo Parcial Escalado**
* Modificando ligeramente el **pivotaje parcial** de la siguiente forma.

* Concretamente, en primer lugar, se calcula el valor máximo en valor absoluto de cada fila $i$-ésima de la matriz del sistema a resolver $A$, $n\times n$:

::: recuadro-gris
$$
s_i = \max_{1 \leq j \leq n} |a_{ij}| \quad \text{para } i = 1, 2, \ldots, n
$$
:::

*Dichos valores $s_i$ sólo se calculan una vez al principio del algoritmo y no aumentan demasiado el coste computacional del mismo.

* A continuación, imaginemos que estamos en el $k$-ésimo pase del **algoritmo de Gauss**. Entonces, en lugar de hallar el máximo de $|a^{(k)}_{ji}|$ desde $j=k$ hasta $j=n$, se halla el máximo de $\left|\frac{a^{(k)}_{ji}}{s_j}\right|$ desde $j=i$ hasta $j=n$:

::: recuadro-gris
$$
\max_{k \leq j \leq n} \left|\dfrac{a^{(k)}_{ji}}{s_j}\right|
$$
:::

* Es decir, “escalamos” cada fila por el valor $s_j$  y hallamos el máximo de los valores escalados.
* Llamemos $j_{\max}$ a dicho valor.
* Igual que en el pivoto parcial, realizamos un cambio entre las filas $k$ y $j_{\max}$: $E_k \leftrightarrow E_{j_{\max}}$.
* El **pivoteo parcial escalado** es especialmente útil cuando los elementos de la matriz del sistema varían mucho en magnitud.

::: caja-ejemplo
#### Ejemplo 3:
Resolvamos el siguiente sistema usando pivotaje parcial escalado:

::: recuadro-gris
$$
\left\{
\begin{array}{rrrrrrcr}
E_1:  & x_1 & + x_2 & - 3x_3  & -x_4 & -2x_5 & = & 2 \\
E_2:  & x_1 & + 2x_2 & + 2x_3  &  & + 3x_5 & = & 2 \\
E_3:  & x_1 & -x_2 & +3x_3  & +2x_4 & - x_5 & = & 2 \\
E_4:  &  & x_2 &   & + 4x_4 & -x_5 & = & 3 \\
E_5:  & x_1 & + 3x_2 & + x_3  &  & + 5x_5 & = & 1 \\
\end{array}
\right.
$$
:::

Los valores $s_i$ son en este caso:
$$
s_1 = 3, \quad s_2 = 3, \quad s_3 = 3, \quad s_4 = 4, \quad s_5 = 5
$$
* Primer paso (no se ha realizado ningún cambio de filas ya que el máximo valor entre los valores
$$
\left|\dfrac{1}{3}\right|, \left|\dfrac{1}{3}\right|, \left|\dfrac{1}{3}\right|, 0, \left|\dfrac{1}{5}\right|
$$

es $\dfrac{1}{3}$. Aplicamos la eliminación gaussiana para obtener:

::: recuadro-gris
$$
A^{(1)} = \begin{pmatrix}
1 & 1 & -3 & -1 & -2 \\
0 & 1 & 5 & 1 & 5 \\
0 & -2 & 6 & 3 & 1 \\
0 & 1 & 0 & 4 & -1 \\
0 & 2 & 4 & 1 & 7
\end{pmatrix}, \quad
b^{(1)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1
\end{pmatrix}
$$
:::

* Segundo paso (cambiando las filas 2 y 3 ya que el máximo valor entre los valores
$$
\left|\dfrac{1}{5}\right|, \left|\dfrac{-2}{6}\right|, \left|\dfrac{1}{4}\right|, \left|\dfrac{2}{7}\right|
$$
es $\dfrac{2}{6}=\dfrac{1}{3}$. Aplicamos la eliminación gaussiana para obtener:

::: recuadro-gris
$$
A^{(2)} = \begin{pmatrix}
1 & 1 & -3 & -1 & -2 \\
0 & -2 & 6 & 3 & 1 \\
0 & 0 & 8 & 2.5 & 5.5  \\
0 & 0 & 3 & 5.5 & -0.5 \\
0 & 0 & 10 & 4 & 8
\end{pmatrix}, \quad
b^{(2)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1
\end{pmatrix}
$$
:::

* Tercer paso (no hay cambio de filas ya que el máximo valor entre los valores
$$
\left|\dfrac{8}{8}\right|, \left|\dfrac{3}{5.5}\right|, \left|\dfrac{10}{10}\right|
$$
es $\dfrac{8}{8}=1$. Aplicamos la eliminación gaussiana para obtener:

::: recuadro-gris
$$
A^{(3)} = \begin{pmatrix}
1 & 1 & -3 & -1 & -2 \\
0 & -2 & 6 & 3 & 1 \\
0 & 0 & 8 & 2.5 & 5.5  \\
0 & 0 & 0 & 4.5625 & -2.5625\\
0 & 0 & 0 & 0.8750 & 1.1250
\end{pmatrix}, \quad
b^{(3)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1
\end{pmatrix}
$$
:::

* Cuarto paso (no hay cambio de filas ya que el máximo valor entre los valores
$$
\left|\dfrac{4.5625}{4.5625}\right|, \left|\dfrac{0.875}{1.5}\right|
$$
es $\dfrac{4.5625}{4.5625}=1$. Aplicamos la eliminación gaussiana para obtener: 

::: recuadro-gris
$$
U = A^{(4)} = \begin{pmatrix}
1 & 1 & -3 & -1 & -2 \\
0 & -2 & 6 & 3 & 1 \\
0 & 0 & 8 & 2.5 & 5.5  \\
0 & 0 & 0 & 4.5625 & -2.5625\\
0 & 0 & 0 & 0 & 1.6164
\end{pmatrix}, \quad
y = b^{(4)} = \begin{pmatrix}
2 \\
0 \\ 0 \\ 3 \\ -1.5753
\end{pmatrix}
$$
:::
* Finalmente, resolvemos el sistema triangular superior $Ux = y$ mediante sustitución regresiva para obtener la solución del sistema original:

::: recuadro-gris
$$
\begin{align*}
x_5 & = \dfrac{-1.5753}{1.6164} \approx -0.97457 \\
x_4 & = \dfrac{3 - (-2.5625)(-0.97457)}{4.5625} \approx 0.11018 \\
x_3 & = \dfrac{0 - 2.5(0.11018) - 5.5(-0.97457)}{8} \approx 0.63559 \\
x_2 & = \dfrac{0 - 6(0.63559) - 3(0.11018)-1(-0.97457)}{-2} \approx 1.5848 \\
x_1 & = 2 - 1(1.5848) - (-3)(0.63559) - (-1)(0.11018) - (-2)(-0.97457) \approx 0.48310
\end{align*}
$$
:::

:::

* **Pivoteo Completo o Total**
* Una manera de optimizar el error de redondeo cometido cuando aplicamos el algoritmo de Gauss es realizar un pivotaje global:
  -es decir, modificar el algoritmo de Gauss en el $k$-ésimo paso eligiendo el máximo valor en valor absoluto de la submatriz de $A^(k)$ formada por las filas $k,k+1,\ldots,n$ y por las columnas $k,k+1,\ldots,n$.
  - Concretamente hallamos $i_{\max}$,$j_{\max}$ tal que:

::: recuadro-gris
$$
\left|a^{(k)}_{i_{\max},j_{\max}}\right| = \max_{i,j=k,\ldots,n}\left|a^{(k)_{i,j}}\right|
$$
:::

* A continuación, realizamos un cambio entre las filas $k$ y $i_{\max}$ y un cambio entre las columnas $k$ y $j_{\max}$.
* **Permutar filas** no afecta a las soluciones del sistema de ecuaciones al obtener un sistema de ecuaciones equivalente. Sin embargo, **permutar columnas** equivale a permutar los valores de las variables correspondientes.
* Concretamente, cuando cambiamos las columnas $k$ y $j_{\max}$, intercambiamos el “papel” de las variables $x_k$
 y $x_{j_{\max}}$.
* Por tanto, necesitamos recordar todos los cambios de columnas para realizar el cambio inverso en la solución final.

::: caja-ejemplo
#### Ejemplo 4
Aplicando esta tecnica de pivoteo al ejemplo anterior

::: recuadro-gris
$$
\left\{
\begin{array}{rrrrrrcr}
E_1:  & x_1 & + x_2 & - 3x_3  & -x_4 & -2x_5 & = & 2 \\
E_2:  & x_1 & + 2x_2 & + 2x_3  &  & + 3x_5 & = & 2 \\
E_3:  & x_1 & -x_2 & +3x_3  & +2x_4 & - x_5 & = & 2 \\
E_4:  &  & x_2 &   & + 4x_4 & -x_5 & = & 3 \\
E_5:  & x_1 & + 3x_2 & + x_3  &  & + 5x_5 & = & 1 \\
\end{array}
\right. \equiv
\left(\begin{array}{rrrrr|r}
1 & 1 & -3 & -1 & -2 & 2\\
1 & 2 & 2 & 0 & 3 & 2\\
1 & -1 & 3 & 2& -1 & 2\\
0 & 1 & 0 & 4 & -1 & 3\\
1 & 3 & 1 & 0 & 5 & 1\\
\end{array}\right)
$$
:::

* Paso 1. El máximo valor de la submatriz del sistema formada por las 5 primeras filas y columnas vale 5
 que corresponde a la fila 5 y a la columna 5. Entonces permutamos la fila 1 con la 5 y a continuación la columna 1
 con la 5 y aplicamos el algoritmo de Gauss obteniendo la matriz siguiente:
 
::: recuadro-gris
 $$
 A^{(1)}=\left(\begin{array}{rrrrr|r}
  5 & 3 & 1 & 0 & 1 & 1\\
  0 & 0.2 & 1.4 & 0 & 0.4 & 1.4\\
  0 & -0.4 & 3.2 & 2 & 1.2 & 2.2\\
  0 & 1.6 & 0.2 & 4 & 0.2 & 3.2\\
  0 & 2.2 & -2.6 & -1 & 1.4 & 2.4
  \end{array}\right)
 $$
:::
 
* Las variables del sistema anterior quedan en el siguiente orden $x_5$, $x_2$, $x_3$, $x_4$ y $x_1$.
 
* Paso 2. El máximo valor de la submatriz del sistema formada por las 4 últimas filas y columnas vale 4 que corresponde a la fila 4 y a la columna 4. Entonces permutamos la fila 2 con la 4 y a continuación la columna 2 con la 4 y aplicamos el algoritmo de Gauss obteniendo la matriz siguiente:
 
::: recuadro-gris
 $$
 A^{(2)}=\left(\begin{array}{rrrrr|r}
   5 & 0 & 1 & 3 & 1 & 1\\
   0 & 4 & 0.2 & 1.6 & 0.2 & 3.2\\
   0 & 0 & 3.1 & -1.2 & 1.1 & 0.6\\
   0 & 0 & 1.4 & 0.2 & 0.4 & 1.4\\
   0 & 0 & -2.55 & 2.6 & 1.45 & 3.2
 \end{array}
 \right)
 $$
:::
 
* Las variables del sistema anterior son las siguientes $x_5$, $x_4$, $x_3$, $x_2$ y $x_1$.
 
* Paso 3. El máximo valor de la submatriz del sistema formada por las 3 últimas filas y columnas vale $3.1$ que corresponde a la fila 3 y a la columna 3. Entonces no realizamos ninguna permutación y aplicamos el algoritmo de Gauss obteniendo la matriz siguiente:
 
::: recuadro-gris
 $$
 A^{(3)}\left(
 \begin{array}{rrrrr|r}
  5 & 0 & 1 & 3 & 1 & 1\\
  0 & 4 & 0.2 & 1.6 & 0.2 & 3.2\\
  0 & 0 & 3.1 & -1.2 & 1.1 & 0.6\\
  0 & 0 & 0 & 0.74194 & -0.096774 & 1.1290\\
  0 & 0 & 0 & 1.6129 & 2.3548 & 3.6935\\
 \end{array}
 \right)
 $$
:::

* Paso 4. El máximo valor de la submatriz del sistema formada por las 2 últimas filas y columnas vale $2.3548$ que corresponde a la fila 5 y a la columna 5. Entonces permutamos la fila 4 con la 5 y a continuación la columna 4 con la 5
 y aplicamos el algoritmo de Gauss obteniendo la matriz siguiente: 

::: recuadro-gris
$$
A^{(4)}\left(\begin{array}{rrrrr|r}
   5 & 0 & 1 & 1 & 3 & 1\\
   0 & 4 & 0.2 & 0.2 & 1.6 & 3.2\\
   0 & 0 & 3.1 & 1.1 & -1.2 & 0.6\\
   0 & 0 & 0 & 2.3548 & 1.6129 & 3.6935\\
   0 & 0 & 0 & 0 & 0.80822 & 1.2808
\end{array}\right)
$$
:::
* Las variables del sistema anterior son las siguientes $x_5$, $x_4$, $x_3$, $x_1$ y $x_2$.

* La solución del sistema será usando el método de sustitución hacia atrás:

::: recuadro-gris
$$
\begin{align*}
x_2 & = \dfrac{1.2808}{0.80822} \approx 1.5847 \\
x_1 & = \dfrac{3.6935 - 1.6129(1.5847)}{2.3548} \approx 0.48306 \\
x_3 & = \dfrac{0.6 - (1.1)(0.48306) +1.2 (1.5847)}{3.1} \approx 0.63555 \\
x_4 & = \dfrac{3.2 - 0.2(0.63555) - 0.2(0.48306) - 1.6(1.5847)}{4.0} \approx 0.11020 \\
x_5 & = \dfrac{1 - 0(0.11020) - 1(0.63555) - 1(0.48306) - 3(1.5847)}{0.5} \approx -0.97456
\end{align*}
$$
:::
:::


### Conteo de operaciones
* El número total de operaciones (sumas, restas, multiplicaciones y divisiones) necesarias para resolver un sistema de $n$ ecuaciones con $n$ incógnitas mediante el método de eliminación gaussiana es aproximadamente:

::: recuadro-gris
$(+/-)$
$$
\sum_{k=1}^{n-1} ((n-k)^2 + (n-k)) = \dfrac{1}{3}n^3 - \dfrac{1}{3}n \approx \mathcal{O}(n^3)
$$
$(×/÷)$
$$
\sum_{k=1}^{n-1} (2(n-k) + (n-k)^2) = \dfrac{2n^3+3n^2-5n}{6} \approx \mathcal{O}(n^3)
$$
::: 

### Número de condición
* ¿Cómo se evalua el buen o mal condicionamiento de la matriz de coeficientes?
* El número de condioción de una matriz permite cuantificar su nivel de condicionamiento.
* El número de condición de una matriz $A$ se define como:

::: recuadro-gris
$$\kappa(A) = ||A|| \cdot ||A^{-1}||
$$
:::

* Donde $||\cdot||$ denota una norma matricial.
* Si $\kappa(A)$ es cercano a 1, la matriz está bien condicionada.
* Si $\kappa(A)$ es grande, la matriz está mal condicionada.

::: caja-ejemplo
#### Ejemplo 5:
Calcular el número de condición de la siguiente matriz utilizando la norma 2:
$$A = \begin{pmatrix}
2 & 3.01 \\
4 & 6
\end{pmatrix}$$

```{python}
import numpy as np
A = np.array([[2, 3.01],
              [4, 6]], dtype=float)
def numero_de_condicion(A):
    norma_A = np.linalg.norm(A, 2)
    norma_A_inv = np.linalg.norm(np.linalg.inv(A), 2)
    kappa = norma_A * norma_A_inv
    return kappa
kappa_A = numero_de_condicion(A)
print("Número de condición de A:", kappa_A)
# Alternativamente, usando la función cond de NumPy
print(np.linalg.cond(A, 2))
```
:::

* Una cota válida para el nuúmero de condición es:

::: recuadro-gris
$$\kappa(A) \geq 1 $$
::: 

* Si $\kappa(A)$ es muy grande, pequeños errores en los datos de entrada pueden provocar grandes errores en la solución del sistema.

## Descomposición de Matrices

* Al aplicar el método de Gauss al sistema $Ax = b$ se realizan transformaciones elementales para triangularizar la matriz del sistema.
* Si este proceso puede realizarse sin intercambios de filas, la matriz triangular superior $U$ obtenida viene determinada por el producto de un número finito de transformaciones filas
$N_{n−1}N_{n−2}\cdots N_2N_1$ aplicadas a la matriz $A$. Llamando $L^{−1} = N_{n−1}N_{n−2}\cdots N_2N_1$ se tiene que

::: recuadro-gris
$$
L^{−1}A = U \Rightarrow A = LU.
$$
:::

* Además $L$ es una matriz triangular inferior con unos en la diagonal.
* Entonces para resolver el sistema $Ax = b$, sabiendo que $A$ posee descomposición $LU$, debemos hacer lo siguiente:
  - $Ax = b \Rightarrow LUx = b$, llamando $z$ al producto $Ux$ tenemos
que:
  - $Lz = b \Rightarrow$ sustitución progresiva.
  - $Ux = z \Rightarrow$ sustitución regresiva. 
  
Si $A$ es estrictamente diagonal dominante entonces $A$ admite una descomposición $LU$ que se obtiene mediante el proceso de eliminación Gaussiana.

::: recuadro-gris
Se dice que una matriz $A$ es diagonalmente dominante si
$$
|a_{ii}| > \sum_{\substack{j=1 \\ j \ne i}}^{n} |a_{ij}| \quad \text{para } i = 1, 2, \ldots, n
$$
:::

::: cuadro-alg
**FUNCION Factorizacion_LU_Doolittle(A):**
    # Asume que A es una matriz cuadrada (n x n)
    n = dimension de A (número de filas)
    L = Matriz Identidad n x n
    U = Matriz n x n (copia de A, que será modificada)

    # El algoritmo se ejecuta en A y sus elementos modificados forman L y U.
    # Por simplicidad, calculamos U in-place en A y L en una matriz separada.

    PARA k = 1 HASTA n:

        # 1. Cálculo de U_{k,k} (Elemento Pivote de la fila k)
        #    u_{k,k} = a_{k,k} - SUM_{p=1}^{k-1} l_{k,p} * u_{p,k}
        
        # El elemento pivote u_kk ya está en A[k,k] si no hay pivoteo parcial.

        # 2. CÁLCULO DE LA FILA k DE U: (j = k+1 hasta n)
        PARA j = k+1 HASTA n:
            # u_{k,j} = a_{k,j} - SUM_{p=1}^{k-1} l_{k,p} * u_{p,j}
            suma_U = 0
            PARA p = 1 HASTA k-1:
                suma_U = suma_U + L[k, p] * U[p, j] 
            FIN PARA
            U[k, j] = A[k, j] - suma_U  # Guardamos el resultado en U
        FIN PARA

        # 3. CÁLCULO DE LA COLUMNA k DE L: (i = k+1 hasta n)
        # L_{i,k} = (1 / U_{k,k}) * [ a_{i,k} - SUM_{p=1}^{k-1} l_{i,p} * u_{p,k} ]
        SI U[k, k] es CASI CERO ENTONCES
            RETORNAR "Falla: Pivoteo necesario o matriz singular."
        FIN SI

        factor = 1.0 / U[k, k]

        PARA i = k+1 HASTA n:
            # l_{i,k}
            suma_L = 0
            PARA p = 1 HASTA k-1:
                suma_L = suma_L + L[i, p] * U[p, k]
            FIN PARA
            
            L[i, k] = factor * (A[i, k] - suma_L) 
        FIN PARA

    FIN PARA
    
    RETORNAR L, U
**FIN FUNCION**
:::

::: caja-ejemplo
#### Ejemplo 6:
Realizar la descomposición LU de la siguiente matriz:
$$A = \begin{pmatrix}
1 & 2 & 0 \\
-1 & 1 & 3 \\
2 & 1 & -1
\end{pmatrix}$$
```{python}
import numpy as np
A = np.array([[1, 2, 0],
              [-1, 1, 3],
              [2, 1, -1]], dtype=float)
def factorizacion_LU_doolittle(A):
    n = A.shape[0]
    L = np.eye(n)
    U = np.zeros((n, n))
    for k in range(n):
        suma_U = sum(L[k, p] * U[p, k] for p in range(k))
        U[k, k] = A[k, k] - suma_U
        for j in range(k+1, n):
            suma_U = sum(L[k, p] * U[p, j] for p in range(k))
            U[k, j] = A[k, j] - suma_U
        factor = 1.0 / U[k, k]
        for i in range(k+1, n):
            suma_L = sum(L[i, p] * U[p, k] for p in range(k))
            L[i, k] = factor * (A[i, k] - suma_L)
    return L, U
L, U = factorizacion_LU_doolittle(A)
print("Matriz L:")
print(L)
print("Matriz U:")
print(U)
print("Verificación LU:")
print(np.dot(L, U))
```
:::

### Factorización LU con Pivoteo
* En la práctica, para garantizar la estabilidad numérica del proceso de eliminación Gaussiana, es necesario incorporar el pivoteo parcial en la descomposición LU.
* Esto implica que durante el proceso de eliminación, se selecciona el pivote como el elemento de mayor valor absoluto en la columna actual, y se intercambian las filas correspondientes.

Definición de matriz de permutación. Diremos que una matriz $P_{ij}=(p_{kl})\, k,l=1,\ldots,n$, $n\times n$, es una **matriz de permutación** si

::: recuadro-gris
$$
p_{kl} = \begin{cases}
1, & \text{si }k=l\neq i,j,\\
1, & \text{si }k=i,l= j,\\
1 & \text{si }k=j, l=i,\\
0 & \text{en caso contrario}
\end{cases}
$$
:::

* Al utilizar eliminación Gaussiana con pivoteo en cada paso se introduce una matriz de permutación (que puede ser la identidad) de modo que:

::: recuadro-gris
$$
N_{n-1}P_{n-1}N_{n-2}P_{n-2}\cdots N_2P_2N_1P_1A = U 
$$

:::

* Donde $P_i$ es la matriz de permutación que intercambia las filas correspondientes en el paso $i$.
* Llamando $P = P_1P_2\cdots P_{n-1}$ y $L^{-1} = N_{n-1}P_{n-1}N_{n-2}P_{n-2}\cdots N_2P_2N_1P_1$ se tiene que:

::: recuadro-gris
$$  
L^{-1}A = U \Rightarrow PA = PP^{-1}LU \Rightarrow PA = LU
$$
:::

* Por lo tanto para resolver el sistema $Ax = b$ premultiplicando por $P$, queda $PAx = Pb$, y dado que $PA = LU$, entonces:
  - $LUx = Pb$, llamando $z$ al producto $Ux$ tenemos que:
  - $Lz = Pb \Rightarrow$ sustitución progresiva.
  - $Ux = z \Rightarrow$ sustitución regresiva.

::: caja-ejemplo
#### Ejemplo 7:
Realizar la descomposición $PA=LU$ con pivoteo de la siguiente matriz:
$$
A = \begin{pmatrix}
0 & 2 & 0 \\
-1 & 1 & 3 \\
2 & 1 & -1
\end{pmatrix}
$$

::: recuadro-gris
$$
P_1 = \begin{pmatrix}
0 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 0
\end{pmatrix} \qquad P_1A = \begin{pmatrix}
2 & 1 & -1 \\
-1 & 1 & 3 \\
0 & 2 & 0
\end{pmatrix}
$$

$$
N_1= \begin{pmatrix}
1 & 0 & 0 \\
0.5 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix} \qquad N_1P_1A = \begin{pmatrix}
2 & 1 & -1 \\
0 & 1.5 & 2.5 \\
0 & 2 & 0
\end{pmatrix}
$$

$$
P_2 = \begin{pmatrix}
1 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0 
\end{pmatrix} \qquad P_2N_1P_1A = \begin{pmatrix}
2 & 1 & -1 \\
0 & 2 & 0 \\
0 & 1.5 & 2.5
\end{pmatrix}
$$

$$
N_2 = \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & -0.75 & 1
\end{pmatrix} \qquad N_2P_2N_1P_1A = \begin{pmatrix}
2 & 1 & -1 \\
0 & 2 & 0 \\
0 & 0 & 2.5
\end{pmatrix} = U
$$

$$
L^{-1} = N_2P_2N_1P_1 = \begin{pmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
-0.75 & 1 & 0.5
\end{pmatrix} \Rightarrow L = \begin{pmatrix}
0 & 1 & 0 \\
-0.5 & 0.75 & 1 \\
1 & 0 & 0
\end{pmatrix}
$$

$$
PA = LU \quad \text{con} \quad P = P_2P_1 = \begin{pmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{pmatrix} \qquad 
$$

$$
LU = \begin{pmatrix}
2 & 1 & -1 \\
0 & 2 & 0 \\
-1 & 1 & 3
\end{pmatrix} = PA
$$

:::

:::

### Factorización de Cholesky
* La factorización de Cholesky es un método de descomposición de matrices que permite expresar una **matriz simétrica** y **definida positiva** como el producto de una matriz triangular inferior y su transpuesta.
* Este método es especialmente útil en la resolución de sistemas de ecuaciones lineales, optimización y simulaciones en estadística y finanzas, facilitando cálculos en diversos campos como la estadística y la ingeniería.

**Proposición (Criterio de Sylvester de matriz definida positiva)**

Sea $A$ una matriz real simétrica $n \times n$. Dicha matriz es definida positiva si y sólo si todos sus menores principales son positivos, es decir,  para todo $k=1,\ldots,n$, $\det(A_k)>0$, donde $A_k$ recordemos que es la submatriz formada por las $k$ primeras filas y columnas de la matriz $A$.

Partiendo de que $A = LU$ y $A$ es simétrica tenemos que:

::: recuadro-gris
$$
\begin{align*}
LU = A = A^T = (LU)^T = U^TL^T & \Rightarrow LU = U^TL^T\\
LU(L^T)^{−1} = U^TL^T(L^T)^{−1} & \Rightarrow LU(L^T)^{−1} = U^T\\
L^{−1}LU(L^T)^{−1} = L^{−1}U^T & \Rightarrow U(L^T)^{−1} = L^{−1}U^T\\
U(L^T)^{−1} = D \Rightarrow U(L^T)^{−1}L^T = DL^T & \Rightarrow U = DL^T \Rightarrow A = LDL^T
\end{align*}
$$
:::

* Donde $D$ es una matriz diagonal.
* Si $A$ es definida positiva, los elementos diagonales de $D$ son positivos, por lo que podemos escribir $D = D^{1/2}D^{1/2}$.
* Entonces:

::: recuadro-gris
$$
A = LDL^T = (LD^{1/2})(D^{1/2}L^T) = LL^T
$$
:::

* Donde $L = LD^{1/2}$ es una matriz triangular inferior con elementos positivos en la diagonal.
* Por tanto, la factorización de Cholesky de una matriz simétrica definida positiva $A$ es $A = LL^T$.
* La factorización de Cholesky se puede calcular mediante el siguiente algoritmo:

::: cuadro-alg
**FUNCION Factorizacion_Cholesky(A):**
    # Asume que A es una matriz simétrica definida positiva (n x n)
    n = dimension de A (número de filas)
    L = Matriz n x n (inicializada a ceros)

    PARA i = 1 HASTA n:
        # Cálculo de los elementos de la fila i de L
        PARA j = 1 HASTA i:
            suma = 0
            PARA k = 1 HASTA j-1:
                suma = suma + L[i, k] * L[j, k]
            FIN PARA
            
            SI i == j ENTONCES
                L[i, j] = sqrt(A[i, i] - suma)  # Elemento diagonal
            SINO
                L[i, j] = (1 / L[j, j]) * (A[i, j] - suma)  # Elemento fuera de la diagonal
            FIN SI
        FIN PARA
    FIN PARA
    
    RETORNAR L
**FIN FUNCION**
:::

::: caja-ejemplo
#### Ejemplo 8
Determine si la siguiente matriz es simétrica y postiva definida
$$
A=\begin{pmatrix}
5 & 2 & -2 & -1 & -1\\
2 & 7 & 1 & 1 & 6\\
-2 & 1 & 9 & 2 & 1\\
-1 & 1 & 2 & 11 & -1\\
-1 & 6 & 1 & -1 & 13
\end{pmatrix}
$$

``` {python, eval=TRUE}
import numpy as np

def is_square(A):
    A = np.asarray(A)
    return A.ndim == 2 and A.shape[0] == A.shape[1]

def is_symmetric_by_transpose(A, tol=1e-8):
    """
    Comprueba simetría comparando A y A.T con tolerancia absoluta.
    """
    A = np.asarray(A, dtype=float)
    if not is_square(A):
        return False
    return np.allclose(A, A.T, atol=tol, rtol=0)

def is_positive_definite_by_principal_minors(A, tol=1e-12):
    """
    Comprueba que todos los menores principales (determinantes de A[:k,:k], k=1..n)
    son positivos. Esto es una condición necesaria y suficiente para simetría
    y positividad definida en matrices reales simétricas.
    """
    A = np.asarray(A, dtype=float)
    if not is_symmetric_by_transpose(A):
        return False
    n = A.shape[0]
    for k in range(1, n+1):
        minor = A[:k, :k]
        det = np.linalg.det(minor)
        if det <= tol:
            return False
    return True

def check_matrix_simple(A, tol_sym=1e-8, tol_minor=1e-12):
    A = np.asarray(A, dtype=float)
    square = is_square(A)
    symmetric = square and is_symmetric_by_transpose(A, tol=tol_sym)
    pd = False
    if symmetric:
        pd = is_positive_definite_by_principal_minors(A, tol=tol_minor)
    return {
        "square": square,
        "symmetric": symmetric,
        "positive_definite": pd
    }

# Ejemplo: usar la matriz A del enunciado (5x5) y calcular la factorización de Cholesky
if __name__ == "__main__":
    A = np.array([[5, 2, -2, -1, -1],
                  [2, 7, 1, 1, 6],
                  [-2, 1, 9, 2, 1],
                  [-1, 1, 2, 11, -1],
                  [-1, 6, 1, -1, 13]], dtype=float)

    print("Matriz A:")
    print(A)

    res = check_matrix_simple(A)
    print("square:", res["square"]) 
    print("symmetric:", res["symmetric"]) 
    print("positive_definite (por menores principales):", res["positive_definite"]) 

    # Mostrar determinantes de los menores principales para diagnóstico
    if res["symmetric"]:
        print("Determinantes de los menores principales:")
        for k in range(1, A.shape[0] + 1):
            det = np.linalg.det(A[:k, :k])
            print(f"det(A[:{k},:{k}]) = {det:.8f}")

    # Implementación explícita del algoritmo de Cholesky (tal como aparece en el Rmd)
    def factorizacion_cholesky(A, tol=1e-12):
        A = np.asarray(A, dtype=float)
        if A.ndim != 2 or A.shape[0] != A.shape[1]:
            raise ValueError("La matriz debe ser cuadrada")
        n = A.shape[0]
        L = np.zeros((n, n), dtype=float)
        for i in range(n):
            for j in range(i+1):
                suma = 0.0
                for k in range(j):
                    suma += L[i, k] * L[j, k]
                if i == j:
                    val = A[i, i] - suma
                    if val <= tol:
                        # No es definida positiva (o es numéricamente no PD)
                        raise np.linalg.LinAlgError(f"No es definida positiva: diagonal negativa en i={i}, valor={val}")
                    L[i, j] = np.sqrt(val)
                else:
                    L[i, j] = (A[i, j] - suma) / L[j, j]
        return L

    # Intentar la factorización y mostrar resultados
    try:
        L = factorizacion_cholesky(A)
        print("\nFactor L (Cholesky):")
        print(L)
        recon = L.dot(L.T)
        print("\nReconstrucción L @ L.T:")
        print(recon)
        err = np.max(np.abs(A - recon))
        print(f"\nMáximo error |A - LL^T| = {err:.3e}")
    except Exception as e:
        print("Error al calcular Cholesky:", e)
```
:::

* Al ser la factorización de Cholesky una factorización $LU$, para resolver el sistema lineal $Ax=b$  y al ser $A=LL^T$, se sigue el siguiente procedimiento:
    - $Ax=b \Rightarrow LL^Tx=b \Rightarrow L(L^Tx)=b$, llamando $z=L^Tx$, se resuelve $Lz=b$ aplicando sustitución progresiva.
    - Hallado los valores de $z$, se devuelve el cambio planteado resolviendo $L^Tx=z$ aplicando sustitución regresiva.

## Métodos Iterativos
Aqui va la parte de Iterativos

------------------------------------------------------------------------
  
<small>Última revisión: `r format(Sys.Date(), "%d de %B, %Y")`</small>
