---
title: "Solución Numérica de Ecuaciones No Lineales"
author: "José Luis Ramírez"
date: "Noviembre 2025"
output:
  html_document:
    self_contained: true
    mathjax: default
    pandoc_args: "--mathjax"
    toc: true
    toc_depth: 4
    toc_float: true
    theme: readable
    css: css/estilos.css
    number_sections: true
---

# Motivación

* La determinación de las raíces de una ecuación o de un sistema de ecuaciones, es uno de los problemas más antiguos de aproximación numérica que se presenta con frecuencia en la solución de una gran variedad de problemas en la matemática aplicada.

* En muchos problemas de ingeniería, inteligencia artificial u otras disciplinas afines tenemos que resolver ecuaciones del tipo $f(x)=0$, donde $f$ es una función que dada una cantidad $x$, nos devuelve $f(x)$.

::: caja-ejemplo
#### Ejemplo 1

Se considera por ejemplo la siguiente ecuación obtenida a partir de la segunda ley de Newton para calcular la velocidad de un paracaidista,
$$
v=\dfrac{g \cdot m}{c}\left(1-e^{\frac{c}{m}t}\right)
$$
donde la velocidad $v$ depende de la variable independiente tiempo, $t$, $g$ es la constante de gravitación, $c$ el coeficiente de resistencia y $m$ es la masa del paracaidista.

Si se quisiera obtener el coeficiente de resistencia del paracaidista con una masa dada, ¿cómo se obtendría este valor para alcanzar una velocidad determinada en un periodo establecido? La respuesta se obtendría calculando $c$ que hace cero a la siguiente función:
$$
f(c) = \dfrac{g \cdot m}{c}\left(1-e^{\frac{c}{m}t}\right)-v
$$

esto es, habría que calcular el valor de $c$ tal que $f(c)=0$.

En este caso no es posible encontrar analíticamente el cero de la función o su cálculo es complicado.
:::

* En dichos problemas, conocer explícitamente la función $f$ en muchos casos no es posible, sólo tenemos un algoritmo que dado un valor $x$, nos devuelve $f(x)$.

* Entonces, dependiendo de nuestro conocimiento de la función $f$, podremos aplicar un método numérico u otro.

* Todos los métodos numéricos que hallan aproximaciones de ceros construyen una **sucesión** $\{x_n\}_n$ que queremos que converja hacia el cero $\hat{x}$ de la función $f$.

* Cuanto mayor sea la **velocidad de convergencia** de la sucesión $\{x_n\}_n$, mejor será el método usado.

## Método Gráfico

El método gráfico es un método muy simple, consiste en calcular valores de la variable dependiente para distintos valores de la variable independiente, para luego observar el punto de intersección de la función con el eje de las abscisas. Este punto proporciona una primera aproximación a la raíz de la ecuación.

```{python, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt

# 1. Se define la función a graficar
def f(x):
  return x**2 - 4 * np.sin(x)

# 2. Se crea un conjunto de puntos en el intervalo [-2, 3]
x = np.linspace(-2, 3, 500)

# 3. Se evalúa la función en cada punto
y = f(x)

# 4. Se procede a la graficación
plt.figure(figsize=(10, 6))
plt.plot(x, y, label='f(x) = $x^2 - 4sin(x)$', color='blue')

# Se añade una línea en y=0 para identificar visualmente las raíces
plt.axhline(0, color='red', linestyle='--', linewidth=0.8)

# Se agregan detalles al gráfico para una mejor interpretación
plt.title('Gráfica de la función $f(x) = x^2 - 4sin(x)$')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.legend()

# 5. Se muestra el gráfico
plt.show()
```

# Velocidad de convergencia

Se distinguen distintas velocidades de convergencia que se pasarán a definir.

::: caja-lema
**Definición:**

Dada una sucesión de números reales, $\left\{x _k\right\}_{k = 1}^{\infty}$, convergente hacia un punto $\overline x$, se dice que su orden o velocidad de convergencia es $p$, con $p \ge 1$ si existe una constante $C_p \ge 0$ tal que: 

::: recuadro-gris
$$\left| {{x_{k + 1}} - \overline x }
\right| \le {C_p}{\left| {{x_k} - \overline x \,} \right|^p}
$$
:::

para todo $k \ge k_0$ siendo $k_0$ entero.
:::

En particular,

* Si $p=1$ y $C_p< 1$, la convergencia es **lineal**. Si $C_p \ge 1$, la convergencia es **sublineal**.

* Si $p=2$, la convergencia es **cuadrática**.

* Si $p=3$, la convergencia es **cúbica**.

::: caja-ejemplo
#### Ejemplo 2
Se considera la sucesión ${x_k} = \frac{1}{6} - \frac{1}{3}{\left(-\frac{1}{2}\right)^k}$ que converge a $\frac{1}{6}$. Se ve que la convergencia es lineal.

Se puede comprobar que:
$$
\left|x_{k + 1} - \frac{1}{6} \right| \le \frac{1}{2}\left|x_k - \frac{1}{6} \right|
$$
ya que 
$$
\left|x_{k + 1} - \frac{1}{6}\right| = \frac{1}{3 \cdot 2^{k + 1}} \qquad \left|x_k - \frac{1}{6} \right| = \frac{1}{3 \cdot 2^k}
$$
:::

::: caja-ejemplo
#### Ejemplo 3
Suponga que $C_p=0.5$ y que $\left| x_k - \overline x \right| < 0.01$. Considerando $p=1$ y $p=2$, analiza cómo se aproxima al valor límite, que se denotará por $\overline x$, considerando el término $k+1$ y $k+2$ de la sucesión $\{x_k\}$.

Se tiene que

* Si $p=1$,
$$
\begin{align*}
\left| x_{k + 1} - \overline x \right| &< C_p\left| x_{k} - \overline x \right| < 5 \cdot 10^{-1} \cdot 10^{-2} = 5 \cdot 10^{-3}\\
\left| x_{k + 2} - \overline x \right| &< C_p \left| x_{k + 1} - \overline x \right| <5 \cdot 10^{-1} \cdot (5 \cdot 10^{-3}) =25 \cdot 10^{-4}
\end{align*}
$$

* Si $p=2$,
$$
\begin{align*}
\left| x_{k + 1} - \overline x \right| & < C_p\left| x_{k} - \overline x \right|^2 < 5 \cdot 10^{-1} \cdot (10^{-2})^2 =5 \cdot 10^{-5}\\
\left| x_{k + 2} - \overline x \right| & < C_p \left| x_{k + 1} - \overline x \right|^2 < 5 \cdot 10^{-1} \cdot (5\cdot 10^{-5})^2=125 \cdot 10^{-11}
\end{align*}
$$
:::

# Método de Bisección

::: caja-lema
**Teorma del Valor Intermedio de Bolzano**

Supongamos que $f \in \mathcal{C}[a, b]$ y que $L$ es cualquier número entre $f(a)$ y $f(b)$. Entonces existe un número $c$ en $(a, b)$ tal que
$f(c) = L$.
:::

* Supongamos que $f$ es una función continua en un intervalo $[a, b]$, y $f(a) \cdot f(b) < 0$. Entonces, por el **Teorema de Bolzano**, existe al menos un $p \in (a, b)$, tal que $f(p) = 0$.

* El método, también llamado **método de intervalos encajados**, va construyendo una sucesión de intervalos encajados:
$$
[a_0,b_0] \supset[a_1,b_1]\supset\cdots\supset[a_n,b_n]\supset\cdots
$$
tal que el cero $x$ siempre está en todos los intervalos $[a_n,b_n]$ y la longitud de cada intervalo $[a_n,b_n]$ vale $\frac{b_0-a_0}{2^n}$. De esta manera el cero x se calcula de forma más precisa.

* Una primera aproximación de este punto $p$ puede ser el punto medio:

::: recuadro-gris
$$
p_1 = \dfrac{a_0+b_0}{2}
$$
:::

* Dado que la función es continua, si $f(a) \cdot f(p_1) < 0$ en el intervalo $[a, p_1]$ habrá al menos una solución de la ecuación.

* Y si $f(a) \cdot f(p_1) > 0$ en el intervalo $[p_1, b]$ existirá al menos una raíz.

* Por tanto se habrá definido un nuevo intervalo $[a_1, b_1]$ en el que existirá una solución. Al que puede aplicársele nuevamente el proceso anterior.

A continuación, se ilustra gráficamente el funcionamiento del método de bisección para la función $f(x) = x^2 - 4\sin(x)$ en el intervalo inicial $[1, 3]$.

```{python, echo=FALSE}
import numpy as np
import matplotlib.pyplot as plt

# 1. Definir la función y el intervalo inicial
def f(x):
    return x**2 - 4 * np.sin(x)

a, b = 1.0, 3.0
iteraciones = 4
historial = []

# 2. Ejecutar el método de bisección y guardar el historial
for i in range(iteraciones):
    m = (a + b) / 2
    fa, fb, fm = f(a), f(b), f(m)
    
    # Guardar los puntos de la iteración actual
    historial.append({'a': a, 'b': b, 'm': m, 'fa': fa, 'fb': fb, 'fm': fm})
    
    # Actualizar el intervalo
    if np.sign(fa) != np.sign(fm):
        b = m
    else:
        a = m

# 3. Preparar la gráfica
plt.figure(figsize=(12, 8))
x_vals = np.linspace(0, 3.5, 500)
y_vals = f(x_vals)

# Graficar la función y el eje x
plt.plot(x_vals, y_vals, label='$f(x) = x^2 - 4sin(x)$', color='black', zorder=1)
plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)

# Denotar a_0 y b_0 en la gráfica
initial_a, initial_b = historial[0]['a'], historial[0]['b']
plt.text(initial_a, 0.5, '$a_0$', ha='center', va='bottom', fontsize=12, color='black')
plt.text(initial_b, 0.5, '$b_0$', ha='center', va='bottom', fontsize=12, color='black')

# 4. Graficar cada iteración del método
for i, data in enumerate(historial):
    color = plt.cm.get_cmap('tab10')(i)
    a_i, b_i, m_i = data['a'], data['b'], data['m']
    
    # Dibuja el intervalo [a, b] en el eje x
    plt.plot([a_i, b_i], [0, 0], '|', markersize=10, color=color, label=f'Intervalo {i+1}: [{a_i:.2f}, {b_i:.2f}]')
    plt.plot(m_i, 0, 'x', color=color, markersize=8, label=f'Punto medio {i+1}: {m_i:.2f}')
    
    # Añade la etiqueta p_i debajo de cada punto medio
    plt.text(m_i, -0.5, f'$p_{i+1}$', ha='center', va='top', color=color, fontsize=12)

# 5. Añadir detalles al gráfico
plt.title('Visualización del Método de Bisección')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True, linestyle=':', alpha=0.6)
plt.tight_layout()
plt.show()
```

::: cuadro-alg
**FUNCION BISECCION (f,a,b,TOL,N_MAX)**

    # f: la función para la cual se busca la raíz
    # a, b: los extremos del intervalo inicial [a, b]
    # TOL: la tolerancia para el criterio de parada
    # N_MAX: el número máximo de iteraciones

    # Paso 1: Verificar la condición inicial (f(a) y f(b) deben tener signos opuestos)
    SI f(a) * f(b) >= 0 ENTONCES
        IMPRIMIR "El método de bisección puede no funcionar. f(a) y f(b) deben tener signos opuestos."
        RETORNAR NULO
    FIN SI

    # Paso 2: Inicializar el contador de iteraciones
      iteracion = 0

    # Paso 3: Bucle principal de iteraciones
    MIENTRAS iteracion < N_MAX:
      # Calcular el punto medio del intervalo
      p = (a + b) / 2

      # Paso 4: Evaluar el criterio de parada (si f(p) es suficientemente cercano a cero)
      SI ABS(f(p)) < TOL ENTONCES
        IMPRIMIR "Raíz encontrada en p =", p, "en", iteracion, "iteraciones."
        RETORNAR p
      FIN SI

      # Paso 5: Actualizar el intervalo
      SI f(a) * f(p) < 0 ENTONCES
        # La raíz está en el subintervalo [a, p]
        b = p
      SINO
        # La raíz está en el subintervalo [p, b]
        a = p
      FIN SI

      # Paso 6: Incrementar el contador de iteraciones
      iteracion = iteracion + 1

      # Opcional: Criterio de parada basado en la longitud del intervalo
      SI (b - a) / 2 < TOL ENTONCES
        p = (a + b) / 2
        IMPRIMIR "Intervalo suficientemente pequeño. Raíz aproximada en p =", p, "en", iteracion, "iteraciones."
        RETORNAR p
      FIN SI
    FIN MIENTRAS

    # Paso 7: Si se alcanza el número máximo de iteraciones sin encontrar la raíz
    IMPRIMIR "Número máximo de iteraciones alcanzado. El método puede no haber convergido a la tolerancia deseada."
    RETORNAR (a + b) / 2 # Devolver la última aproximación
**FIN FUNCION**

:::

::: caja-ejemplo
#### Ejemplo 4: Bisección con seguimiento en tabla

A continuación, se aplica el método de bisección para encontrar una raíz de la función $f(x) = x^4 - 2x^3 - 4x^2 + 4x + 4$ en el intervalo $[-2, -1]$. La tabla muestra el progreso del método en cada iteración.

Verificando la hipótesis inicial del Método de Bisección
$$
f(-2) = (-2)^4-2(-2)^3-4(-2)^2+4(-2)+4 = 16+16-16-8+4 =12
$$
$$
f(-1) = (-1)^4-2(-1)^3-4(-1)^2+4(-1)+4 = 1+2-4-4+4 =-1
$$

Se observa que $f(-2)\cdot f(-1)<0$, por lo tanto el Teorema de Bolzano asegura la existencia de un valor $p\in(-2,-1)$ tal que $f(p)=0$

``` {python, echo=FALSE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 1. Definir la función
def f(x):
    return x**4 - 2*x**3 - 4*x**2 + 4*x + 4

# 2. Configuración inicial
a = -2.0
b = -1.0
TOL = 1e-5
nmax = 20

# Lista para almacenar los resultados de cada iteración
historial_tabla = []

# 3. Implementación del método de Bisección
k = 0
while k < nmax:
    p = a + (b - a) / 2
    fp = f(p)    
    error = (b - a) / 2
    error2 = 0
    if k > 0:
      error2 = np.abs(p-pold)
    pold = p
    
    
    # Almacenar los datos de la iteración actual
    historial_tabla.append([k + 1, a, b, p, np.abs(fp), error, error2])
    
    # Criterio de parada
    if error < TOL:
        print(f"Solución encontrada después de {k+1} iteraciones.")
        break
        
    # Actualizar el intervalo
    if f(a) * fp < 0:
        b = p
    else:
        a = p
        
    k += 1

# 4. Crear y mostrar la tabla con pandas
columnas = ['k', 'a', 'b', 'p_k', '|f(p_k)|', '(b-a)/2', '|p - pant|']
df = pd.DataFrame(historial_tabla, columns=columnas)
formatters = {
    'a': '{:.8f}'.format,
    'b': '{:.8f}'.format,
    'p_k': '{:.8f}'.format,
    '|f(p_k)|': '{:.4e}'.format,
    '(b-a)/2': '{:.4e}'.format,
    '|p - pant|': '{:.4e}'.format
}

print("Tabla de iteraciones del Método de Bisección:")
print(df.to_string(index=False, formatters=formatters))

# 5. Graficar la convergencia del error
plt.figure(figsize=(10, 6))
plt.plot(df['k'], df['(b-a)/2'], marker='o', linestyle='-', color='c', label='Error de intervalo |(b-a)/2|')
plt.plot(df['k'], df['|f(p_k)|'], marker='x', linestyle='--', color='m', label='Error de función |f(p_k)|')
plt.title('Convergencia del Error en el Método de Bisección')
plt.xlabel('Iteración (k)')
plt.ylabel('Valor del Error (escala logarítmica)')
plt.yscale('log')  # Usar escala logarítmica para ver mejor la convergencia
plt.legend()
plt.grid(True, which="both", ls="--")
plt.xticks(df['k']);
plt.show()
```
:::

* Observaciones:
  - Vemos que el método, es lento. Por ejemplo, para pasar de un error menor que 0.1 a un error menor que 0.01, usando el método de parada $|f(x_n)|<\epsilon$, se necesitan cuatro iteraciones apriximadamente, o sea, se necesitan cuatro iteraciones para “ganar” una cifra significativa en la aproximación del cero.
  - Los dos criterios de parada (dos últimas columnas) son equivalentes en el sentido que el nuevo aproximado $p_k$ se encuentra en el punto del intervalo $[a_{k-1},b_{k-1}]$.
  - Los dos criterios de parada (columna 4 y 6) son equivalentes desde el punto de vista de que se necesitan aproximadamente el mismo número de iteraciones para que se “gane” una cifra significativa en el cero. Además, los valores que se obtienen con los dos criterios son del mismo orden, es decir, que si en la iteración $n$, $|f(x_n)|\approx c_1⋅10^{-k}$, entonces $|x_n-x_{n-1}|\approx c_2⋅10^{-k}$; por ejemplo para $n=13$, $|f(x_{13})|\approx 2.31\cdot 10^{-4}$ y $|x_{13}-x_{12}|\approx 1.22\cdot 10^{-4}$.

# Método de Punto Fijo

* El método del punto fijo consiste en transformar la ecuación $f(x)=0$ en la ecuación $x=g(x)$ mediante operaciones algebraicas “básicas”.

* Entonces el cero de la ecuación $f(x)=0$ se transforma en lo que llamamos un punto fijo de la función $g(x)$:

::: caja-lema
**Definición de Punto Fijo**

Sea $g$ una función real de variable real. Diremos que $\hat{x}$ es un punto fijo de la función $g$ si $g(\hat{x})=\hat{x}$.
:::

**Observación**: Gráficamente, un punto fijo resulta de la intersección de la recta diagonal $y=x$ y de la función $y=g(x)$.

La idea es considerar la sucesión definida de forma recurrente como $x_n=g(x_{n-1})$ y ver bajo qué condiciones la sucesión $\{x_n\}_n$ converge hacia el punto fijo $\hat{x}$ de $g$ que recordemos será el cero de la función $f$ buscado.

::: cuadro-alg
**FUNCIÓN PUNTO_FIJO(g, p0, TOL, N_MAX)**

    # ENTRADA:
    #   g: la función de iteración x = g(x)
    #   p0: la aproximación inicial
    #   TOL: la tolerancia para el criterio de parada
    #   N_MAX: el número máximo de iteraciones

    # SALIDA:
    #   p: la solución aproximada o un mensaje de error

    i = 1
    MIENTRAS i <= N_MAX:
        # Calcular el siguiente término de la sucesión
        p = g(p0)

        # Verificar el criterio de parada (error absoluto)
        SI |p - p0| < TOL ENTONCES
            IMPRIMIR "Solución encontrada en p =", p, "en", i, "iteraciones."
            RETORNAR p
        FIN SI

        # Actualizar para la siguiente iteración
        i = i + 1
        p0 = p
    FIN MIENTRAS

    IMPRIMIR "El método fracasó después de", N_MAX, "iteraciones."

:::

```{python, echo=FALSE, fig.cap="Visualización de convergencia y divergencia en el Método de Punto Fijo.", fig.asp=0.6, out.width="100%"}
import numpy as np
import matplotlib.pyplot as plt
from typing import Callable, List, Tuple

def plot_cobweb(
    g: Callable[[float], float],
    p0: float,
    x_range: Tuple[float, float],
    max_iter: int = 10,
    ax: plt.Axes = None,
    **kwargs
):
    """
    Genera un diagrama de telaraña para visualizar el método de punto fijo.
    """
    if ax is None:
        fig, ax = plt.subplots(figsize=(10, 8))

    # Graficar g(x) y la línea y=x
    x_vals = np.linspace(*x_range, 400)
    ax.plot(x_vals, g(x_vals), label=f'$g(x)$', color=kwargs.get('g_color', 'purple'))
    ax.plot(x_vals, x_vals, label='$y = x$', color='red', linestyle='--')

    # Generar puntos para el diagrama de telaraña
    px, py = [p0], [0]
    p_current = p0
    for i in range(max_iter):
        p_next = g(p_current)
        px.extend([p_current, p_next])
        py.extend([p_next, p_next])
        p_current = p_next

    # Graficar las iteraciones
    ax.plot(px, py, color='orange', linestyle='-', marker='o', markersize=4, label='Iteraciones')
    ax.text(p0, -0.15, '$p_0$', ha='center', va='top', fontsize=12, color='darkorange')

    # Configuración del gráfico
    ax.axhline(0, color='black', linewidth=0.8)
    ax.axvline(0, color='black', linewidth=0.8)
    ax.set_title(kwargs.get('title', 'Diagrama de Telaraña'))
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.legend()
    ax.grid(True, linestyle=':', alpha=0.7)
    if 'ylim' in kwargs:
        ax.set_ylim(kwargs['ylim'])
    ax.set_aspect('equal', adjustable='box')

def g_converge(x: float) -> float:
    """Función g(x) = cos(x), que converge."""
    return np.cos(x)

def g_diverge(x: float) -> float:
    """Función g(x) = 1 - x^3, que diverge para p0=0.8."""
    return 1 - x**3

# --- Ejecución ---
# Crear una figura con dos subplots uno al lado del otro
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

# Gráfico de convergencia
plot_cobweb(g=g_converge, p0=0.1, x_range=(0, 1.5), max_iter=10, ax=ax1, 
            title='Proceso Convergente', g_color='blue', ylim=(-0.2, 1.2))

# Gráfico de divergencia
plot_cobweb(g=g_diverge, p0=0.8, x_range=(-1, 1.5), max_iter=6, ax=ax2,
            title='Proceso Divergente', g_color='purple', ylim=(-1, 1.5))

plt.show()
```

::: caja-ejemplo
#### Ejemplo 5: Punto fijo para $f(x)=0$ de 4 maneras diferentes

Hallar numéricamente una de las raíces de $f(x) = x^2 - x - 2 = 0$. Sus raíces exactas son $x_1 = -1$ y $x_2 = 2$. Hallando únicamente en el cálculo de la raíz $\hat{x} = 2$ tomando como punto de partida $x_0 = 2.1$ y definiendo $g$ como.

* $g_1(x) = x^2-2$

* $g_2(x) = \sqrt{x+2}$

* $g_3(x) = 1+\dfrac{2}{x}$

* $g_4(x) = \dfrac{x^2+2}{2x-1}$

```{python, echo=FALSE}
import numpy as np
import pandas as pd
from itertools import zip_longest

# Definición de las funciones de iteración
def g1(x):
    return x**2 - 2

def g2(x):
    return np.sqrt(x + 2)

def g3(x):
    return 1 + 2/x

def g4(x):
    return (x**2 + 2) / (2*x - 1)

def fixed_point_iterations(g, p0, max_iter):
    """
    Realiza iteraciones de punto fijo y devuelve los resultados en una lista.
    """
    history = [p0]
    p_current = p0
    for i in range(max_iter):
        p_next = g(p_current)
        history.append(p_next)
        p_current = p_next
    return history

# Punto inicial
p0 = 2.1

# Calcular las iteraciones para cada función
iter_g1 = fixed_point_iterations(g1, p0, 10)
iter_g2 = fixed_point_iterations(g2, p0, 8)
iter_g3 = fixed_point_iterations(g3, p0, 15)
iter_g4 = fixed_point_iterations(g4, p0, 3)

# Combinar los resultados en una sola tabla
data = list(zip_longest(iter_g1, iter_g2, iter_g3, iter_g4))
df = pd.DataFrame(data, columns=['g1(x)', 'g2(x)', 'g3(x)', 'g4(x)'])
df.index.name = 'Iters'

# Mostrar la tabla combinada
print("--- Tabla Comparativa de Iteraciones de Punto Fijo ---")
print(df.to_string(
    formatters={
    'g1(x)': '{:.8e}'.format,
    'g2(x)': '{:.8f}'.format,
    'g3(x)': '{:.8f}'.format,
    'g4(x)': '{:.8f}'.format
},
    na_rep='---'
))

```
:::

## Existencia del Punto Fijo

El teorema siguiente nos dice cuáles son las condiciones de **existencia** del punto fijo de una función $g$:

::: caja-lema
**Teorema**

En los resultados teóricos que se enuncian a continuación, se dan condiciones suficientes para que haya un punto fijo de una función $g$ en un intervalo y para que la convergencia sea a un único punto fijo.

* Sea $g$ continua en $[a,b]$ que aplica $[a,b]$ en $[a,b]$, entonces $g$ tiene un punto fijo en $[a,b]$.

* Sea $g$ continua en $[a,b]$ con $g'$ cumpliendo 

::: recuadro-gris
$$
\left| {g'\left( x \right)}\right| \le L < 1 \quad \text{ para todo }x \in \left(a,b\right)
$$
:::

* Si $x_0$ es un punto cualquiera en $(a,b)$ entonces la solución

::: recuadro-gris
$$
{x_{k + 1}} = g\left( {{x_k}} \right)\qquad k \ge 0 
$$
:::

converge al único punto fijo en [a,b].

:::

* **Observación**:
    - Si al utilizar el método del punto fijo $x_{k+1}$ y $x_k$ coinciden dentro de la exactitud especificada $\epsilon$, no significa que $\overline x \approx {x_{k+1}}$ con la misma exactitud. En general, esta afirmación no es correcta. Si $g'(x)$ está próxima a la unidad, entonces la cantidad $\left| {{x_k} - \overline x} \right|$ puede ser grande, aún cuando $\left|{{x_{k+1}} - x_k} \right|$ sea extremadamente pequeña.

::: caja-ejemplo
#### Ejemplo 6

La función $g_2 : [1, 3] \to [1, 3]$ definida por $g_2(x) = \sqrt{x + 2}$ cumple con el Teorema dado en $X = [1, 3]$.

* Primero verificando que $g(x) \in [1, 3]$ si $x \in [1, 3]$:

$$
x \in [1, 3] \Leftrightarrow 1 \leq x \leq 3 \Leftrightarrow 3 \leq x+2 \leq 5 \Leftrightarrow \sqrt{3} \leq \sqrt{x + 2} \leq
\sqrt{5}
$$
Por lo tanto, $g_2(x) = \sqrt{x + 2} \in [1, 3]$.


```{python, echo=FALSE, fig.cap="Análisis gráfico de los puntos fijos para las funciones del Ejemplo 5."}
import numpy as np
import matplotlib.pyplot as plt

# 1. Definir las funciones g(x) del Ejemplo 5
def g1(x): return x**2 - 2
def g2(x): return np.sqrt(x + 2)
def g3(x): return 1 + 2/x
def g4(x): return (x**2 + 2) / (2*x - 1)

# Se definen también sus derivadas para calcular la tangente
def dg1(x): return 2*x
def dg2(x): return 1 / (2 * np.sqrt(x + 2))
def dg3(x): return -2 / x**2
def dg4(x): return (2*x**2 - 2*x - 4) / (2*x - 1)**2

functions = [
    {'g': g1, 'dg': dg1, 'title': '$g_1(x) = x^2-2$'},
    {'g': g2, 'dg': dg2, 'title': '$g_2(x) = \\sqrt{x+2}$'},
    {'g': g3, 'dg': dg3, 'title': '$g_3(x) = 1+2/x$'},
    {'g': g4, 'dg': dg4, 'title': '$g_4(x) = \\frac{x^2+2}{2x-1}$'}
]

# Punto de tangencia
p0 = 2.1

# 2. Configuración del rango y la figura
x = np.linspace(1, 3, 400)
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.ravel()

# 3. Generar cada subgráfica
for i, func_data in enumerate(functions):
    ax = axes[i]
    
    # Graficar g(x) y la recta identidad y=x
    ax.plot(x, func_data['g'](x), label='$g(x)$', color='blue')
    ax.plot(x, x, label='$y=x$', color='red', linestyle='--')
    
    # Calcular y graficar la recta tangente en p0
    g_p0 = func_data['g'](p0)
    dg_p0 = func_data['dg'](p0)
    tangent_line = np.abs(dg_p0) * (x - p0) + g_p0
    ax.plot(x, tangent_line, label=f'Tangente en x={p0}', color='green', linestyle=':')
    ax.plot(p0, g_p0, 'o', color='darkorange') # Marcar el punto de tangencia

    # Configuración de la subgráfica
    a=ax.set_title(func_data['title'])
    a=ax.set_xlabel('x')
    a=ax.set_ylabel('y')
    a=ax.grid(True, which='both', linestyle='--', linewidth=0.5)
    a=ax.legend();
    a=ax.set_ylim(1, 3); # Ajustar el eje y para una mejor visualización cerca de la raíz x=2

plt.suptitle('Intersección de $g(x)$ con la Recta Identidad $y=x$', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()
```

:::


A continuación, se presenta un gráfico dedicado a la convergencia del error para la funciones $g_2(x)$, $g_3(x)$ y $g_4(x)$.
```{python, echo=FALSE, fig.cap="Convergencia del error para $g_3$, $g_4$ y $g_5$ en escala logarítmica."}
import numpy as np
import matplotlib.pyplot as plt

# Definición de las funciones de iteración
def g2(x): return np.sqrt(x + 2)
def g3(x): return 1 + 2.0/x
def g4(x): return (x**2 + 2) / (2*x - 1)

def fixed_point_iterations_safe(g, p0, max_iter):
    """
    Realiza iteraciones de punto fijo manejando errores de dominio/aritmética.
    Devuelve lista de valores (longitud <= max_iter+1). Si ocurre un error
    se rellenan los valores restantes con np.nan.
    """
    history = [p0]
    p_current = p0
    for _ in range(max_iter):
        try:
            p_next = g(p_current)
            # Si p_next no es real o es nan, interrumpir
            if isinstance(p_next, complex) or np.isnan(p_next):
                history.append(np.nan)
                break
            history.append(float(p_next))
            p_current = p_next
        except Exception:
            # En caso de error (dominio, división por cero, etc.) rellenar con nan y salir
            history.append(np.nan)
            break
    # Asegurar longitud max_iter+1 rellenando con nan si es necesario
    while len(history) < max_iter + 1:
        history.append(np.nan)
    return np.array(history, dtype=float)

# Parámetros
p0 = 2.1
p_exact = 2.0
max_iter = 12

# Iteraciones seguras
iter_g2 = fixed_point_iterations_safe(g2, p0, max_iter)
iter_g3 = fixed_point_iterations_safe(g3, p0, max_iter)
iter_g4 = fixed_point_iterations_safe(g4, p0, max_iter)

# Errores absolutos respecto a la raíz exacta p_exact
err_g2 = np.abs(iter_g2 - p_exact)
err_g3 = np.abs(iter_g3 - p_exact)
err_g4 = np.abs(iter_g4 - p_exact)

# Preparar índice de iteraciones
k = np.arange(len(err_g2))

# Graficar
plt.figure(figsize=(10, 6))
plt.plot(k, err_g2, marker='s', linestyle='-', color='green', label=r'Error $g_2(x)=\sqrt{x+2}$')
plt.plot(k, err_g3, marker='o', linestyle='--', color='blue', label=r'Error $g_3(x)=1+\frac{2}{x}$')
plt.plot(k, err_g4, marker='^', linestyle='-.', color='magenta', label=r'Error $g_4(x)=\frac{x^2+2}{2x-1}$')

plt.title('Convergencia del Error para $g_2$, $g_3$ y $g_4$')
plt.xlabel('Iteración (k)')
plt.ylabel('Error absoluto $|p_k - 2|$ (escala logarítmica)')
plt.yscale('log')
a=plt.ylim(bottom=1e-16)
b=plt.xticks(k)
b=plt.grid(True, which="both", ls="--", alpha=0.6)
b=plt.legend()
b=plt.tight_layout()
plt.show()
```

# Método de Newton-Raphson

* El método de Newton-Raphson es uno de los métodos más conocidos y más potentes en el sentido de su **velocidad de convergencia**.

* Sin embargo, al contrario que el método de la bisección, su convergencia no está asegurada.

* La idea de este método se basa en la aproximación lineal que proporciona la recta tangente. Una vez obtenido $x_k$, se calcula $x_{k+1}$ utilizando la aproximación lineal de $f(x)$ mediante la recta tangente que pasa por el punto $(x_k,f(x_k))$:

::: recuadro-gris
$$
f(x) \approx f(x_k) + f'(x_k)(x-x_k)
$$
:::

* Se toma $x_{k+1}$ como la intersección de esta recta con el eje de abscisas, es decir, la solución de

::: recuadro-gris
$$
f(x_k) + f'(x _k )(x - x_k ) = 0
$$
:::

* Si en esta expresión se despeja $x$ obtenemos una aproximación de $\hat{x}$ más exacta de lo que era la estimación $x_k$.

* Así, detotando como $x_{k+1}$ a este punto de corte, el Método de Newton-Raphson genera la sucesión de aproximacioones a partir del valor inicial $x_0$ dado por el esquema:

::: recuadro-gris
$$
x_{k+1} = x_k - \dfrac{f(x_k)}{f'(x_k)} \quad k \geq 0
$$
:::

* Veamos gráficamente en qué consiste el método de Newton-Raphson:

```{python, echo=FALSE, fig.cap="Visualización del Método de Newton-Raphson."}
import numpy as np
import matplotlib.pyplot as plt

# 1. Definir la función y su derivada
def f(x):
    return x**2 - 4 * np.sin(x)

def df(x):
    return 2*x - 4 * np.cos(x)

# 2. Configuración inicial
x0 = 3.0  # Aproximación inicial
iteraciones = 3 # Número de iteraciones a visualizar

# 3. Preparar la gráfica
plt.figure(figsize=(12, 8))
x_vals = np.linspace(0, 3.5, 500)
y_vals = f(x_vals)

# Graficar la función y el eje x
plt.plot(x_vals, y_vals, label='$f(x)$', color='black', zorder=1)
plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)

xk = x0
# 4. Bucle para visualizar las iteraciones
for i in range(iteraciones):
    fxk = f(xk)
    dfxk = df(xk)
    
    # Calcular la siguiente aproximación
    xk_next = xk - fxk / dfxk
    
    # Graficar la recta tangente
    tangente_x = np.array([xk - 0.5, xk + 0.5])
    tangente_y = dfxk * (tangente_x - xk) + fxk
    plt.plot(tangente_x, tangente_y, '--', color=f'C{i}', label=f'Tangente en $x_{i}$')
    
    # Marcar los puntos y las transiciones
    plt.plot([xk, xk], [0, fxk], ':', color=f'C{i}') # Línea vertical al punto
    plt.plot(xk, fxk, 'o', color=f'C{i}') # Punto (xk, f(xk))
    plt.plot([xk, xk_next], [fxk, 0], '--', color=f'C{i}') # Línea de la tangente al eje x
    plt.text(xk, -0.4, f'$x_{i}$', ha='center', va='top', color=f'C{i}', fontsize=12)
    
    xk = xk_next

# 5. Añadir detalles al gráfico
plt.title('Visualización del Método de Newton-Raphson')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.6)
a=plt.ylim(-2.5, 10)
a=plt.xlim(1.75, 3.2)
plt.show()
```

::: cuadro-alg
**FUNCIÓN NEWTON_RAPHSON (f, df, p0, TOL, N_MAX)**

    # ENTRADA:
    #   f:      la función para la cual se busca la raíz.
    #   df:     la derivada de la función f.
    #   p0:     la aproximación inicial.
    #   TOL:    la tolerancia para el criterio de parada.
    #   N_MAX:  el número máximo de iteraciones.

    # SALIDA:
    #   p:      la solución aproximada o un mensaje de error.

    # Paso 1: Inicializar el contador de iteraciones.
    iteracion = 1

    # Paso 2: Iniciar el bucle que se ejecutará hasta N_MAX iteraciones.
    MIENTRAS iteracion <= N_MAX:
        # Paso 3: Calcular el siguiente punto usando la fórmula de Newton-Raphson.
        #         Se debe verificar que la derivada no sea cero para evitar una división indefinida.
        SI ABS(df(p0)) < 1E-15 ENTONCES
            IMPRIMIR "Error: La derivada es cero o muy cercana a cero. El método no puede continuar."
            RETORNAR NULO
        FIN SI

        p = p0 - f(p0) / df(p0)

        # Paso 4: Comprobar si la aproximación es suficientemente buena (criterio de parada).
        #         El error se mide como la diferencia absoluta entre la iteración actual y la anterior.
        SI |p - p0| < TOL ENTONCES
            IMPRIMIR "Solución encontrada en p =", p, "después de", iteracion, "iteraciones."
            RETORNAR p
        FIN SI

        # Paso 5: Actualizar para la siguiente iteración.
        iteracion = iteracion + 1
        p0 = p
        
    FIN MIENTRAS

    # Paso 6: Si se alcanza el número máximo de iteraciones sin converger, mostrar un mensaje.
    IMPRIMIR "El método fracasó después de", N_MAX, "iteraciones. No se alcanzó la convergencia."
:::

::: caja-ejemplo
#### Ejemplo 7

Obtener la raíz del polinomio $p\left( x \right) = {x^3} + 3{x^2} + 2$ que se encuentra en el intervalo $[-4,-2]$. Se puede considerar como punto de inicio el punto medio del intervalo $x_0=-3$, una tolerancia de $10^{-8}$ y un numero máximo de 50 iteraciones. 


```{python, echo=FALSE, fig.cap="Convergencia del error para el método de Newton-Raphson."}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 1. Definir la función y su derivada
def p(x):
    return x**3 + 3*x**2 + 2

def dp(x):
    return 3*x**2 + 6*x

# 2. Configuración inicial
x0 = -3.0
tol = 1e-8
nmax = 50

# 3. Implementación del método de Newton-Raphson
historial = []
xk = x0

for k in range(nmax):
    pxk = p(xk)
    dpxk = dp(xk)

    # Almacenar datos antes de la actualización
    # Para la primera iteración, el error de paso no está definido
    error_step = np.nan if k == 0 else np.abs(xk - historial[-1][1])
    historial.append([k, xk, np.abs(pxk), error_step])

    # Criterio de parada
    if np.abs(pxk) < tol or (k > 0 and error_step < tol):
        print(f"Solución encontrada en la iteración {k}.")
        break

    # Evitar división por cero
    if np.abs(dpxk) < 1e-15:
        print("Derivada cercana a cero. El método se detiene.")
        break

    # Calcular la siguiente aproximación
    xk = xk - pxk / dpxk
else:
    # Este bloque se ejecuta si el bucle termina sin 'break'
    print("Se alcanzó el número máximo de iteraciones.")

# 4. Crear y mostrar la tabla de resultados
columnas = ['k', 'x_k', '|p(x_k)|', '|x_k - x_{k-1}|']
df = pd.DataFrame(historial, columns=columnas)
formatters = {
    'x_k': '{:.8f}'.format,
    '|p(x_k)|': '{:.4e}'.format,
    '|x_k - x_{k-1}|': '{:.4e}'.format,
}
print("--- Tabla de Iteraciones del Método de Newton-Raphson ---")
print(df.to_string(index=False, formatters=formatters, na_rep='---'))

# 5. Graficar la convergencia del error
plt.figure(figsize=(10, 6))
plt.plot(df['k'][1:], df['|p(x_k)|'][1:], marker='o', linestyle='-', label='$|p(x_k)|$ (Error de función)')
plt.plot(df['k'][1:], df['|x_k - x_{k-1}|'][1:], marker='x', linestyle='--', label='$|x_k - x_{k-1}|$ (Error de paso)')
plt.title('Convergencia del Error en el Método de Newton-Raphson')
plt.xlabel('Iteración (k)')
plt.ylabel('Valor del Error (escala logarítmica)')
plt.yscale('log')
a=plt.xticks(df['k'])
plt.grid(True, which="both", ls="--")
plt.legend()
plt.show()
```

:::

* El método de Newton es uno de los métodos más usados para calcular numéricamente raíces de ecuaciones no lineales, pues si se escoge adecuadamente la aproximación inicial $x_0$, la sucesión obtenida por iteración converge razonablemente rápido a la solución.

* Sin embargo, este es un método denominado local, pues si $x_0$ no es lo suficientemente cercano a $\hat{x}$ y la función $f(x)$
no tiene **buenas propiedades**, se pueden presentar algunos problemas. El siguiente ejemplo muestra algunas de las situaciones indeseables en el método de Newton.

::: caja-ejemplo
#### Ejemplo 8

Ahora, se resolverá el mismo problema del **Ejemplo 7**, pero tomando como punto de partida $x_0=1$. Este ejemplo ilustra un caso donde el método de Newton-Raphson no converge a la raíz esperada debido a una mala elección del punto inicial.

```{python, echo=FALSE, fig.cap="Comportamiento divergente del método de Newton-Raphson con un punto inicial inadecuado."}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 1. Definir la función y su derivada
def p(x):
    return x**3 + 3*x**2 + 2

def dp(x):
    return 3*x**2 + 6*x

# 2. Configuración inicial con x0 = 1
x0 = 1.0
tol = 1e-8
nmax = 20 # Se limita a 10 para observar la divergencia

# 3. Implementación del método de Newton-Raphson
historial = []
xk = x0

for k in range(nmax):
    pxk = p(xk)
    dpxk = dp(xk)

    # Almacenar datos antes de la actualización
    error_step = np.nan if k == 0 else np.abs(xk - historial[-1][1])
    historial.append([k, xk, np.abs(pxk), error_step])

    # Criterio de parada (es poco probable que se cumpla)
    if np.abs(pxk) < tol or (k > 0 and error_step < tol):
        print(f"Solución encontrada en la iteración {k}.")
        break

    # Evitar división por cero
    if np.abs(dpxk) < 1e-15:
        print("Derivada cercana a cero. El método se detiene.")
        break

    # Calcular la siguiente aproximación
    xk = xk - pxk / dpxk
else:
    print(f"Se alcanzó el número máximo de {nmax} iteraciones sin convergencia.")

# 4. Crear y mostrar la tabla de resultados
columnas = ['k', 'x_k', '|p(x_k)|', '|x_k - x_{k-1}|']
df = pd.DataFrame(historial, columns=columnas)
formatters = {
    'x_k': '{:.8f}'.format,
    '|p(x_k)|': '{:.4e}'.format,
    '|x_k - x_{k-1}|': '{:.4e}'.format,
}
print("--- Tabla de Iteraciones con x0 = 1 (Divergencia) ---")
print(df.to_string(index=False, formatters=formatters, na_rep='---'))

# 5. Graficar el comportamiento del error
plt.figure(figsize=(10, 6))
plt.plot(df['k'][1:], df['|p(x_k)|'][1:], marker='o', linestyle='-', label='$|p(x_k)|$ (Error de función)')
plt.plot(df['k'][1:], df['|x_k - x_{k-1}|'][1:], marker='x', linestyle='--', label='$|x_k - x_{k-1}|$ (Error de paso)')
plt.title('Comportamiento del Error en el Método de Newton-Raphson (x0=1)')
plt.xlabel('Iteración (k)')
plt.ylabel('Valor del Error (escala logarítmica)')
plt.yscale('log')
a=plt.xticks(df['k'])
plt.grid(True, which="both", ls="--")
plt.legend()
plt.show()
```
:::

# Método de la Secante

* El método de Newton es un método muy potente pero tiene un inconveniente: necesitamos conocer la función derivada $f'(x)$.

* En muchos problemas de análisis numérico, dicho conocimiento es un lujo. Sabemos cómo evaluar la función $f$ pero no hay forma de tener una expresión de la función derivada $f'$. En estos casos, el metodo de Newton-Raphson no es aplicable.

* Para solventar esta dificultad, podemos usar el cociente incremental como aproximación a $f'(x_{n-1})$:

::: recuadro-gris
$$
f'(x_{n-1}) \approx \dfrac{f(x_{n-1})-f(x_{n-2})}{x_{n-1}-x_{n-2}}
$$
:::

* Si sustituimos la expresión anterior de $f'(x_{n-1})$ en el método de Newton-Raphson, obtenemos el denominado metodo de la secante:

::: recuadro-gris
$$
x_n = x_{n-1}- \dfrac{f(x_{n-1})}{\frac{f(x_{n-1})-f(x_{n-2})}{x_{n-1}-x_{n-2}}} = x_{n-1}-\dfrac{f(x_{n-1})\cdot(x_{n-1}-x_{n-2})}{f(x_{n-1})-f(x_{n-2})}
$$
:::

* Esta fórmula de iteración puede deducirse de hallar el punto de corte con el eje $x$ de la recta secante que pasa por los puntos $(x_{n-2},f(x_{n-2}))$ y $(x_{n-1},f(x_{n-1}))$, lo cual se puede ver en el siguiente gráfico:

```{python, echo=FALSE, fig.cap="Visualización del Método de la Secante."}
import numpy as np
import matplotlib.pyplot as plt

# 1. Definir la función
def f(x):
    return x**2 - 4 * np.sin(x)

# 2. Configuración inicial
x0, x1 = 1.0, 3.0  # Aproximaciones iniciales
iteraciones = 3   # Número de iteraciones a visualizar

# 3. Preparar la gráfica
plt.figure(figsize=(12, 8))
x_vals = np.linspace(0, 3.5, 500)
y_vals = f(x_vals)

# Graficar la función y el eje x
plt.plot(x_vals, y_vals, label='$f(x)$', color='black', zorder=1)
plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)

# 4. Bucle para visualizar las iteraciones
xk_prev, xk = x0, x1
for i in range(iteraciones):
    fxk_prev, fxk = f(xk_prev), f(xk)
    
    # Calcular la siguiente aproximación
    xk_next = xk - fxk * (xk - xk_prev) / (fxk - fxk_prev)
    
    # Definir el color para la iteración actual
    color = f'C{i}'
    
    # Graficar la recta secante
    # La recta secante pasa por (xk_prev, fxk_prev) y (xk, fxk) y se extiende hasta (xk_next, 0)
    plt.plot([xk_prev, xk, xk_next], [fxk_prev, fxk, 0], '--', color=color, label=f'Secante {i+1}')
    
    # Marcar los puntos
    plt.plot([xk_prev, xk], [fxk_prev, fxk], 'o', color=color)
    
    # Etiquetar los puntos de forma clara y correcta
    if i == 0:
        plt.text(xk_prev, -0.4, f'$x_0$', ha='center', va='top', fontsize=12, color=color)
    
    plt.text(xk, -0.4, f'$x_{i+1}$', ha='center', va='top', fontsize=12, color=color)
    
    # Actualizar puntos para la siguiente iteración
    xk_prev, xk = xk, xk_next

# 5. Añadir detalles al gráfico
plt.title('Visualización del Método de la Secante')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.grid(True, linestyle=':', alpha=0.6)
a=plt.ylim(-5, 15)
plt.xlim(0.5, 3.2);
plt.show()
```

::: cuadro-alg
**FUNCIÓN SECANTE (f, p0, p1, TOL, N_MAX)**

    # ENTRADA:
    #   f:      la función para la cual se busca la raíz.
    #   p0, p1: las dos aproximaciones iniciales.
    #   TOL:    la tolerancia para el criterio de parada.
    #   N_MAX:  el número máximo de iteraciones.

    # SALIDA:
    #   p:      la solución aproximada o un mensaje de error.

    # Paso 1: Inicializar el contador de iteraciones.
    iteracion = 1

    # Paso 2: Iniciar el bucle que se ejecutará hasta N_MAX iteraciones.
    MIENTRAS iteracion <= N_MAX:
        # Paso 3: Calcular el siguiente punto usando la fórmula de la secante.
        #         Se debe verificar que el denominador no sea cero.
        SI ABS(f(p1) - f(p0)) < 1E-15 ENTONCES
            IMPRIMIR "Error: División por cero o un valor muy pequeño. El método no puede continuar."
            RETORNAR NULO
        FIN SI

        p = p1 - f(p1) * (p1 - p0) / (f(p1) - f(p0))

        # Paso 4: Comprobar si la aproximación es suficientemente buena.
        #         El error se mide como la diferencia absoluta entre la iteración actual y la anterior.
        SI |p - p1| < TOL ENTONCES
            IMPRIMIR "Solución encontrada en p =", p, "después de", iteracion, "iteraciones."
            RETORNAR p
        FIN SI

        # Paso 5: Actualizar las aproximaciones para la siguiente iteración.
        iteracion = iteracion + 1
        p0 = p1
        p1 = p
        
    FIN MIENTRAS

    # Paso 6: Si se alcanza el número máximo de iteraciones sin converger, mostrar un mensaje.
    IMPRIMIR "El método fracasó después de", N_MAX, "iteraciones. No se alcanzó la convergencia."
:::

::: caja-ejemplo
#### Ejemplo 9

Se retoma el problema del **Ejemplo 7** para encontrar la raíz del polinomio $p(x) = x^3 + 3x^2 + 2$, pero esta vez aplicando el método de la secante. Se utilizarán como puntos iniciales los extremos del intervalo $[-4, -2]$, es decir, $x_0 = -4$ y $x_1 = -2$, con una tolerancia de $10^{-8}$ y un máximo de 50 iteraciones.

```{python, echo=FALSE, fig.cap="Convergencia del error para el método de la Secante."}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 1. Definir la función
def p(x):
    return x**3 + 3*x**2 + 2

# 2. Configuración inicial
p0 = -4.0
p1 = -2.0
tol = 1e-8
nmax = 50

# 3. Implementación del método de la Secante
historial = []

# Almacenar las dos primeras aproximaciones
historial.append([0, p0, np.abs(p(p0)), np.nan])
historial.append([1, p1, np.abs(p(p1)), np.abs(p1 - p0)])

for k in range(2, nmax):
    fp0 = p(p0)
    fp1 = p(p1)

    # Evitar división por cero
    if np.abs(fp1 - fp0) < 1e-15:
        print("La diferencia f(p1) - f(p0) es cercana a cero. El método se detiene.")
        break

    # Calcular la siguiente aproximación
    p_next = p1 - fp1 * (p1 - p0) / (fp1 - fp0)
    
    error_step = np.abs(p_next - p1)
    historial.append([k, p_next, np.abs(p(p_next)), error_step])

    # Criterio de parada
    if error_step < tol:
        print(f"Solución encontrada en la iteración {k}.")
        break

    # Actualizar puntos para la siguiente iteración
    p0, p1 = p1, p_next
else:
    print("Se alcanzó el número máximo de iteraciones.")

# 4. Crear y mostrar la tabla de resultados
columnas = ['k', 'x_k', '|p(x_k)|', '|x_k - x_{k-1}|']
df = pd.DataFrame(historial, columns=columnas)
formatters = {'x_k': '{:.8f}'.format, '|p(x_k)|': '{:.4e}'.format, '|x_k - x_{k-1}|': '{:.4e}'.format}
print("--- Tabla de Iteraciones del Método de la Secante ---")
print(df.to_string(index=False, formatters=formatters, na_rep='---'))

# 5. Graficar la convergencia del error
plt.figure(figsize=(10, 6))
plt.plot(df['k'][1:], df['|p(x_k)|'][1:], marker='o', linestyle='-', label='$|p(x_k)|$ (Error de función)')
plt.plot(df['k'][1:], df['|x_k - x_{k-1}|'][1:], marker='x', linestyle='--', label='$|x_k - x_{k-1}|$ (Error de paso)')
plt.title('Convergencia del Error en el Método de la Secante')
plt.xlabel('Iteración (k)'); plt.ylabel('Valor del Error (escala logarítmica)'); plt.yscale('log')
plt.xticks(df['k']); plt.grid(True, which="both", ls="--"); plt.legend(); plt.show()
```
:::

* Este método tiene convergencia local si la función $f$ es derivable cerca de la raíz, si la raíz es simple y si las aproximaciones iniciales están suficientemente próximas a ella.

* Su orden de convergencia es el número áureo, $\phi = \frac{{1 + \sqrt 5 }}{2} \approx 1.618$, lo que implica una convergencia superlineal, aunque inferior a la del método de Newton.

* Si las aproximaciones iniciales están demasiado alejadas de la raíz o si esta es múltiple, el método no asegura la convergencia.

# Método de Regula Falsi

* Los métodos de Newton-Raphson y de la secante son métodos relativamente rápidos en cuanto a su convergencia pero tienen el inconveniente que no tenemos asegurada dicha convergencia.

* Recordemos el Teorema donde nos dice si el cero es simple ($f'(\hat x)\neq 0$), existe un entorno del cero $\hat x$ tal que para cualquier valor inicial $x_0$ perteneciente a dicho entorno, el método de Newton-Raphson es convergente. El problema es que hallar dicho entorno es un problema mucho más difícil que hallar el cero en sí.

* Por dicho motivo, de cara a asegurar la convergencia de la sucesión $\{x_n\}_n$ tenemos el método de Regula falsi que usa las ventajas de los métodos de la bisección y de la secante.

* En pocas palabras, el método de regula falsi supone que hay un cambio de signo entre $x_{n-2}$ y $x_{n-1}$, es decir, $f(x_{n-2})\cdot f(x_{n-1})<0$ y halla $x_n$ usando el método de la secante. Como $x_n$ estará entre $x_{n-2}$ y $x_{n-1}$ habrá un cambio de signo de $f$ entre $x_{n-2}$ y $x_n$ o entre $x_{n-1}$ y $x_n$.

* Se calcula $x_n$ según el método de la secante:

::: recuadro-gris
$$
x_n = x_{n-1}-\dfrac{f(x_{n-1})\cdot(x_{n-1}-x_{n-2})}{f(x_{n-1})-f(x_{n-2})}
$$
::::

* **Caso 1**. Si $f(x_n)\cdot f(x_{n-1})<0$, tenemos un cambio de signo de $f$ en el intervalo $(x_{n-1},x_n)$. En este caso, no hacemos nada y la sucesión quedará $\ldots,x_{n-2},x_{n-1},x_n$

* **Caso 2**. Si $f(x_n)\cdot f(x_{n-2})<0$ tenemos un cambio de signo de $f$ en el intervalo $(x_{n-2},x_n)$. En este caso, cambiamos el orden de $x_{n-2}$ y $x_{n-1}$ y la sucesión quedará $\ldots,x_{n-1},x_{n-2},x_n.

* Volvemos al paso inicial para hallar $x_{n+1}$.


```{python, echo=FALSE, fig.cap="Visualización del Método de Regula Falsi."}
import numpy as np
import matplotlib.pyplot as plt

# 1. Definir la función
def f(x):
    return x**2 - 4 * np.sin(x)

# 2. Configuración inicial
a, b = 1.0, 3.0  # Intervalo inicial [a, b]
iteraciones = 3   # Número de iteraciones a visualizar

# 3. Preparar la gráfica
plt.figure(figsize=(12, 8))
x_vals = np.linspace(0, 3.5, 500)
y_vals = f(x_vals)

# Graficar la función y el eje x
plt.plot(x_vals, y_vals, label='$f(x) = x^2 - 4sin(x)$', color='black', zorder=1)
plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)

# 4. Bucle para visualizar las iteraciones del método de Regula Falsi
a_k, b_k = a, b
for i in range(iteraciones):
    fa, fb = f(a_k), f(b_k)
    
    # Calcular la siguiente aproximación usando la fórmula de Regula Falsi
    p_next = b_k - fb * (b_k - a_k) / (fb - fa)
    fp_next = f(p_next)
    
    # Definir el color para la iteración actual
    color = f'C{i}'
    
    # Graficar la recta secante
    plt.plot([a_k, b_k], [fa, fb], '--', color=color, label=f'Secante {i+1}')
    
    # Marcar los puntos
    plt.plot([a_k, b_k], [fa, fb], 'o', color=color)
    plt.plot(p_next, 0, 'x', color=color, markersize=10, markeredgewidth=2)
    
    # Etiquetar los puntos de forma clara
    if i == 0:
        plt.text(a_k, fa + 0.5, f'$a_0$', ha='center', va='bottom', fontsize=12)
    plt.text(b_k, fb - 1, f'$b_{i}$', ha='center', va='top', fontsize=12)
    plt.text(p_next, -0.5, f'$p_{i+1}$', ha='center', va='top', fontsize=12, color=color)
    
    # Actualizar el intervalo para la siguiente iteración
    if fa * fp_next < 0:
        b_k = p_next
    else:
        a_k = p_next

# 5. Añadir detalles al gráfico
plt.title('Visualización del Método de Regula Falsi')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(loc='upper left')
plt.grid(True, linestyle=':', alpha=0.6)
plt.ylim(-5, 10);
plt.xlim(0.5, 3.2);
plt.show()
```

::: cuadro-alg
**FUNCIÓN REGULA_FALSI(f, a, b, TOL, N_MAX)**

    # ENTRADA:
    #   f:      la función para la cual se busca la raíz.
    #   a, b:   los extremos del intervalo [a, b] donde f(a) * f(b) < 0.
    #   TOL:    la tolerancia para el criterio de parada.
    #   N_MAX:  el número máximo de iteraciones.

    # SALIDA:
    #   p:      la solución aproximada o un mensaje de error.

    # Paso 1: Verificar la condición inicial.
    SI f(a) * f(b) >= 0 ENTONCES
        IMPRIMIR "Error: f(a) y f(b) deben tener signos opuestos."
        RETORNAR NULO
    FIN SI

    # Paso 2: Inicializar el contador de iteraciones y la aproximación previa.
    iteracion = 0
    p_prev = a # O cualquier valor que no sea la primera p

    # Paso 3: Iniciar el bucle.
    MIENTRAS iteracion < N_MAX:
        # Paso 4: Calcular la nueva aproximación con la fórmula de la secante.
        p = b - f(b) * (b - a) / (f(b) - f(a))

        # Paso 5: Comprobar el criterio de parada.
        SI |p - p_prev| < TOL O ABS(f(p)) < TOL ENTONCES
            IMPRIMIR "Solución encontrada en p =", p, "después de", iteracion, "iteraciones."
            RETORNAR p
        FIN SI

        # Paso 6: Actualizar el intervalo.
        SI f(a) * f(p) < 0 ENTONCES
            b = p
        SINO
            a = p
        FIN SI

        # Paso 7: Actualizar para la siguiente iteración.
        p_prev = p
        iteracion = iteracion + 1
        
    FIN MIENTRAS

    # Paso 8: Si se alcanza el número máximo de iteraciones.
    IMPRIMIR "El método fracasó después de", N_MAX, "iteraciones."
    RETORNAR p
:::


::: caja-ejemplo
#### Ejemplo 10

Se retoma el problema del **Ejemplo 7** para encontrar la raíz del polinomio $p(x) = x^3 + 3x^2 + 2$, pero esta vez aplicando el método de Regula Falsi. Se utilizarán como puntos iniciales los extremos del intervalo $[-4, -2]$, es decir, $a = -4$ y $b = -2$, con una tolerancia de $10^{-8}$ y un máximo de 50 iteraciones.

```{python, echo=FALSE, fig.cap="Convergencia del error para el método de Regula Falsi."}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 1. Definir la función
def p(x):
    return x**3 + 3*x**2 + 2

# 2. Configuración inicial
a = -4.0
b = -2.0
tol = 1e-8
nmax = 50

# 3. Implementación del método de Regula Falsi
historial = []
p_prev = np.nan # No hay p_prev en la primera iteración

if p(a) * p(b) >= 0:
    print("El método de Regula Falsi requiere que f(a) y f(b) tengan signos opuestos.")
else:
    for k in range(nmax):
        fa = p(a)
        fb = p(b)
        
        # Fórmula de Regula Falsi (igual que la secante)
        p_next = b - fb * (b - a) / (fb - fa)
        
        error_step = np.abs(p_next - p_prev) if k > 0 else np.nan
        historial.append([k, a, b, p_next, np.abs(p(p_next)), error_step])

        # Criterio de parada
        if np.abs(p(p_next)) < tol or (k > 0 and error_step < tol):
            print(f"Solución encontrada en la iteración {k}.")
            break

        # Actualizar el intervalo
        if p(a) * p(p_next) < 0:
            b = p_next
        else:
            a = p_next
        
        p_prev = p_next
    else:
        print("Se alcanzó el número máximo de iteraciones.")

# 4. Crear y mostrar la tabla de resultados
columnas = ['k', 'a_k', 'b_k', 'p_k', '|p(p_k)|', '|p_k - p_{k-1}|']
df = pd.DataFrame(historial, columns=columnas)
formatters = {
    'a_k': '{:.8f}'.format,
    'b_k': '{:.8f}'.format,
    'p_k': '{:.8f}'.format,
    '|p(p_k)|': '{:.4e}'.format,
    '|p_k - p_{k-1}|': '{:.4e}'.format
}
print("--- Tabla de Iteraciones del Método de Regula Falsi ---")
print(df.to_string(index=False, formatters=formatters, na_rep='---'))

# 5. Graficar la convergencia del error
plt.figure(figsize=(10, 6))
plt.plot(df['k'][1:], df['|p(p_k)|'][1:], marker='o', linestyle='-', label='$|p(p_k)|$ (Error de función)')
plt.plot(df['k'][1:], df['|p_k - p_{k-1}|'][1:], marker='x', linestyle='--', label='$|p_k - p_{k-1}|$ (Error de paso)')
plt.title('Convergencia del Error en el Método de Regula Falsi')
plt.xlabel('Iteración (k)')
plt.ylabel('Valor del Error (escala logarítmica)')
plt.yscale('log')
plt.xticks(df['k']);
plt.grid(True, which="both", ls="--")
plt.legend()
plt.show()
```
:::


------------------------------------------------------------------------
  
<small>Última revisión: `r format(Sys.Date(), "%d de %B, %Y")`</small>
